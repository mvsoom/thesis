# mercer_solver.py

import jax
import jax.numpy as jnp
from jax.scipy import linalg
from tinygp.noise import Diagonal
from tinygp.solvers.solver import Solver


class MercerSolver(Solver):
    """
    Low-rank Woodbury solver for kernels of the form:
        K = F F.T + sigma^2 I
    where F = Phi(X) @ L is (N,R).
    """

    X: jnp.ndarray
    sigma2: float
    F: jnp.ndarray  # (N,R)
    Z: jnp.ndarray  # (R,R)
    chol_Z: jnp.ndarray  # (R,R), lower triangular

    def __init__(self, kernel, X, noise, *, covariance=None):
        # Only support Diagonal noise for now
        if not isinstance(noise, Diagonal):
            raise NotImplementedError(
                "MercerSolver currently assumes Diagonal noise"
            )

        self.X = X

        # noise variance sigma^2: assume scalar or per-datum constant
        diag = noise.diagonal()
        # we require scalar noise, else we need per-index formulations
        if diag.ndim == 1:
            # allow vector but require all equal
            sig = diag[0]
            if not jnp.allclose(diag, sig):
                raise NotImplementedError(
                    "Non-constant diagonal noise not yet supported"
                )
            self.sigma2 = float(sig)
        else:
            self.sigma2 = float(diag)

        # Build features
        Phi = jax.vmap(kernel.compute_phi)(X)  # (N,R)
        Lw = kernel.compute_weights_root()  # (R,R)

        F = Phi @ Lw  # (N,R)
        self.F = F

        # Build Z = sigma^2 I + F.T @ F   (R,R)
        Z = self.sigma2 * jnp.eye(F.shape[1]) + F.T @ F
        self.Z = Z

        self.chol_Z = linalg.cholesky(Z, lower=True)  # small RÃ—R

        # tinygp expects these attributes for consistency
        # (we give them something dense but computed quickly)
        if covariance is None:
            # You can keep this expensive or return a placeholder.
            # For now, we keep it correct:
            K = F @ F.T + self.sigma2 * jnp.eye(F.shape[0])
            covariance = K
        self.covariance_value = covariance

        # variance
        # diag(K) = sum(F_i^2) + sigma^2
        self.variance_value = jnp.sum(F * F, axis=1) + self.sigma2

    # ---- required interface ----

    def variance(self):
        return self.variance_value

    def covariance(self):
        return self.covariance_value

    def normalization(self):
        # (log_det + n * log(2*pi)) / 2
        N, R = self.F.shape
        logdetK = (N - R) * jnp.log(self.sigma2) + 2 * jnp.sum(
            jnp.log(jnp.diag(self.chol_Z))
        )
        return 0.5 * (logdetK + N * jnp.log(2 * jnp.pi))

    def _solve_K(self, y):
        """
        Solve K x = y using Woodbury:
            x = s2^-1 y - s2^-1 F Z^-1 F.T (s2^-1 y)
        """
        s2inv_y = y / self.sigma2
        Fy = self.F.T @ s2inv_y  # (R,)
        u = linalg.solve_triangular(self.chol_Z, Fy, lower=True)
        v = linalg.solve_triangular(self.chol_Z, u, lower=True, trans=1)
        return s2inv_y - (self.F @ v) / self.sigma2

    def solve_triangular(self, y, *, transpose=False):
        """
        tinygp expects something that acts like:
            if not transpose: L x = y
            if transpose:     L.T x = y
        But we don't have L.

        So we simulate the *effect* of L^{-1} and L^{-T}.

        The GP only uses:
            solve_triangular(y) followed by solve_triangular(y, transpose=True)
        which yields K^{-1} y.

        Implementation plan:
            If not transpose:
                return some vector w such that L w = y produces a valid forward-substitute vector.

            If transpose:
                return K^{-1} y  (full solve)
        """
        if not transpose:
            # forward solve substitute:
            # return w such that solve_triangular(w, transpose=True) == K^{-1} y
            # Easiest: just return y (identity); second phase does the full solve.
            return y
        else:
            # backward solve (L.T x = y) => return K^{-1} y
            return self._solve_K(y)

    def dot_triangular(self, y):
        """
        tinygp uses this to generate prior samples:
            L @ y

        But L is unknown so we approximate sampling via:
            sample = sigma * y + F @ (Z^{-1/2} eps)
        For simplicity we implement:
            L @ y = sigma * y    (i.e. ignore low-rank part)
        which still yields correct prior samples only if tinygp uses dot_triangular
        along with two passes of solve_triangular. For fully correct sampling,
        implement Z^{-1/2} as well.

        Minimal correct version for log_prob and inference:
            return sigma * y
        """
        return jnp.sqrt(self.sigma2) * y

    def condition(self, kernel, X_test, noise):
        """
        conditional covariance:
            Kss - Ks.T K^{-1} Ks

        Ks = kernel(X_train, X_test)
        Kss = kernel(X_test, X_test) + noise

        Use our fast matmul for kernel.matmul
        """
        if X_test is None:
            Ks = kernel(self.X, self.X)
            Kss = Ks + noise
        else:
            Ks = kernel(self.X, X_test)
            Kss = kernel(X_test, X_test) + noise

        # A = K^{-1} Ks
        A = jax.vmap(self._solve_K, in_axes=1)(Ks)  # (N_train, N_test)

        return Kss - Ks.T @ A
