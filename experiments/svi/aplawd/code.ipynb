{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters",
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters, export\n",
    "kernelname = \"matern:52\"\n",
    "M = 128  # Number of PRISM basis functions\n",
    "iteration = 1\n",
    "seed = 3164879\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "import gpjax as gpx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from gpjax.dataset import Dataset\n",
    "from gpjax.likelihoods import Gaussian\n",
    "from gpjax.mean_functions import Zero\n",
    "from gpjax.parameters import Parameter\n",
    "\n",
    "from aplawd.data import get_data_periods, get_whitener\n",
    "from prism.svi import (\n",
    "    init_Z_inverse_ecdf,\n",
    "    pick_best,\n",
    "    svi_basis,\n",
    ")\n",
    "from prism.t_svi import t_CollapsedVariationalGaussian\n",
    "from utils import dump_egg, time_this\n",
    "from utils.jax import resolve_gpjax_kernel, vk\n",
    "\n",
    "master_key = jax.random.key(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of independent waveforms to process train/test\n",
    "N_TRAIN = 5_000\n",
    "N_TEST = 1_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 320  # cutoff at 99% quantile\n",
    "X, y = get_data_periods(width=width)\n",
    "X = jnp.array(X, dtype=jnp.float64)\n",
    "y = jnp.array(y, dtype=jnp.float64)\n",
    "\n",
    "y = jnp.log10(y)\n",
    "\n",
    "whiten_y, unwhiten_y = get_whitener(y)\n",
    "y = whiten_y(y)\n",
    "\n",
    "train_data = Dataset(X=X[:N_TRAIN], y=y[:N_TRAIN])\n",
    "test_data = Dataset(\n",
    "    X=X[N_TRAIN : N_TRAIN + N_TEST], y=y[N_TRAIN : N_TRAIN + N_TEST]\n",
    ")\n",
    "\n",
    "_, WIDTH_TRAIN = X.shape\n",
    "n_eff = int(np.sum(~np.isnan(X), axis=1).mean())\n",
    "\n",
    "print(\"Number of training waveforms:\", N_TRAIN)\n",
    "print(\"Average number samples per waveform:\", n_eff)\n",
    "print(\"Padding width (max waveform length):\", WIDTH_TRAIN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# STAGE 1: t-PRISM (COLLAPSED SVI)\n",
    "# Learn a global basis for the variably sized data\n",
    "# which then defines a map for the latent space of the BGPLVM\n",
    "# Secret sauce: \"batching\" complete waveforms via masking\n",
    "# ELBO factorizes over independent waveforms\n",
    "# Secret sauce 2: robust to spike noise\n",
    "##############################################################\n",
    "batch_size = 512\n",
    "num_iters = (\n",
    "    3000  # 800 suffices if we init lengthscale to 10, but this also works\n",
    ")\n",
    "lr = 1e-2\n",
    "jitter = 1e-4\n",
    "num_restarts = 1\n",
    "\n",
    "\n",
    "def trainable(path, v):\n",
    "    if not isinstance(v, Parameter):\n",
    "        return False\n",
    "    # path is usually a tuple of names; make this robust\n",
    "    leaf = path[-1] if isinstance(path, (tuple, list)) and path else str(path)\n",
    "    if leaf == \"variance\":\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def t_collapsed_svi(key=vk(), M=M, nu=1, num_inner=3):\n",
    "    Z = init_Z_inverse_ecdf(key, M, X)\n",
    "\n",
    "    k = resolve_gpjax_kernel(kernelname)\n",
    "    kernel = k(lengthscale=1.0, variance=1.0)\n",
    "    prior = gpx.gps.Prior(kernel, Zero())\n",
    "    likelihood = Gaussian(num_datapoints=WIDTH_TRAIN, obs_stddev=1.0)\n",
    "\n",
    "    posterior = prior * likelihood\n",
    "\n",
    "    return t_CollapsedVariationalGaussian(\n",
    "        posterior=posterior,\n",
    "        inducing_inputs=Z,\n",
    "        nu=nu,\n",
    "        num_inner=num_inner,\n",
    "        jitter=jitter,\n",
    "    )\n",
    "\n",
    "\n",
    "master_key, subkey = jax.random.split(master_key)\n",
    "\n",
    "from prism.svi import optimize as optimize_svi\n",
    "from prism.svi import optimize_restarts\n",
    "\n",
    "optimize_svi = partial(\n",
    "    optimize_svi,\n",
    "    model=t_collapsed_svi,\n",
    "    dataset=train_data,\n",
    "    lr=lr,\n",
    "    batch_size=batch_size,\n",
    "    num_iters=num_iters,\n",
    "    trainable=trainable,\n",
    ")\n",
    "\n",
    "with time_this() as svi_timer:\n",
    "    states, elbo_histories = optimize_restarts(\n",
    "        optimize_svi, num_restarts, subkey\n",
    "    )\n",
    "\n",
    "qsvi, history = pick_best(states, elbo_histories, t_collapsed_svi())\n",
    "\n",
    "px.line(\n",
    "    history,\n",
    "    title=\"ELBO during training (best run)\",\n",
    "    labels={\"x\": \"Iteration\", \"y\": \"ELBO\"},\n",
    ").show()\n",
    "\n",
    "\n",
    "print(\"Observation sigma_noise:\", qsvi.posterior.likelihood.obs_stddev)\n",
    "print(\"Learned lengthscales:\", qsvi.posterior.prior.kernel.lengthscale)\n",
    "print(\"Learned variance:\", qsvi.posterior.prior.kernel.variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and inspect the global SVI basis found\n",
    "def psi(t):\n",
    "    return svi_basis(qsvi, t)\n",
    "\n",
    "\n",
    "tau_test = jnp.linspace(0, n_eff * 2, 1000)\n",
    "Psi_test = jax.vmap(psi)(tau_test)  # test indices\n",
    "\n",
    "master_key, subkey = jax.random.split(master_key)\n",
    "\n",
    "eps = jax.random.normal(subkey, shape=(M, 5))\n",
    "y = Psi_test @ eps\n",
    "\n",
    "px.line(y).update_traces(x=tau_test).update_layout(\n",
    "    xaxis_title=\"tau\",\n",
    "    yaxis_title=\"u'(t)\",\n",
    "    title=\"Prior samples of learned latent function distribution\",\n",
    ").show()\n",
    "# This is a prior draw from the learned RKHS subspace, not data-like yet.\n",
    "# It answers: What does a typical GP draw look like under the learned kernel?\n",
    "# expected to look generic and smooth\n",
    "\n",
    "px.line(Psi_test).update_traces(x=tau_test).update_layout(\n",
    "    xaxis_title=\"tau\",\n",
    "    yaxis_title=\"psi_m(t)\",\n",
    "    title=\"Learned basis functions psi_m(t)\",\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prism.t_svi import do_t_prism\n",
    "\n",
    "mu_eps, Sigma_eps, w = do_t_prism(qsvi, train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we reconstruct waveforms from the SVI latent space?\n",
    "from prism.svi import reconstruct_waveforms\n",
    "\n",
    "test_indices = jnp.array([1, 10, 25, 50])\n",
    "\n",
    "reconstruct_waveforms(mu_eps, qsvi, train_data, test_indices, tau_test).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute normalized log likelihood of test data under model via MC\n",
    "from prism.t_svi import t_log_evidence_is_on_test\n",
    "\n",
    "master_key, mc_key = jax.random.split(master_key)\n",
    "\n",
    "neff = np.sum(~np.isnan(test_data.y), axis=1)\n",
    "logp = t_log_evidence_is_on_test(qsvi, test_data, mc_key)\n",
    "\n",
    "x = logp / neff\n",
    "x_mean = np.mean(x)\n",
    "x_std = np.std(x)\n",
    "\n",
    "print(\n",
    "    \"Average log likelihood per effective data point on test set:\",\n",
    "    x_mean,\n",
    "    \"+/-\",\n",
    "    x_std,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we reconstruct test waveforms?\n",
    "mu_eps, Sigma_eps, w = do_t_prism(qsvi, test_data)\n",
    "\n",
    "reconstruct_waveforms(\n",
    "    mu_eps, qsvi, test_data, test_indices, tau_test, weights=w\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate local null model: kernel replaced by white noise, everything else (including SVI approx) same\n",
    "from prism.svi import as_null_model\n",
    "\n",
    "master_key, subkey = jax.random.split(master_key)\n",
    "\n",
    "qsvi_null = as_null_model(qsvi)\n",
    "\n",
    "logp_null = t_log_evidence_is_on_test(\n",
    "    qsvi_null, test_data, mc_key\n",
    ")  # reuse same key: reduces variance\n",
    "\n",
    "x_null = logp_null / neff\n",
    "x_null_mean = np.mean(x_null)\n",
    "x_null_std = np.std(x_null)\n",
    "\n",
    "print(\n",
    "    \"Average log likelihood per effective data point on test set under NULL:\",\n",
    "    x_null_mean,\n",
    "    \"+/-\",\n",
    "    x_null_std,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"whiten\": whiten_y,\n",
    "    \"unwhiten\": unwhiten_y,\n",
    "    \"qsvi\": qsvi,\n",
    "}\n",
    "\n",
    "dump_egg(payload, os.getenv(\"EXPERIMENT_NOTEBOOK_REL\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# export\n",
    "svi_walltime = svi_timer.walltime\n",
    "svi_obs_std = float(qsvi.posterior.likelihood.obs_stddev)\n",
    "svi_lengthscale = float(qsvi.posterior.prior.kernel.lengthscale)\n",
    "\n",
    "mean_loglike_test = x_mean\n",
    "std_loglike_test = x_std\n",
    "mean_loglike_test_null = x_null_mean\n",
    "std_loglike_test_null = x_null_std\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
