{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters",
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters, export\n",
    "M = 128  # Number of PRISM basis functions\n",
    "Q = 9  # Latent dimensionality of qBGPLVM\n",
    "iteration = 1\n",
    "seed = 2455473317\n",
    "d = 1\n",
    "am = \"rbf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from itertools import combinations\n",
    "\n",
    "import gpjax as gpx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from gpjax.dataset import Dataset\n",
    "from gpjax.kernels import RBF, ProductKernel, RationalQuadratic\n",
    "from gpjax.likelihoods import Gaussian\n",
    "from gpjax.mean_functions import Zero\n",
    "from gpjax.parameters import NonNegativeReal, Parameter\n",
    "from gpjax.variational_families import CollapsedVariationalGaussian\n",
    "\n",
    "from egifa.data import get_data\n",
    "from lvm.bgplvm import BayesianGPLVM\n",
    "from lvm.plots import (\n",
    "    oq_sensitivity_spearman,\n",
    "    pair_plots_oq,\n",
    "    plot_cluster_means_in_data_space,\n",
    "    plot_cluster_samples_in_data_space,\n",
    "    plot_oq_vs_loglik,\n",
    "    sample_latent_gmm_pointwise,\n",
    "    single_plot_oq,\n",
    "    xd_gmm_plots,\n",
    ")\n",
    "from lvm.qgpvlm import loglikelihood_on_test, make_qgpvlm\n",
    "from prism.pack import NormalizedPACK\n",
    "from prism.svi import (\n",
    "    init_Z_inverse_ecdf,\n",
    "    offdiag_energy_fraction,\n",
    "    optimize_restarts_scan,\n",
    "    pick_best,\n",
    "    svi_basis,\n",
    ")\n",
    "from utils import dump_egg, time_this\n",
    "from utils.constants import NOISE_FLOOR_POWER\n",
    "from utils.jax import pca_reduce, vk\n",
    "\n",
    "master_key = jax.random.key(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of independent waveforms to process train/test\n",
    "N_TRAIN = 2500\n",
    "N_TEST = 500\n",
    "N_HELD = 210\n",
    "\n",
    "WIDTH = 8192\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, meta = get_data(n=N_TRAIN, width=WIDTH, with_metadata=True)\n",
    "X = jnp.array(X, dtype=jnp.float64)\n",
    "y = jnp.array(y, dtype=jnp.float64)\n",
    "train_data = Dataset(X=X, y=y)\n",
    "\n",
    "oq = np.array([np.mean(m[\"oq\"]) for m in meta])\n",
    "n_eff = int(np.sum(~np.isnan(X), axis=1).mean())\n",
    "\n",
    "print(\"Number of training waveforms:\", N_TRAIN)\n",
    "print(\"Average number samples per waveform:\", n_eff)\n",
    "print(\"Padding width (max waveform length):\", WIDTH)\n",
    "\n",
    "occupancy = (~np.isnan(y)).sum() / np.prod(y.shape)\n",
    "print(\n",
    "    f\"Data occupancy after padding at WIDTH={WIDTH}: {100 * (1 - occupancy):.2f}%\"\n",
    ")\n",
    "\n",
    "qs = np.quantile(np.nanmax(X, axis=1), [0.25, 0.5, 0.75, 0.95])\n",
    "print(f\"Quantiles of covered cycles at WIDTH={WIDTH}:\", qs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# STAGE 1: PRISM (COLLAPSED SVI)\n",
    "# Learn a global basis for the variably sized data\n",
    "# which then defines a map for the latent space of the BGPLVM\n",
    "# Secret sauce: \"batching\" complete waveforms via masking\n",
    "# ELBO factorizes over independent waveforms\n",
    "##############################################################\n",
    "batch_size = 32\n",
    "num_iters = 10_000\n",
    "lr = 1e-3\n",
    "jitter = 1e-4\n",
    "\n",
    "\n",
    "def trainable(path, v):\n",
    "    if not isinstance(v, Parameter):\n",
    "        return False\n",
    "    # path is usually a tuple of names; make this robust\n",
    "    leaf = path[-1] if isinstance(path, (tuple, list)) and path else str(path)\n",
    "    if leaf == \"sigma_a\":\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def collapsed_svi(key=vk(), d=d, J=1, M=M):\n",
    "    \"\"\"Memory cost is O(B M W) where W == WIDTH\"\"\"\n",
    "    Z = init_Z_inverse_ecdf(key, M, X)\n",
    "\n",
    "    if am == \"rbf\":\n",
    "        modulation = RBF()\n",
    "    elif am == \"rationalquadratic\":\n",
    "        modulation = RationalQuadratic(variance=1.0, alpha=NonNegativeReal(1.0))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown amplitude modulation kernel: {am}\")\n",
    "\n",
    "    kernel = ProductKernel(\n",
    "        [\n",
    "            modulation,\n",
    "            NormalizedPACK(d=d, J=J, sigma_a=1.0),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prior = gpx.gps.Prior(kernel, Zero())\n",
    "    likelihood = Gaussian(num_datapoints=WIDTH)\n",
    "    posterior = prior * likelihood\n",
    "\n",
    "    return CollapsedVariationalGaussian(\n",
    "        posterior=posterior, inducing_inputs=Z, jitter=jitter\n",
    "    )\n",
    "\n",
    "\n",
    "master_key, subkey = jax.random.split(master_key)\n",
    "\n",
    "from prism.svi import optimize as optimize_svi\n",
    "\n",
    "with time_this() as svi_timer:\n",
    "    qsvi, history = optimize_svi(\n",
    "        subkey,\n",
    "        collapsed_svi,\n",
    "        train_data,\n",
    "        lr,\n",
    "        batch_size,\n",
    "        num_iters,\n",
    "        trainable=trainable,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(\n",
    "    history,\n",
    "    title=\"ELBO during training (best run)\",\n",
    "    labels={\"x\": \"Iteration\", \"y\": \"ELBO\"},\n",
    ").show()\n",
    "\n",
    "print(\"Observation sigma_noise:\", qsvi.posterior.likelihood.obs_stddev)\n",
    "print(\n",
    "    \"Learned AM lengthscale:\",\n",
    "    qsvi.posterior.prior.kernel.kernels[0].lengthscale,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and inspect the global SVI basis found\n",
    "def psi(t):\n",
    "    return svi_basis(qsvi, t)\n",
    "\n",
    "\n",
    "tau_test = jnp.linspace(0, 7, 512)\n",
    "Psi_test = jax.vmap(psi)(tau_test)  # test indices\n",
    "\n",
    "master_key, subkey = jax.random.split(master_key)\n",
    "\n",
    "eps = jax.random.normal(subkey, shape=(M, 5))\n",
    "y = Psi_test @ eps\n",
    "\n",
    "px.line(y).update_traces(x=tau_test).update_layout(\n",
    "    xaxis_title=\"tau\",\n",
    "    yaxis_title=\"u'(t)\",\n",
    "    title=\"Prior samples of learned latent function distribution\",\n",
    ").show()\n",
    "# This is a prior draw from the learned RKHS subspace, not data-like yet.\n",
    "# It answers: What does a typical GP draw look like under the learned kernel?\n",
    "# expected to look generic and smooth\n",
    "\n",
    "px.line(Psi_test).update_traces(x=tau_test).update_layout(\n",
    "    xaxis_title=\"tau\",\n",
    "    yaxis_title=\"psi_m(t)\",\n",
    "    title=\"Learned basis functions psi_m(t)\",\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the training data via the learned SVI basis (PRISM)\n",
    "from prism.svi import do_prism_scan\n",
    "\n",
    "mu_eps, Sigma_eps = do_prism_scan(qsvi, train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we reconstruct waveforms from the SVI latent space?\n",
    "from prism.svi import reconstruct_waveforms\n",
    "\n",
    "test_indices = jnp.array([10, 100, 250, 500])\n",
    "\n",
    "reconstruct_waveforms(mu_eps, qsvi, train_data, test_indices, tau_test).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# STAGE 2: B-GP-LVM\n",
    "# Dimensionality reduction in the learned SVI basis space\n",
    "# Secret sauce: noisy data via diagonal covariances only\n",
    "#########################################################\n",
    "\n",
    "# Global whitening transform to get near-diagonal matrices for our modified BGPLVM algorithm\n",
    "from prism.svi import make_whitener\n",
    "\n",
    "whiten, unwhiten = make_whitener(mu_eps, Sigma_eps)\n",
    "\n",
    "mu_eps_whitened, Sigma_eps_whitened = whiten(mu_eps, Sigma_eps)\n",
    "\n",
    "diag_eps_whitened = jnp.diagonal(Sigma_eps_whitened, axis1=1, axis2=2)\n",
    "\n",
    "offdiag = offdiag_energy_fraction(Sigma_eps_whitened)\n",
    "print(\"Whitened offdiag energy fraction:\", offdiag)\n",
    "\n",
    "# extract diagonal only\n",
    "diag_eps_whitened = jnp.diagonal(\n",
    "    Sigma_eps_whitened, axis1=1, axis2=2\n",
    ")  # (N, num_inducing_svi)\n",
    "\n",
    "# just a hack to get means and vars to model via Dataset\n",
    "dataset_bgplvm = Dataset(X=diag_eps_whitened, y=mu_eps_whitened)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize via PCA\n",
    "X_mean_init = pca_reduce(mu_eps_whitened, Q)\n",
    "X_var_init = np.ones((mu_eps_whitened.shape[0], Q))\n",
    "\n",
    "\n",
    "def bayesian_gplvm(key=vk(), num_inducing_bgplvm=32, Q=Q, jitter=1e-4):\n",
    "    lengthscale = jnp.ones((Q,))\n",
    "    kernel = gpx.kernels.RBF(lengthscale=lengthscale)\n",
    "\n",
    "    permutation = jax.random.permutation(key, X_mean_init.shape[0])\n",
    "    Z = X_mean_init[permutation[:num_inducing_bgplvm]]\n",
    "\n",
    "    return BayesianGPLVM(\n",
    "        kernel, X_mu=X_mean_init, X_var=X_var_init, Z=Z, jitter=jitter\n",
    "    )\n",
    "\n",
    "\n",
    "# Can get trapped early so restarts are needed here (no batching so no noise; restarts just init positions of inducing inputs)\n",
    "# Because of OOM and time pressure we keep restarts at the meta \"iteration\" level\n",
    "\n",
    "num_iters = 15_000\n",
    "lr = 5e-2\n",
    "num_restarts = 1\n",
    "\n",
    "master_key, subkey = jax.random.split(master_key)\n",
    "\n",
    "\n",
    "from lvm.bgplvm import optimize as optimize_bgplvm\n",
    "\n",
    "optimize_bgplvm = partial(\n",
    "    optimize_bgplvm,\n",
    "    model=bayesian_gplvm,\n",
    "    dataset=dataset_bgplvm,\n",
    "    lr=lr,\n",
    "    num_iters=num_iters,\n",
    ")\n",
    "\n",
    "with time_this() as lvm_timer:\n",
    "    states, elbo_histories = optimize_restarts_scan(\n",
    "        optimize_bgplvm, num_restarts, subkey\n",
    "    )\n",
    "\n",
    "qlvm, history = pick_best(states, elbo_histories, bayesian_gplvm())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(\n",
    "    history,\n",
    "    title=\"ELBO during training (best run)\",\n",
    "    labels={\"x\": \"Iteration\", \"y\": \"ELBO\"},\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# export\n",
    "svi_walltime = svi_timer.walltime\n",
    "svi_obs_std = float(qsvi.posterior.likelihood.obs_stddev)\n",
    "svi_am_lengthscale = float(qsvi.posterior.prior.kernel.kernels[0].lengthscale)\n",
    "\n",
    "lvm_walltime = lvm_timer.walltime\n",
    "lvm_obs_std = np.sqrt(qlvm.sigma2)\n",
    "\n",
    "inverse_lengthscale = 1.0 / np.array(qlvm.kernel.lengthscale)\n",
    "\n",
    "print(\"Learned noise std:\", lvm_obs_std)\n",
    "print(\"Average data std:\", np.std(dataset_bgplvm.y, axis=0).mean())\n",
    "print(\"Inverse lengthscales:\", inverse_lengthscale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(\n",
    "    x=np.arange(Q),\n",
    "    y=inverse_lengthscale,\n",
    "    title=\"Inverse lengthscales by latent dimension\",\n",
    "    labels={\"x\": \"Latent dimension\", \"y\": \"Inverse lengthscale\"},\n",
    ").show()\n",
    "\n",
    "print(\"Inferred sqrt(variance) of random point:\")\n",
    "print(np.sqrt(qlvm.X_var[0, :]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3 = np.argsort(-inverse_lengthscale)[:3]\n",
    "\n",
    "pairs = list(combinations(top3, 2))\n",
    "\n",
    "showdensity = False\n",
    "showscatter = True\n",
    "\n",
    "if Q >= 2:\n",
    "    pair_plots_oq(qlvm, pairs, showdensity, showscatter, oq).show()\n",
    "\n",
    "if Q >= 3:\n",
    "    single_plot_oq(qlvm, top3, oq).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# STAGE 3: GMM in latent space of BGPLVM\n",
    "# Learn a density mode via local full-covariance Gaussians\n",
    "# Secret sauce: XD-GMM handles input uncertainties\n",
    "# Secret sauce #2: background component handles outliers\n",
    "#########################################################\n",
    "from lvm.xdgmm import fit_xdgmm\n",
    "\n",
    "# Load test data for STAGE 5\n",
    "X_test, Y_test, meta_test = get_data(\n",
    "    n=N_TEST, offset=N_TRAIN, width=WIDTH, with_metadata=True\n",
    ")\n",
    "\n",
    "oq_test = np.array([np.mean(m[\"oq\"]) for m in meta_test])\n",
    "\n",
    "with jax.default_device(jax.devices(\"cpu\")[0]):\n",
    "    # Map through PRISM\n",
    "    Psi_test = np.array(jax.vmap(jax.vmap(psi))(X_test))\n",
    "    mask_test = ~np.isnan(X_test)\n",
    "\n",
    "    f_list = []\n",
    "    Psi_list = []\n",
    "\n",
    "    for mask, Y, Psi in zip(mask_test, Y_test, Psi_test):\n",
    "        f_list.append(Y[mask])\n",
    "        Psi_list.append(Psi[mask])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_K(K, key=vk()):\n",
    "    print(\"**************************************\")\n",
    "    print(f\"qGPLVM with K={K} components\")\n",
    "    print(\"**************************************\")\n",
    "\n",
    "    m = qlvm.X_mu\n",
    "    S = jax.vmap(jnp.diag)(qlvm.X_var)\n",
    "\n",
    "    gmm, history = fit_xdgmm(m, S, K, verbose=True, n_iter=500)\n",
    "\n",
    "    for fig in xd_gmm_plots(gmm, history, qlvm, top3):\n",
    "        fig.show()\n",
    "\n",
    "    # Draw samples from the latent GMM and push **pointwise** through the BGPLVM map to data space\n",
    "    # works for any grid, any resolution, any duration\n",
    "    plvm = qlvm.build_posterior(dataset_bgplvm.y)\n",
    "\n",
    "    sample_latent_gmm_pointwise(gmm, plvm, psi, tau_test, unwhiten).show()\n",
    "\n",
    "    #########################################################\n",
    "    # STAGE 4: Push GMM components via linearized BGPLVM map\n",
    "    # to get extremely low rank GMM in data space; each component\n",
    "    # defines a low-rank GP learned from data\n",
    "    #########################################################\n",
    "    qgp = make_qgpvlm(gmm, plvm, psi, whiten, unwhiten)\n",
    "\n",
    "    plot_cluster_means_in_data_space(qgp, tau_test).show()\n",
    "\n",
    "    plot_cluster_samples_in_data_space(key, qgp, tau_test, nsamples=9).show()\n",
    "\n",
    "    #########################################################\n",
    "    # STAGE 5: Evaluate surrogate GMM likelihood on test set\n",
    "    # p(f | tau) = sum_k pi_k N(f | Psi(tau) mu_k, Psi Sigma_k Psi^T + sigma_obs^2 I)\n",
    "    #########################################################\n",
    "    neff = np.sum(~np.isnan(Y_test), axis=1)\n",
    "\n",
    "    log_prob_gmm = loglikelihood_on_test(\n",
    "        qgp,\n",
    "        f_list=f_list,\n",
    "        Psi_list=Psi_list,\n",
    "        obs_std=qsvi.posterior.likelihood.obs_stddev,\n",
    "        noise_floor=np.sqrt(NOISE_FLOOR_POWER),\n",
    "    )\n",
    "\n",
    "    lp = log_prob_gmm / neff\n",
    "    mean_loglike_test = np.mean(lp)\n",
    "    std_loglike_test = np.std(lp)\n",
    "\n",
    "    plot_oq_vs_loglik(oq_test, lp).show()\n",
    "    spearman = oq_sensitivity_spearman(oq_test, log_prob_gmm)\n",
    "\n",
    "    print(\n",
    "        f\"[K={K}] Average log likelihood per effective data point on test set: {mean_loglike_test} +/- {std_loglike_test}\"\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"[K={K}] OQ sensitivity (Spearman):\",\n",
    "        spearman[\"oq_sensitivity\"],\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"qgp\": qgp,\n",
    "        \"K\": K,\n",
    "        \"mean_loglike_test\": mean_loglike_test,\n",
    "        \"std_loglike_test\": std_loglike_test,\n",
    "        \"oq_sensitivity\": spearman[\"oq_sensitivity\"],\n",
    "        \"oq_sensitivity_p\": spearman[\"oq_sensitivity_p\"],\n",
    "    }\n",
    "\n",
    "\n",
    "def dump_models(results):\n",
    "    payload = {\n",
    "        \"qsvi\": qsvi,\n",
    "        \"qgp\": {r[\"K\"]: r.pop(\"qgp\") for r in results},\n",
    "    }\n",
    "    dump_egg(payload, os.getenv(\"EXPERIMENT_NOTEBOOK_REL\"))\n",
    "    return results\n",
    "\n",
    "\n",
    "KS = [1, 2, 4, 8, 16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# export\n",
    "results = dump_models(\n",
    "    [\n",
    "        process_K(K, subkey)\n",
    "        for K, subkey in zip(KS, jax.random.split(master_key, len(KS)))\n",
    "    ]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
