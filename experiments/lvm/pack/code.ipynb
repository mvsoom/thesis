{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters",
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters, export\n",
    "M = 8  # Number of PRISM basis functions\n",
    "Q = 1  # Latent dimensionality of qBGPLVM\n",
    "iteration = 1\n",
    "seed = 2455473317\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from itertools import combinations\n",
    "\n",
    "import gpjax as gpx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from gpjax.dataset import Dataset\n",
    "from gpjax.likelihoods import Gaussian\n",
    "from gpjax.mean_functions import Zero\n",
    "from gpjax.variational_families import CollapsedVariationalGaussian\n",
    "\n",
    "from lvm.bgplvm import BayesianGPLVM\n",
    "from lvm.plots import (\n",
    "    pair_plots_oq,\n",
    "    plot_cluster_means_in_data_space,\n",
    "    plot_cluster_samples_in_data_space,\n",
    "    plot_logl_histogram,\n",
    "    sample_latent_gmm_pointwise,\n",
    "    single_plot_oq,\n",
    "    xd_gmm_plots,\n",
    ")\n",
    "from lvm.qgpvlm import loglikelihood_on_test, make_qgpvlm\n",
    "from prism.pack import NormalizedPACK\n",
    "from prism.svi import (\n",
    "    do_prism,\n",
    "    init_Z_grid,\n",
    "    offdiag_energy_fraction,\n",
    "    optimize_restarts_scan,\n",
    "    pick_best,\n",
    "    svi_basis,\n",
    ")\n",
    "from surrogate.prism import get_test_data, get_train_data\n",
    "from utils import nats_to_ban, time_this\n",
    "from utils.constants import NOISE_FLOOR_POWER\n",
    "from utils.jax import pca_reduce, vk\n",
    "\n",
    "master_key = jax.random.key(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of independent waveforms to process train/test\n",
    "N_TRAIN = 3000\n",
    "N_TEST = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, oq = get_train_data(n=N_TRAIN)\n",
    "train_data = Dataset(X=X, y=y)\n",
    "\n",
    "_, WIDTH_TRAIN = X.shape\n",
    "n_eff = int(np.sum(~np.isnan(X), axis=1).mean())\n",
    "\n",
    "print(\"Number of training waveforms:\", N_TRAIN)\n",
    "print(\"Average number samples per waveform:\", n_eff)\n",
    "print(\"Padding width (max waveform length):\", WIDTH_TRAIN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# STAGE 1: PRISM (COLLAPSED SVI)\n",
    "# Learn a global basis for the variably sized data\n",
    "# which then defines a map for the latent space of the BGPLVM\n",
    "# Secret sauce: \"batching\" complete waveforms via masking\n",
    "# ELBO factorizes over independent waveforms\n",
    "##############################################################\n",
    "batch_size = 256\n",
    "num_iters = 5000\n",
    "lr = 1e-3\n",
    "jitter = 1e-4\n",
    "\n",
    "\n",
    "def collapsed_svi(key=vk(), d=1, J=1, M=M):\n",
    "    Z = init_Z_grid(key, M)\n",
    "\n",
    "    kernel = NormalizedPACK(d=d, J=J)\n",
    "    prior = gpx.gps.Prior(kernel, Zero())\n",
    "    likelihood = Gaussian(num_datapoints=WIDTH_TRAIN)\n",
    "    posterior = prior * likelihood\n",
    "\n",
    "    return CollapsedVariationalGaussian(\n",
    "        posterior=posterior, inducing_inputs=Z, jitter=jitter\n",
    "    )\n",
    "\n",
    "\n",
    "master_key, subkey = jax.random.split(master_key)\n",
    "\n",
    "from prism.svi import optimize as optimize_svi\n",
    "\n",
    "with time_this() as svi_timer:\n",
    "    qsvi, history = optimize_svi(\n",
    "        subkey, collapsed_svi, train_data, lr, batch_size, num_iters\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(\n",
    "    history,\n",
    "    title=\"ELBO during training (best run)\",\n",
    "    labels={\"x\": \"Iteration\", \"y\": \"ELBO\"},\n",
    ").show()\n",
    "\n",
    "print(\"Observation sigma_noise:\", qsvi.posterior.likelihood.obs_stddev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and inspect the global SVI basis found\n",
    "def psi(t):\n",
    "    return svi_basis(qsvi, t)\n",
    "\n",
    "\n",
    "tau_test = jnp.linspace(0, 2, 500)\n",
    "Psi_test = jax.vmap(psi)(tau_test)  # test indices\n",
    "\n",
    "master_key, subkey = jax.random.split(master_key)\n",
    "\n",
    "eps = jax.random.normal(subkey, shape=(M, 5))\n",
    "y = Psi_test @ eps\n",
    "\n",
    "px.line(y).update_traces(x=tau_test).update_layout(\n",
    "    xaxis_title=\"tau\",\n",
    "    yaxis_title=\"u'(t)\",\n",
    "    title=\"Prior samples of learned latent function distribution\",\n",
    ").show()\n",
    "# This is a prior draw from the learned RKHS subspace, not data-like yet.\n",
    "# It answers: What does a typical GP draw look like under the learned kernel?\n",
    "# expected to look generic and smooth\n",
    "\n",
    "px.line(Psi_test).update_traces(x=tau_test).update_layout(\n",
    "    xaxis_title=\"tau\",\n",
    "    yaxis_title=\"psi_m(t)\",\n",
    "    title=\"Learned basis functions psi_m(t)\",\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the training data via the learned SVI basis (PRISM)\n",
    "mu_eps, Sigma_eps = do_prism(qsvi, train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we reconstruct waveforms from the SVI latent space?\n",
    "from prism.svi import reconstruct_waveforms\n",
    "\n",
    "test_indices = jnp.array([10, 100, 250, 500])\n",
    "\n",
    "reconstruct_waveforms(mu_eps, qsvi, train_data, test_indices, tau_test).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# STAGE 2: B-GP-LVM\n",
    "# Dimensionality reduction in the learned SVI basis space\n",
    "# Secret sauce: noisy data via diagonal covariances only\n",
    "#########################################################\n",
    "\n",
    "# Global whitening transform to get near-diagonal matrices for our modified BGPLVM algorithm\n",
    "from prism.svi import make_whitener\n",
    "\n",
    "whiten, unwhiten = make_whitener(mu_eps, Sigma_eps)\n",
    "\n",
    "mu_eps_whitened, Sigma_eps_whitened = whiten(mu_eps, Sigma_eps)\n",
    "\n",
    "diag_eps_whitened = jnp.diagonal(Sigma_eps_whitened, axis1=1, axis2=2)\n",
    "\n",
    "offdiag = offdiag_energy_fraction(Sigma_eps_whitened)\n",
    "print(\"Whitened offdiag energy fraction:\", offdiag)\n",
    "\n",
    "# extract diagonal only\n",
    "diag_eps_whitened = jnp.diagonal(\n",
    "    Sigma_eps_whitened, axis1=1, axis2=2\n",
    ")  # (N, num_inducing_svi)\n",
    "\n",
    "# just a hack to get means and vars to model via Dataset\n",
    "dataset_bgplvm = Dataset(X=diag_eps_whitened, y=mu_eps_whitened)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize via PCA\n",
    "X_mean_init = pca_reduce(mu_eps_whitened, Q)\n",
    "X_var_init = np.ones((N_TRAIN, Q))\n",
    "\n",
    "\n",
    "def bayesian_gplvm(key=vk(), num_inducing_bgplvm=32, Q=Q, jitter=1e-4):\n",
    "    lengthscale = jnp.ones((Q,))\n",
    "    kernel = gpx.kernels.RBF(lengthscale=lengthscale)\n",
    "\n",
    "    permutation = jax.random.permutation(key, X_mean_init.shape[0])\n",
    "    Z = X_mean_init[permutation[:num_inducing_bgplvm]]\n",
    "\n",
    "    return BayesianGPLVM(\n",
    "        kernel, X_mu=X_mean_init, X_var=X_var_init, Z=Z, jitter=jitter\n",
    "    )\n",
    "\n",
    "\n",
    "# Can get trapped early so restarts are needed here (no batching so no noise; restarts just init positions of inducing inputs)\n",
    "# Because of OOM and time pressure we keep restarts at the meta \"iteration\" level\n",
    "\n",
    "num_iters = 15_000\n",
    "lr = 1e-2\n",
    "num_restarts = 1\n",
    "\n",
    "master_key, subkey = jax.random.split(master_key)\n",
    "\n",
    "\n",
    "from lvm.bgplvm import optimize as optimize_bgplvm\n",
    "\n",
    "optimize_bgplvm = partial(\n",
    "    optimize_bgplvm,\n",
    "    model=bayesian_gplvm,\n",
    "    dataset=dataset_bgplvm,\n",
    "    lr=lr,\n",
    "    num_iters=num_iters,\n",
    ")\n",
    "\n",
    "with time_this() as lvm_timer:\n",
    "    states, elbo_histories = optimize_restarts_scan(\n",
    "        optimize_bgplvm, num_restarts, subkey\n",
    "    )\n",
    "\n",
    "qlvm, history = pick_best(states, elbo_histories, bayesian_gplvm())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(\n",
    "    history,\n",
    "    title=\"ELBO during training (best run)\",\n",
    "    labels={\"x\": \"Iteration\", \"y\": \"ELBO\"},\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# export\n",
    "svi_walltime = svi_timer.walltime\n",
    "svi_obs_std = float(qsvi.posterior.likelihood.obs_stddev)\n",
    "\n",
    "lvm_walltime = lvm_timer.walltime\n",
    "lvm_obs_std = np.sqrt(qlvm.sigma2)\n",
    "\n",
    "inverse_lengthscale = 1.0 / np.array(qlvm.kernel.lengthscale)\n",
    "\n",
    "print(\"Learned noise std:\", lvm_obs_std)\n",
    "print(\"Average data std:\", np.std(dataset_bgplvm.y, axis=0).mean())\n",
    "print(\"Inverse lengthscales:\", inverse_lengthscale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(\n",
    "    x=np.arange(Q),\n",
    "    y=inverse_lengthscale,\n",
    "    title=\"Inverse lengthscales by latent dimension\",\n",
    "    labels={\"x\": \"Latent dimension\", \"y\": \"Inverse lengthscale\"},\n",
    ").show()\n",
    "\n",
    "print(\"Inferred sqrt(variance) of random point:\")\n",
    "print(np.sqrt(qlvm.X_var[0, :]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3 = np.argsort(-inverse_lengthscale)[:3]\n",
    "\n",
    "pairs = list(combinations(top3, 2))\n",
    "\n",
    "showdensity = False\n",
    "showscatter = True\n",
    "\n",
    "if Q >= 2:\n",
    "    pair_plots_oq(qlvm, pairs, showdensity, showscatter, oq).show()\n",
    "\n",
    "if Q >= 3:\n",
    "    single_plot_oq(qlvm, top3, oq).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# STAGE 3: GMM in latent space of BGPLVM\n",
    "# Learn a density mode via local full-covariance Gaussians\n",
    "# Secret sauce: XD-GMM handles input uncertainties\n",
    "# Secret sauce #2: background component handles outliers\n",
    "#########################################################\n",
    "from lvm.xdgmm import fit_xdgmm\n",
    "\n",
    "# Load test data for STAGE 5\n",
    "X_test, Y_test, log_prob_u = get_test_data(n=N_TEST, offset=N_TRAIN)\n",
    "\n",
    "# Map through PRISM\n",
    "Psi_test = np.array(jax.vmap(jax.vmap(psi))(X_test))\n",
    "mask_test = ~np.isnan(X_test)\n",
    "\n",
    "f_list = []\n",
    "Psi_list = []\n",
    "\n",
    "for mask, Y, Psi in zip(mask_test, Y_test, Psi_test):\n",
    "    f_list.append(Y[mask])\n",
    "    Psi_list.append(Psi[mask])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_K(K, key=vk()):\n",
    "    print(\"**************************************\")\n",
    "    print(f\"qGPLVM with K={K} components\")\n",
    "    print(\"**************************************\")\n",
    "\n",
    "    m = qlvm.X_mu\n",
    "    S = jax.vmap(jnp.diag)(qlvm.X_var)\n",
    "\n",
    "    gmm, history = fit_xdgmm(m, S, K, verbose=True, n_iter=500)\n",
    "\n",
    "    for fig in xd_gmm_plots(gmm, history, qlvm, top3):\n",
    "        fig.show()\n",
    "\n",
    "    # Draw samples from the latent GMM and push **pointwise** through the BGPLVM map to data space\n",
    "    # works for any grid, any resolution, any duration\n",
    "    plvm = qlvm.build_posterior(dataset_bgplvm.y)\n",
    "\n",
    "    sample_latent_gmm_pointwise(gmm, plvm, psi, tau_test, unwhiten).show()\n",
    "\n",
    "    #########################################################\n",
    "    # STAGE 4: Push GMM components via linearized BGPLVM map\n",
    "    # to get extremely low rank GMM in data space; each component\n",
    "    # defines a low-rank GP learned from data\n",
    "    #########################################################\n",
    "    qgp = make_qgpvlm(gmm, plvm, psi, whiten, unwhiten)\n",
    "\n",
    "    plot_cluster_means_in_data_space(qgp, tau_test).show()\n",
    "\n",
    "    plot_cluster_samples_in_data_space(key, qgp, tau_test, nsamples=9).show()\n",
    "\n",
    "    #########################################################\n",
    "    # STAGE 5: Evaluate surrogate GMM likelihood on test set\n",
    "    # p(f | tau) = sum_k pi_k N(f | Psi(tau) mu_k, Psi Sigma_k Psi^T + sigma_obs^2 I)\n",
    "    #########################################################\n",
    "    log_prob_gmm = loglikelihood_on_test(\n",
    "        qgp,\n",
    "        f_list=f_list,\n",
    "        Psi_list=Psi_list,\n",
    "        obs_std=qsvi.posterior.likelihood.obs_stddev,\n",
    "        noise_floor=np.sqrt(NOISE_FLOOR_POWER),\n",
    "    )\n",
    "\n",
    "    plot_logl_histogram(\n",
    "        log_prob_gmm,\n",
    "        log_prob_u,\n",
    "        n_eff,\n",
    "        K,\n",
    "    ).show()\n",
    "\n",
    "    mean_gmm_loglikelihood = log_prob_gmm.mean()\n",
    "    mean_lf_loglikelihood = log_prob_u.mean()\n",
    "\n",
    "    D_KL = mean_lf_loglikelihood - mean_gmm_loglikelihood\n",
    "\n",
    "    print(\n",
    "        f\"[K={K}] Average log likelihood per sample (LF model):\",\n",
    "        mean_lf_loglikelihood / n_eff,\n",
    "    )\n",
    "    print(\n",
    "        f\"[K={K}] Average log likelihood per sample (GMM model)\",\n",
    "        mean_gmm_loglikelihood / n_eff,\n",
    "    )\n",
    "\n",
    "    print(f\"[K={K}] D_KL (nats):\", D_KL)\n",
    "    print(f\"[K={K}] D_KL (bans):\", nats_to_ban(D_KL))\n",
    "    print(f\"[K={K}] D_KL (bans/sample):\", nats_to_ban(D_KL) / n_eff)\n",
    "\n",
    "    return {\n",
    "        \"K\": K,\n",
    "        \"mean_gmm_loglikelihood\": mean_gmm_loglikelihood,\n",
    "        \"mean_lf_loglikelihood\": mean_lf_loglikelihood,\n",
    "        \"D_KL_bans_per_sample\": nats_to_ban(D_KL) / n_eff,\n",
    "    }\n",
    "\n",
    "\n",
    "KS = [1, 2, 4, 8, 16, 32, 64]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# export\n",
    "results = [\n",
    "    process_K(K, subkey)\n",
    "    for K, subkey in zip(KS, jax.random.split(master_key, len(KS)))\n",
    "]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
