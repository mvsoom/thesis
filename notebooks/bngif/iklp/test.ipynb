{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iklp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iklp import gig, hyperparams, mercer_op, periodic, state, util, vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.random\n",
    "\n",
    "hyperparams.random_periodic_kernel_hyperparams(jax.random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "from bngif.iklp import build_Psi, build_X\n",
    "\n",
    "from utils.plotting import iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/NeilGirdhar/efax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from numpy.random import randn\n",
    "\n",
    "M, P = 2048, 30\n",
    "a = randn(P)\n",
    "x = jnp.linspace(1.0, 6.0, M)\n",
    "\n",
    "Psi = build_Psi(M, a)\n",
    "X = build_X(x, P)\n",
    "\n",
    "Psi.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infinite Kernel Linear Prediction (IKLP) Variational Inference Implementation (Yoshii & Goto 2013)\n",
    "# pip install jax jaxlib\n",
    "# pip install scipy\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.linalg import solve_triangular\n",
    "import scipy.special as sp\n",
    "\n",
    "\n",
    "def build_X_matrix(x, p):\n",
    "    \"\"\"\n",
    "    Build design matrix X for AR of order p (Eq. (3) structure).\n",
    "    X is N×p where X[n,m] = x[n-m-1] for m < n (zero-padded for n <= m).\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    if p == 0:\n",
    "        return jnp.zeros((N, 0))\n",
    "    # Use Toeplitz structure: each column is a shifted version of x\n",
    "    cols = []\n",
    "    for m in range(1, p + 1):\n",
    "        # Pad m zeros at start, then take x[0:N-m] for remainder\n",
    "        col = jnp.concatenate([jnp.zeros(m), x[:-m]])\n",
    "        cols.append(col)\n",
    "    X = jnp.stack(cols, axis=1)\n",
    "    return X\n",
    "\n",
    "\n",
    "def iklp_vi_naive(\n",
    "    x, a, K_list, p, aw, bw, ae, be, alpha, max_iter=100, tol=1e-6\n",
    "):\n",
    "    \"\"\"\n",
    "    Naive IKLP Variational Inference.\n",
    "    Uses explicit matrix operations as in the paper (Equations (15)–(27)).\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    I = len(K_list)\n",
    "    # Initialize variational parameters (posteriors) and AR coefficients\n",
    "    # a = jnp.zeros(p)  # AR filter coefficients (point estimate, MAP)\n",
    "    # Initialize expectations for theta (one per kernel) from prior (Eq. (16))\n",
    "    E_theta = jnp.full((I,), 1.0 / I)  # E[θ_i] ≈ 1/I initially\n",
    "    E_inv_theta = (\n",
    "        1.0 / E_theta\n",
    "    )  # E[1/θ_i] (approx reciprocal of mean to avoid infinities)\n",
    "    # Initialize expectations for ν_w, ν_e from priors (Eq. (17))\n",
    "    E_nu_w = aw / bw\n",
    "    E_inv_nu_w = (\n",
    "        (aw - 1) / bw if aw > 1 else 1.0 / E_nu_w\n",
    "    )  # if shape<=1, use reciprocal of mean as approximation\n",
    "    E_nu_e = ae / be\n",
    "    E_inv_nu_e = (ae - 1) / be if ae > 1 else 1.0 / E_nu_e\n",
    "    I_N = jnp.eye(N)\n",
    "    elbo_history = []\n",
    "    last_elbo = -jnp.inf\n",
    "    for it in range(max_iter):\n",
    "        # Eq. (24): Compute Omega = E[ν_w] * sum_i E[θ_i] K_i + E[ν_e] I (covariance of x after filtering)\n",
    "        Omega = (\n",
    "            E_nu_w * sum(E_theta[i] * K_list[i] for i in range(I))\n",
    "            + E_nu_e * I_N\n",
    "        )\n",
    "        Omega_inv = jnp.linalg.inv(Omega)  # explicit inverse (naive)\n",
    "        # Eq. (25): Compute S = ∑_i (E[1/(ν_w θ_i)])^{-1} K_i + (E[1/ν_e])^{-1} I\n",
    "        # Note: E[1/(ν_w θ_i)] = E[1/ν_w]*E[1/θ_i] (due to factorization of q)\n",
    "        S = (\n",
    "            sum(\n",
    "                (1.0 / (E_inv_nu_w * E_inv_theta[i])) * K_list[i]\n",
    "                for i in range(I)\n",
    "            )\n",
    "            + (1.0 / E_inv_nu_e) * I_N\n",
    "        )\n",
    "        S_inv = jnp.linalg.inv(S)\n",
    "        # Compute residual e = Ψ x (using lower-triangular Toeplitz Ψ, Eq. (3))\n",
    "        # e = x + X a, where X is as defined for AR (past samples matrix)\n",
    "        X = build_X_matrix(x, p)\n",
    "        e = x + X @ a\n",
    "        # Solve for u = S^{-1} e  (use explicit inverse for naive)\n",
    "        u = S_inv @ e\n",
    "        # Compute trace terms and quadratic terms for each kernel\n",
    "        tr_vals = jnp.array(\n",
    "            [jnp.trace(Omega_inv @ K) for K in K_list]\n",
    "        )  # tr(Omega^{-1} K_i)\n",
    "        uKu_vals = jnp.array([u @ (K @ u) for K in K_list])  # u^T K_i u\n",
    "        tr_I = jnp.trace(Omega_inv)  # tr(Omega^{-1} I) for identity\n",
    "        # Eq. (27): Update variational posterior parameters (GIG distributions) for θ and ν\n",
    "        # Posterior q(θ_i): γ_i, ρ_i, τ_i\n",
    "        gamma_theta = alpha / I\n",
    "        rho_theta = 2 * alpha + E_nu_w * tr_vals\n",
    "        # E[1/(ν_w θ_i)] = E[1/ν_w] * E[1/θ_i]\n",
    "        E_inv_nu_theta = E_inv_nu_w * E_inv_theta  # vector of E[1/(ν_w θ_i)]\n",
    "        tau_theta = E_inv_nu_w * (1.0 / (E_inv_nu_theta**2)) * uKu_vals\n",
    "        # Posterior q(ν_w): γ_w, ρ_w, τ_w\n",
    "        gamma_nu_w = aw\n",
    "        rho_nu_w = 2 * bw + jnp.sum(E_theta * tr_vals)\n",
    "        tau_nu_w = jnp.sum(E_inv_theta * (1.0 / (E_inv_nu_theta**2)) * uKu_vals)\n",
    "        # Posterior q(ν_e): γ_e, ρ_e, τ_e\n",
    "        gamma_nu_e = ae\n",
    "        rho_nu_e = 2 * be + tr_I\n",
    "        tau_nu_e = (1.0 / (E_inv_nu_e**2)) * (u @ u)\n",
    "        # Update expectations E[...] and E[1/...] using moments of GIG distributions (via Bessel K functions)\n",
    "        sqrt_rho_tau_theta = jnp.sqrt(rho_theta * tau_theta)\n",
    "        # Use SciPy for Bessel K: K_ν(z)\n",
    "        rho_tau_np = jnp.array(sqrt_rho_tau_theta)\n",
    "        K_base = sp.kv(gamma_theta, rho_tau_np)\n",
    "        K_p1 = sp.kv(gamma_theta + 1.0, rho_tau_np)\n",
    "        K_m1 = sp.kv(gamma_theta - 1.0, rho_tau_np)\n",
    "        # E[θ_i] = √(τ_i/ρ_i) * K_{γ_i+1}(√(ρ_i τ_i)) / K_{γ_i}(√(ρ_i τ_i))\n",
    "        # E[1/θ_i] = √(ρ_i/τ_i) * K_{γ_i-1}(√(ρ_i τ_i)) / K_{γ_i}(√(ρ_i τ_i))\n",
    "        E_theta = jnp.sqrt(tau_theta / rho_theta) * (\n",
    "            jnp.array(K_p1) / jnp.array(K_base)\n",
    "        )\n",
    "        E_inv_theta = jnp.sqrt(rho_theta / tau_theta) * (\n",
    "            jnp.array(K_m1) / jnp.array(K_base)\n",
    "        )\n",
    "        # E[ν_w] and E[1/ν_w]\n",
    "        sqrt_rho_tau_nu_w = float(jnp.sqrt(rho_nu_w * tau_nu_w))\n",
    "        K_base = float(sp.kv(gamma_nu_w, sqrt_rho_tau_nu_w))\n",
    "        K_p1 = float(sp.kv(gamma_nu_w + 1.0, sqrt_rho_tau_nu_w))\n",
    "        K_m1 = float(sp.kv(gamma_nu_w - 1.0, sqrt_rho_tau_nu_w))\n",
    "        E_nu_w = jnp.sqrt(tau_nu_w / rho_nu_w) * (K_p1 / K_base)\n",
    "        E_inv_nu_w = jnp.sqrt(rho_nu_w / tau_nu_w) * (K_m1 / K_base)\n",
    "        # E[ν_e] and E[1/ν_e]\n",
    "        sqrt_rho_tau_nu_e = float(jnp.sqrt(rho_nu_e * tau_nu_e))\n",
    "        K_base = float(sp.kv(gamma_nu_e, sqrt_rho_tau_nu_e))\n",
    "        K_p1 = float(sp.kv(gamma_nu_e + 1.0, sqrt_rho_tau_nu_e))\n",
    "        K_m1 = float(sp.kv(gamma_nu_e - 1.0, sqrt_rho_tau_nu_e))\n",
    "        E_nu_e = jnp.sqrt(tau_nu_e / rho_nu_e) * (K_p1 / K_base)\n",
    "        E_inv_nu_e = jnp.sqrt(rho_nu_e / tau_nu_e) * (K_m1 / K_base)\n",
    "        # Update AR coefficients a via MAP (solve regularized normal equation after Eq. (27))\n",
    "        # (X^T Ω^{-1} X + λ I) a = X^T Ω^{-1} x, using current Omega\n",
    "        lam = 1e-6  # small regularization (λ from prior a ~ N(0, λ I))\n",
    "        A = X.T @ (Omega_inv @ X)\n",
    "        b = X.T @ (Omega_inv @ x)\n",
    "        A_reg = A + lam * jnp.eye(p)\n",
    "        L_mat = jnp.linalg.cholesky(A_reg)\n",
    "        y_tmp = solve_triangular(L_mat, b, lower=True)\n",
    "        a = solve_triangular(L_mat.T, y_tmp, lower=False)\n",
    "        # Compute evidence lower bound (ELBO) L for convergence tracking (Eq. (19) with approximations)\n",
    "        M = N\n",
    "        # First term: E[log p(x|θ,a,ν)] (use bound via Eq. (23) as computed above)\n",
    "        logdet_Omega = jnp.log(jnp.linalg.det(Omega) + 1e-12)\n",
    "        term1 = -0.5 * (\n",
    "            M * jnp.log(2 * jnp.pi)\n",
    "            + logdet_Omega\n",
    "            + jnp.sum(E_nu_w * E_theta * tr_vals)\n",
    "            + E_nu_e * tr_I\n",
    "            + jnp.sum(E_inv_nu_w * E_inv_theta * uKu_vals)\n",
    "            + E_inv_nu_e * (u @ u)\n",
    "        )\n",
    "        # Prior terms: E[log p(θ)] + E[log p(ν_w)] + E[log p(ν_e)] + E[log p(a)]\n",
    "        E_log_theta = jnp.log(E_theta + 1e-12)\n",
    "        term2 = jnp.sum(\n",
    "            (alpha / I - 1.0) * E_log_theta - alpha * E_theta\n",
    "        )  # θ prior (Gamma(α/I, α))\n",
    "        E_log_nu_w = jnp.log(E_nu_w + 1e-12)\n",
    "        term3 = (aw - 1.0) * E_log_nu_w - bw * E_nu_w  # ν_w prior\n",
    "        E_log_nu_e = jnp.log(E_nu_e + 1e-12)\n",
    "        term4 = (ae - 1.0) * E_log_nu_e - be * E_nu_e  # ν_e prior\n",
    "        term5 = -0.5 * (\n",
    "            lam * (a @ a) - p * jnp.log(lam + 1e-12)\n",
    "        )  # a prior (Gaussian N(0, λ I))\n",
    "        # Entropy terms: E[log q(θ,...)] (omitted for brevity or treated via L')\n",
    "        L_elbo = term1 + term2 + term3 + term4 + term5\n",
    "        elbo_history.append(L_elbo)\n",
    "        # Check convergence\n",
    "        if jnp.abs(L_elbo - last_elbo) < tol:\n",
    "            break\n",
    "        last_elbo = L_elbo\n",
    "    return {\n",
    "        \"a\": a,\n",
    "        \"E_theta\": E_theta,\n",
    "        \"E_inv_theta\": E_inv_theta,\n",
    "        \"E_nu_w\": E_nu_w,\n",
    "        \"E_inv_nu_w\": E_inv_nu_w,\n",
    "        \"E_nu_e\": E_nu_e,\n",
    "        \"E_inv_nu_e\": E_inv_nu_e,\n",
    "        \"ELBO_history\": jnp.array(elbo_history),\n",
    "    }\n",
    "\n",
    "\n",
    "def iklp_vi_efficient(\n",
    "    x, a, K_list, p, aw, bw, ae, be, alpha, max_iter=100, tol=1e-6\n",
    "):\n",
    "    \"\"\"\n",
    "    Efficient IKLP Variational Inference.\n",
    "    Uses Cholesky decompositions and avoids explicit large-matrix inverses (Woodbury/solves for stability & speed).\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    I = len(K_list)\n",
    "    # Initialize parameters and AR coefficients as before\n",
    "    # a = jnp.zeros(p)\n",
    "    E_theta = jnp.full((I,), 1.0 / I)\n",
    "    E_inv_theta = 1.0 / E_theta\n",
    "    E_nu_w = aw / bw\n",
    "    E_inv_nu_w = (aw - 1) / bw if aw > 1 else 1.0 / E_nu_w\n",
    "    E_nu_e = ae / be\n",
    "    E_inv_nu_e = (ae - 1) / be if ae > 1 else 1.0 / E_nu_e\n",
    "    I_N = jnp.eye(N)\n",
    "    elbo_history = []\n",
    "    last_elbo = -jnp.inf\n",
    "    for it in range(max_iter):\n",
    "        # Compute Omega and its Cholesky (more stable than explicit inverse)\n",
    "        Omega = (\n",
    "            E_nu_w * sum(E_theta[i] * K_list[i] for i in range(I))\n",
    "            + E_nu_e * I_N\n",
    "        )\n",
    "        L = jnp.linalg.cholesky(Omega)  # lower-triangular Cholesky of Omega\n",
    "        # Compute S and solve S u = e by Cholesky (avoid forming S_inv explicitly)\n",
    "        S = (\n",
    "            sum(\n",
    "                (1.0 / (E_inv_nu_w * E_inv_theta[i])) * K_list[i]\n",
    "                for i in range(I)\n",
    "            )\n",
    "            + (1.0 / E_inv_nu_e) * I_N\n",
    "        )\n",
    "        Ls = jnp.linalg.cholesky(S)\n",
    "        # Compute residual e = Ψ x (Toeplitz convolution, Eq. (3))\n",
    "        X = build_X_matrix(x, p)\n",
    "        e = x + X @ a\n",
    "        # Solve S u = e via forward/backward substitution (Cholesky solves)\n",
    "        y = solve_triangular(Ls, e, lower=True)\n",
    "        u = solve_triangular(Ls.T, y, lower=False)\n",
    "        # Compute trace and quadratic terms\n",
    "        # For trace, we use explicit inverse or solve for stability (still O(N^3), but stable)\n",
    "        Omega_inv = jnp.linalg.inv(\n",
    "            Omega\n",
    "        )  # could be replaced with iterative solves if needed\n",
    "        tr_vals = jnp.array([jnp.trace(Omega_inv @ K) for K in K_list])\n",
    "        uKu_vals = jnp.array([u @ (K @ u) for K in K_list])\n",
    "        tr_I = jnp.trace(Omega_inv)\n",
    "        # Variational parameter updates (same as naive)\n",
    "        gamma_theta = alpha / I\n",
    "        rho_theta = 2 * alpha + E_nu_w * tr_vals\n",
    "        E_inv_nu_theta = E_inv_nu_w * E_inv_theta\n",
    "        tau_theta = E_inv_nu_w * (1.0 / (E_inv_nu_theta**2)) * uKu_vals\n",
    "        gamma_nu_w = aw\n",
    "        rho_nu_w = 2 * bw + jnp.sum(E_theta * tr_vals)\n",
    "        tau_nu_w = jnp.sum(E_inv_theta * (1.0 / (E_inv_nu_theta**2)) * uKu_vals)\n",
    "        gamma_nu_e = ae\n",
    "        rho_nu_e = 2 * be + tr_I\n",
    "        tau_nu_e = (1.0 / (E_inv_nu_e**2)) * (u @ u)\n",
    "        # Update expectations via Bessel K functions (GIG moments)\n",
    "        sqrt_rho_tau_theta = jnp.sqrt(rho_theta * tau_theta)\n",
    "        rho_tau_np = jnp.array(sqrt_rho_tau_theta)\n",
    "        K_base = sp.kv(gamma_theta, rho_tau_np)\n",
    "        K_p1 = sp.kv(gamma_theta + 1.0, rho_tau_np)\n",
    "        K_m1 = sp.kv(gamma_theta - 1.0, rho_tau_np)\n",
    "        E_theta = jnp.sqrt(tau_theta / rho_theta) * (\n",
    "            jnp.array(K_p1) / jnp.array(K_base)\n",
    "        )\n",
    "        E_inv_theta = jnp.sqrt(rho_theta / tau_theta) * (\n",
    "            jnp.array(K_m1) / jnp.array(K_base)\n",
    "        )\n",
    "        sqrt_rho_tau_nu_w = float(jnp.sqrt(rho_nu_w * tau_nu_w))\n",
    "        K_base = float(sp.kv(gamma_nu_w, sqrt_rho_tau_nu_w))\n",
    "        K_p1 = float(sp.kv(gamma_nu_w + 1.0, sqrt_rho_tau_nu_w))\n",
    "        K_m1 = float(sp.kv(gamma_nu_w - 1.0, sqrt_rho_tau_nu_w))\n",
    "        E_nu_w = jnp.sqrt(tau_nu_w / rho_nu_w) * (K_p1 / K_base)\n",
    "        E_inv_nu_w = jnp.sqrt(rho_nu_w / tau_nu_w) * (K_m1 / K_base)\n",
    "        sqrt_rho_tau_nu_e = float(jnp.sqrt(rho_nu_e * tau_nu_e))\n",
    "        K_base = float(sp.kv(gamma_nu_e, sqrt_rho_tau_nu_e))\n",
    "        K_p1 = float(sp.kv(gamma_nu_e + 1.0, sqrt_rho_tau_nu_e))\n",
    "        K_m1 = float(sp.kv(gamma_nu_e - 1.0, sqrt_rho_tau_nu_e))\n",
    "        E_nu_e = jnp.sqrt(tau_nu_e / rho_nu_e) * (K_p1 / K_base)\n",
    "        E_inv_nu_e = jnp.sqrt(rho_nu_e / tau_nu_e) * (K_m1 / K_base)\n",
    "        # Update AR coefficients a (solve weighted least squares via Cholesky, avoiding explicit Omega_inv)\n",
    "        # Solve L z = X and L y_vec = x for z and y_vec\n",
    "        z = solve_triangular(L, X, lower=True)\n",
    "        y_vec = solve_triangular(L, x, lower=True)\n",
    "        A_small = z.T @ z\n",
    "        b_small = z.T @ y_vec\n",
    "        lam = 1e-6\n",
    "        A_reg = A_small + lam * jnp.eye(p)\n",
    "        L_small = jnp.linalg.cholesky(A_reg)\n",
    "        y_tmp = solve_triangular(L_small, b_small, lower=True)\n",
    "        a = solve_triangular(L_small.T, y_tmp, lower=False)\n",
    "        # Compute ELBO (L') for monitoring (similar to naive computation)\n",
    "        M = N\n",
    "        logdet_Omega = 2 * jnp.sum(jnp.log(jnp.diag(L) + 1e-12))  # log|Omega|\n",
    "        term1 = -0.5 * (\n",
    "            M * jnp.log(2 * jnp.pi)\n",
    "            + logdet_Omega\n",
    "            + jnp.sum(E_nu_w * E_theta * tr_vals)\n",
    "            + E_nu_e * tr_I\n",
    "            + jnp.sum(E_inv_nu_w * E_inv_theta * uKu_vals)\n",
    "            + E_inv_nu_e * (u @ u)\n",
    "        )\n",
    "        E_log_theta = jnp.log(E_theta + 1e-12)\n",
    "        term2 = jnp.sum((alpha / I - 1.0) * E_log_theta - alpha * E_theta)\n",
    "        E_log_nu_w = jnp.log(E_nu_w + 1e-12)\n",
    "        term3 = (aw - 1.0) * E_log_nu_w - bw * E_nu_w\n",
    "        E_log_nu_e = jnp.log(E_nu_e + 1e-12)\n",
    "        term4 = (ae - 1.0) * E_log_nu_e - be * E_nu_e\n",
    "        term5 = -0.5 * (lam * (a @ a) - p * jnp.log(lam + 1e-12))\n",
    "        L_elbo = term1 + term2 + term3 + term4 + term5\n",
    "        elbo_history.append(L_elbo)\n",
    "        if jnp.abs(L_elbo - last_elbo) < tol:\n",
    "            break\n",
    "        last_elbo = L_elbo\n",
    "    return {\n",
    "        \"a\": a,\n",
    "        \"E_theta\": E_theta,\n",
    "        \"E_inv_theta\": E_inv_theta,\n",
    "        \"E_nu_w\": E_nu_w,\n",
    "        \"E_inv_nu_w\": E_inv_nu_w,\n",
    "        \"E_nu_e\": E_nu_e,\n",
    "        \"E_inv_nu_e\": E_inv_nu_e,\n",
    "        \"ELBO_history\": jnp.array(elbo_history),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iklp_vi_naive(x, K_list, p, aw, bw, ae, be, alpha, max_iter=100, tol=1e-6):\n",
    "\n",
    "from math import sqrt\n",
    "from numpy.random import randn\n",
    "\n",
    "M = 20\n",
    "P = 4\n",
    "_lambda = 0.1\n",
    "\n",
    "\n",
    "def randK():\n",
    "    R = randn(M, M)\n",
    "    R = R @ R.T\n",
    "    return R\n",
    "\n",
    "\n",
    "x = randn(M)\n",
    "a = randn(P) * sqrt(_lambda)\n",
    "K_list = [randK() for _ in range(4)]\n",
    "p = P\n",
    "aw = 1.0\n",
    "bw = 1.0\n",
    "ae = 1.0\n",
    "be = 1.0\n",
    "alpha = 1.0\n",
    "max_iter = 20\n",
    "tol = 1e-6\n",
    "\n",
    "# Run the naive version\n",
    "result_naive = iklp_vi_naive(\n",
    "    x, a, K_list, p, aw, bw, ae, be, alpha, max_iter=max_iter, tol=tol\n",
    ")\n",
    "\n",
    "iplot(\n",
    "    result_naive[\"ELBO_history\"][:-2],\n",
    "    title=\"Naive IKLP Variational Inference ELBO\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.kv??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
