{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bngif.gig import moments\n",
    "from jax import grad, jit\n",
    "\n",
    "moments = jit(moments)\n",
    "moments(1.0, 3.0, 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moments(1.0, 3.0, 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_gig_moments.py\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "import scipy.stats as sp\n",
    "\n",
    "tfm = tfp.math  # --- our helpers --------------------------------------\n",
    "\n",
    "\n",
    "def _log_Kv(v, z):\n",
    "    return tfm.log_bessel_kve(v, z) - jnp.abs(z)\n",
    "\n",
    "\n",
    "def _Kv_ratio(v, z):\n",
    "    return jnp.exp(tfm.log_bessel_kve(v + 1, z) - tfm.log_bessel_kve(v, z))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def _dlogK_dv(v, z, eps=1e-4):\n",
    "    return (_log_Kv(v + eps, z) - _log_Kv(v - eps, z)) / (2.0 * eps)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def gig_moments(p, a, b):\n",
    "    z = jnp.sqrt(a * b)\n",
    "    r = _Kv_ratio(p, z)\n",
    "    mean_x = jnp.sqrt(b / a) * r\n",
    "    mean_invx = jnp.sqrt(a / b) * r - 2.0 * p / b\n",
    "    mean_logx = 0.5 * (jnp.log(b) - jnp.log(a)) + _dlogK_dv(p, z)\n",
    "    return mean_x, mean_invx, mean_logx\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1)   Compare against SciPy’s implementation   (a == b  ⇔ SciPy case)\n",
    "# ---------------------------------------------------------------------\n",
    "def test_against_scipy_geninvgauss():\n",
    "    rng = np.random.default_rng(0)\n",
    "    for _ in range(20):\n",
    "        p = rng.uniform(1.2, 5.0)  # mean exists for p>1\n",
    "        b = rng.uniform(0.1, 6.0)\n",
    "\n",
    "        mean_jax, inv_jax, log_jax = gig_moments(p, b, b)\n",
    "\n",
    "        # SciPy gives mean directly;  ⟨1/X⟩ and ⟨log X⟩ via .expect\n",
    "        scipy_rv = sp.geninvgauss(p, b)\n",
    "        mean_ref = scipy_rv.mean()\n",
    "        inv_ref = scipy_rv.expect(lambda x: 1.0 / x)\n",
    "        log_ref = scipy_rv.expect(np.log)\n",
    "\n",
    "        print(\n",
    "            f\"p={p:.2f}, b={b:.2f}  |  \"\n",
    "            f\"mean={mean_jax:.4f}, inv={inv_jax:.4f}, log={log_jax:.4f}  |  \"\n",
    "            f\"ref_mean={mean_ref:.4f}, ref_inv={inv_ref:.4f}, ref_log={log_ref:.4f}\"\n",
    "        )\n",
    "\n",
    "        assert np.allclose(mean_jax, mean_ref, rtol=1e-11, atol=1e-13)\n",
    "        assert np.allclose(inv_jax, inv_ref, rtol=1e-8, atol=1e-13)\n",
    "        assert np.allclose(log_jax, log_ref, rtol=1e-7, atol=1e-11)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2)   Inverse-Gaussian special case  (p = −½,  b = 1/μ,  scale = μ)\n",
    "#       E[X]  should be  μ\n",
    "# ---------------------------------------------------------------------\n",
    "def test_inverse_gaussian_mean():\n",
    "    mu = 2.0\n",
    "    p, b = -0.5, 1.0 / mu\n",
    "\n",
    "    # our implementation corresponds to \"scale = 1\"\n",
    "    mean_y, *_ = gig_moments(p, b, b)  # Y  ~ GIG scale=1\n",
    "    mean_x = mu * float(mean_y)  # X = μ Y\n",
    "\n",
    "    scipy_mean = sp.invgauss(mu).mean()\n",
    "\n",
    "    print(\n",
    "        f\"p={p:.2f}, b={b:.2f}  |  \"\n",
    "        f\"mean_y={mean_y:.4f}, mean_x={mean_x:.4f}  |  \"\n",
    "        f\"ref_mean={scipy_mean:.4f}\"\n",
    "    )\n",
    "\n",
    "    assert np.allclose(mean_x, scipy_mean, rtol=1e-11, atol=1e-13)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3)   Gamma limit:  b → 0⁺  ⇒  GIG(p,a,b) →  Gamma(k=p, θ=2/a)\n",
    "# ---------------------------------------------------------------------\n",
    "def test_gamma_limit_b_to_zero():\n",
    "    a, p = 3.4, 4.0  # shape k = p  (k>0 ensures mean exists)\n",
    "    b_small = 1e-7\n",
    "    mean_gig, *_ = gig_moments(p, a, b_small)\n",
    "\n",
    "    mean_gamma = sp.gamma.mean(a=p, scale=2.0 / a)\n",
    "\n",
    "    # First-order limit error is O(b), so 1e-4 on the relative scale is fine\n",
    "    print(\n",
    "        f\"p={p:.2f}, a={a:.2f}  |  mean_gig={mean_gig:.4f}, mean_gamma={mean_gamma:.4f}\"\n",
    "    )\n",
    "\n",
    "    assert np.allclose(mean_gig, mean_gamma, rtol=1e-4)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4)   Reciprocal property:   If X~GIG(p,a,b)  ⇒  1/X ~ GIG(−p,b,a)\n",
    "# ---------------------------------------------------------------------\n",
    "def test_reciprocal_identity():\n",
    "    p, a, b = 1.8, 2.0, 0.7\n",
    "    mean_x, mean_invx, _ = gig_moments(p, a, b)\n",
    "\n",
    "    mean_x_recip, *_ = gig_moments(-p, b, a)\n",
    "\n",
    "    print(\n",
    "        f\"p={p:.2f}, a={a:.2f}, b={b:.2f}  |  \"\n",
    "        f\"mean_x={mean_x:.4f}, mean_invx={mean_invx:.4f}  |  \"\n",
    "        f\"mean_x_recip={mean_x_recip:.4f}\"\n",
    "    )\n",
    "\n",
    "    assert np.allclose(mean_invx, mean_x_recip, rtol=1e-12, atol=1e-14)\n",
    "\n",
    "\n",
    "test_against_scipy_geninvgauss()\n",
    "test_inverse_gaussian_mean()\n",
    "test_gamma_limit_b_to_zero()\n",
    "test_reciprocal_identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gig.py  ──────────────────────────────────────────────────────────────\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "\n",
    "tfm = tfp.math  # for bessel_kve, log_bessel_kve\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "#   Core special-function helpers (all JAX, all GPU/TPU-safe)\n",
    "# ---------------------------------------------------------------------\n",
    "def _log_Kv(v: jnp.ndarray, z: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"log K_v(z) via the scaled TFP kernel:  log K = log kve − |z|.\"\"\"\n",
    "    return tfm.log_bessel_kve(v, z) - jnp.abs(z)\n",
    "\n",
    "\n",
    "def _Kv_ratio(v: jnp.ndarray, z: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"K_{v+1}(z) / K_v(z) in log-space for stability.\"\"\"\n",
    "    return jnp.exp(tfm.log_bessel_kve(v + 1, z) - tfm.log_bessel_kve(v, z))\n",
    "\n",
    "\n",
    "def _dlogK_dv(v: jnp.ndarray, z: jnp.ndarray, eps: float = 1e-4) -> jnp.ndarray:\n",
    "    \"\"\"∂/∂v log K_v(z) (central finite difference, AD-compatible).\"\"\"\n",
    "    return (_log_Kv(v + eps, z) - _log_Kv(v - eps, z)) / (2 * eps)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "#   Public: closed-form expectations for any broadcast-shape p,a,b\n",
    "# ---------------------------------------------------------------------\n",
    "@jax.jit\n",
    "def gig_moments(\n",
    "    p: jnp.ndarray, a: jnp.ndarray, b: jnp.ndarray\n",
    ") -> tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
    "    r\"\"\"E[X], E[1/X], E[log X]  for  GIG(p, a, b).\"\"\"\n",
    "    z = jnp.sqrt(a * b)\n",
    "    r = _Kv_ratio(p, z)\n",
    "\n",
    "    mean_x = jnp.sqrt(b / a) * r\n",
    "    mean_invx = jnp.sqrt(a / b) * r - 2.0 * p / b\n",
    "    mean_logx = 0.5 * (jnp.log(b) - jnp.log(a)) + _dlogK_dv(p, z)\n",
    "    return mean_x, mean_invx, mean_logx\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "#   Differential entropy  H(X)  =  −E[log f(X)]\n",
    "# ---------------------------------------------------------------------\n",
    "@jax.jit\n",
    "def gig_entropy(p: jnp.ndarray, a: jnp.ndarray, b: jnp.ndarray) -> jnp.ndarray:\n",
    "    r\"\"\"Differential entropy of  X ∼ GIG(p,a,b).\"\"\"\n",
    "    z = jnp.sqrt(a * b)\n",
    "    # log normalizer   log Z = log 2 + log K_p(z) − (p/2)[log a − log b]\n",
    "    log_Z = jnp.log(2.0) + _log_Kv(p, z) - 0.5 * p * (jnp.log(a) - jnp.log(b))\n",
    "\n",
    "    mean_x, mean_invx, mean_logx = gig_moments(p, a, b)\n",
    "    H = log_Z - (p - 1.0) * mean_logx + 0.5 * (a * mean_x + b * mean_invx)\n",
    "    return H\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "#   Optional convenience: make each GIG a small PyTree leaf\n",
    "# ---------------------------------------------------------------------\n",
    "@jax.tree_util.register_pytree_node_class\n",
    "@dataclass\n",
    "class GIG:\n",
    "    p: jnp.ndarray  # shape-broadcastable\n",
    "    a: jnp.ndarray  # >0\n",
    "    b: jnp.ndarray  # >0\n",
    "\n",
    "    # — PyTree interface —\n",
    "    def tree_flatten(self):\n",
    "        return ((self.p, self.a, self.b), None)\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux, children):\n",
    "        return cls(*children)\n",
    "\n",
    "    # — lightweight methods —\n",
    "    def moments(self):\n",
    "        return gig_moments(self.p, self.a, self.b)\n",
    "\n",
    "    def entropy(self):\n",
    "        return gig_entropy(self.p, self.a, self.b)\n",
    "\n",
    "    def to_scipy(self):\n",
    "        p_val = self.p\n",
    "        b_scipy = jnp.sqrt(self.a * self.b)\n",
    "        scale_val = jnp.sqrt(self.b / self.a)\n",
    "\n",
    "        return sp.geninvgauss(p_val, b_scipy, scale=scale_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# A nested pytree of independent GIG variables\n",
    "z = {\n",
    "    \"nu_w\": GIG(p=2.0, a=3.0, b=4.0),\n",
    "    \"nu_e\": GIG(p=2.0, a=3.0, b=4.0),\n",
    "    \"theta\": [GIG(p=jnp.linspace(-0.4, 0.4, 30), a=1.0, b=3.0)],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# A nested pytree of independent GIG variables\n",
    "z = {\n",
    "    \"nu_w\": GIG(p=2.0, a=3.0, b=4.0),\n",
    "    \"nu_e\": GIG(p=2.0, a=3.0, b=4.0),\n",
    "    \"theta\": [\n",
    "        GIG(p=jnp.linspace(-0.4, 0.4, 20), a=1.0, b=jnp.array([2.0, 3.0])[:, None])\n",
    "    ],\n",
    "}\n",
    "\n",
    "is_gig = lambda x: isinstance(x, GIG)\n",
    "\n",
    "func = jax.jit(\n",
    "    lambda t: jax.tree_util.tree_map(\n",
    "        lambda d: d.entropy(),  # Runs inside JIT\n",
    "        t,\n",
    "        is_leaf=is_gig,\n",
    "    )  # <-- Don't unflatten GIG\n",
    ")\n",
    "\n",
    "tree_entropy = func(z)\n",
    "\n",
    "tree_entropy[\"theta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append right after the `entropy` method inside the GIG class\n",
    "import scipy.stats as sp  # put at top of file with other imports\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "#  convert to a frozen scipy.stats.geninvgauss distribution\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "\n",
    "gig_scalar = GIG(p=1.0, a=100.0, b=40.0)\n",
    "\n",
    "# Get the frozen SciPy object\n",
    "rv = gig_scalar.to_scipy()\n",
    "\n",
    "print(rv.mean(), rv.var())\n",
    "# consistency check\n",
    "mx, _, _ = gig_scalar.moments()\n",
    "print(mx)\n",
    "\n",
    "print(rv.entropy())\n",
    "print(gig_scalar.entropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scope():\n",
    "    x = rv.rvs(100000)\n",
    "    return x.mean(), (1 / x).mean(), jnp.log(x).mean()\n",
    "\n",
    "\n",
    "print(scope())\n",
    "\n",
    "gig_scalar.moments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
