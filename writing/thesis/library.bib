@misc{Abadi2015,
  title = {{{TensorFlow}}: {{Large-scale}} Machine Learning on Heterogeneous Systems},
  author = {Abadi, Mart{\'i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'e}, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'e}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2015}
}

@article{Abbe2017,
  title = {Community Detection and Stochastic Block Models: Recent Developments},
  author = {Abbe, Emmanuel},
  year = {2017},
  eprint = {1703.10146},
  abstract = {The stochastic block model (SBM) is a random graph model with planted clusters. It is widely employed as a canonical model to study clustering and community detection, and provides generally a fertile ground to study the statistical and computational tradeoffs that arise in network and data sciences. This note surveys the recent developments that establish the fundamental limits for community detection in the SBM, both with respect to information-theoretic and computational thresholds, and for various recovery requirements such as exact, partial and weak recovery (a.k.a., detection). The main results discussed are the phase transitions for exact recovery at the Chernoff-Hellinger threshold, the phase transition for weak recovery at the Kesten-Stigum threshold, the optimal distortion-SNR tradeoff for partial recovery, the learning of the SBM parameters and the gap between information-theoretic and computational thresholds. The note also covers some of the algorithms developed in the quest of achieving the limits, in particular two-round algorithms via graph-splitting, semi-definite programming, linearized belief propagation, classical and nonbacktracking spectral methods. A few open problems are also discussed.},
  archiveprefix = {arXiv},
  arxivid = {1703.10146},
  file = {/home/marnix/Zotero/storage/N8FU5KN3/Abbe - 2017 - Community detection and stochastic block models recent developments.pdf}
}

@article{Abdel-Haleem2004,
  title = {Acoustic Space Dimensionality Selection and Combination Using the Maximum Entropy Principle},
  author = {{Abdel-Haleem}, Yasser H and Renals, Steve and Lawrence, Neil D},
  year = {2004},
  journal = {Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP '04)},
  pages = {V-637----V--640},
  issn = {15206149},
  abstract = {We propose a discriminative approach to acoustic space dimensionality selection based on maximum entropy modelling. We form a set of constraints by composing the acoustic space with the space of phone classes, and use a continuous feature formulation of maximum entropy modelling to select an optimal feature set. The suggested approach has two steps: (1) the selection of the best acoustic space that efficiently and economically represents the acoustic data and its variability; (2) the combination of selected acoustic features in the maximum entropy framework to estimate the posterior probabilities over the phonetic labels given the acoustic input. Specific contributions of the paper include a parameter estimation algorithm (generalized improved iterative scaling) that enables the use of negative features, the parameterization of constraint functions using Gaussian mixture models, and experimental results using the TIMIT database.},
  isbn = {1520-6149},
  keywords = {acoustic signal processing,Gaussian processes,it},
  file = {/home/marnix/Zotero/storage/TLTUNVTE/Abdel-Haleem et al. - 2004 - Acoustic space dimensionality selection and combin.pdf}
}

@article{Abraham2011,
  title = {Dynamics--{{The Geometry}} of {{Behavior}}},
  author = {Abraham, Authors and Ralph, H and Frederick, D and Christopher, D and More, Learn},
  year = {2011},
  journal = {Spring},
  volume = {1},
  number = {0},
  pages = {2011},
  isbn = {9780201567175},
  file = {/home/marnix/Zotero/storage/7YD9PQFT/Abraham et al. - 2011 - Dynamics--The Geometry of Behavior.pdf}
}

@article{Abuelezam2019,
  title = {Interaction Patterns of Men Who Have Sex with Men on a Geosocial Networking Mobile App in Seven United States Metropolitan Areas: Observational Study},
  author = {Abuelezam, Nadia N and Reshef, Yakir A and Novak, David and Grad, Yonatan Hagai and Seage III, George R and Mayer, Kenneth and Lipsitch, Marc},
  year = {2019},
  journal = {Journal of Medical Internet Research},
  volume = {21},
  number = {9},
  pages = {e13766},
  publisher = {JMIR Publications Toronto, Canada}
}

@article{Acquisti2015,
  title = {Privacy and Human Behavior in the Age of Information},
  author = {Acquisti, Alessandro and Brandimarte, Laura and Loewenstein, George},
  year = {2015},
  month = jan,
  journal = {Science},
  volume = {347},
  number = {6221},
  pages = {509--514},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aaa1465},
  urldate = {2023-09-02},
  abstract = {This Review summarizes and draws connections between diverse streams of empirical research on privacy behavior. We use three themes to connect insights from social and behavioral sciences: people's uncertainty about the consequences of privacy-related behaviors and their own preferences over those consequences; the context-dependence of people's concern, or lack thereof, about privacy; and the degree to which privacy concerns are malleable---manipulable by commercial and governmental interests. Organizing our discussion by these themes, we offer observations concerning the role of public policy in the protection of privacy in the information age.},
  file = {/home/marnix/Zotero/storage/34PEX7CB/Acquisti et al. - 2015 - Privacy and human behavior in the age of informati.pdf}
}

@book{Acton1990,
  title = {Numerical Methods That Work},
  author = {Acton, Forman S},
  year = {1990},
  publisher = {Mathematical Association of America}
}

@book{Aczel1989,
  title = {Functional Equations in Several Variables},
  author = {Acz{\'e}l, J. and Dhombres, Jean G.},
  year = {1989},
  series = {Encyclopedia of Mathematics and Its Applications},
  number = {v. 31},
  publisher = {Cambridge University Press},
  address = {Cambridge [Cambridgeshire] ; New York},
  isbn = {978-0-521-35276-5},
  langid = {english},
  lccn = {QA431 .A334 1989},
  keywords = {Functional equations,Functions of several real variables},
  file = {/home/marnix/Zotero/storage/PRQYMDPK/Aczél and Dhombres - 1989 - Functional equations in several variables.pdf}
}

@article{Adams2007,
  title = {Bayesian {{Online Changepoint Detection}}},
  author = {Adams, Ryan Prescott and MacKay, David J. C.},
  year = {2007},
  month = oct,
  journal = {arXiv:0710.3742 [stat]},
  eprint = {0710.3742},
  primaryclass = {stat},
  urldate = {2020-04-26},
  abstract = {Changepoints are abrupt variations in the generative parameters of a data sequence. Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics. While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem. Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint. We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. Our implementation is highly modular so that the algorithm may be applied to a variety of types of data. We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/YYACTQLF/Adams and MacKay - 2007 - Bayesian Online Changepoint Detection.pdf;/home/marnix/Zotero/storage/DSQUVHPH/0710.html}
}

@misc{Agrell2019,
  title = {Gaussian Processes with Linear Operator Inequality Constraints},
  author = {Agrell, Christian},
  year = {2019},
  month = sep,
  number = {arXiv:1901.03134},
  eprint = {1901.03134},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-01-08},
  abstract = {This paper presents an approach for constrained Gaussian Process (GP) regression where we assume that a set of linear transformations of the process are bounded. It is motivated by machine learning applications for high-consequence engineering systems, where this kind of information is often made available from phenomenological knowledge. We consider a GP \$f\$ over functions on \${\textbackslash}mathcal\{X\} {\textbackslash}subset {\textbackslash}mathbb\{R\}{\textasciicircum}\{n\}\$ taking values in \${\textbackslash}mathbb\{R\}\$, where the process \${\textbackslash}mathcal\{L\}f\$ is still Gaussian when \${\textbackslash}mathcal\{L\}\$ is a linear operator. Our goal is to model \$f\$ under the constraint that realizations of \${\textbackslash}mathcal\{L\}f\$ are confined to a convex set of functions. In particular, we require that \$a {\textbackslash}leq {\textbackslash}mathcal\{L\}f {\textbackslash}leq b\$, given two functions \$a\$ and \$b\$ where \$a {$<$} b\$ pointwise. This formulation provides a consistent way of encoding multiple linear constraints, such as shape-constraints based on e.g. boundedness, monotonicity or convexity. We adopt the approach of using a sufficiently dense set of virtual observation locations where the constraint is required to hold, and derive the exact posterior for a conjugate likelihood. The results needed for stable numerical implementation are derived, together with an efficient sampling scheme for estimating the posterior process.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/Z38HEC4Y/Agrell - 2019 - Gaussian processes with linear operator inequality.pdf;/home/marnix/Zotero/storage/8M6UGASG/1901.html}
}

@article{Airaksinen2014,
  title = {Quasi {{Closed Phase Glottal Inverse Filtering Analysis With Weighted Linear Prediction}}},
  author = {Airaksinen, Manu and Raitio, Tuomo and Story, Brad and Alku, Paavo},
  year = {2014},
  month = mar,
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {22},
  number = {3},
  pages = {596--607},
  issn = {2329-9304},
  doi = {10.1109/TASLP.2013.2294585},
  abstract = {This study presents a new glottal inverse filtering (GIF) technique based on closed phase analysis over multiple fundamental periods. The proposed quasi closed phase (QCP) analysis method utilizes weighted linear prediction (WLP) with a specific attenuated main excitation (AME) weight function that attenuates the contribution of the glottal source in the linear prediction model optimization. This enables the use of the autocorrelation criterion in linear prediction in contrast to the covariance criterion used in conventional closed phase analysis. The QCP method was compared to previously developed methods by using synthetic vowels produced with the conventional source-filter model as well as with a physical modeling approach. The obtained objective measures show that the QCP method improves the GIF performance in terms of errors in typical glottal source parametrizations for both low- and high-pitched vowels. Additionally, QCP was tested in a physiologically oriented vocoder, where the analysis/synthesis quality was evaluated with a subjective listening test indicating improved perceived quality for normal speaking style.},
  keywords = {Accuracy,Closed phase analysis,Correlation,GIF,glottal inverse filtering,IEEE transactions,Production,Speech,speech analysis,Speech processing,Transfer functions,weighted linear prediction},
  file = {/home/marnix/Zotero/storage/G253CN65/Airaksinen et al. - 2014 - Quasi Closed Phase Glottal Inverse Filtering Analy.pdf;/home/marnix/Zotero/storage/IIICGWAI/6680604.html}
}

@article{Airaksinen2017,
  title = {Quadratic {{Programming Approach}} to {{Glottal Inverse Filtering}} by {{Joint Norm-1}} and {{Norm-2 Optimization}}},
  author = {Airaksinen, Manu and B{\"a}ckstr{\"o}m, Tom and Alku, Paavo},
  year = {2017},
  month = may,
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {25},
  number = {5},
  pages = {929--939},
  issn = {2329-9304},
  doi = {10.1109/TASLP.2016.2620718},
  abstract = {This study proposes an approach for glottal inverse filtering of acoustic speech signals using quadratic programming (QPR). The method aims to jointly model the effect of vocal tract and lip radiation with a single filter whose coefficients are optimized using QPR. This optimization is based on the principles of closed phase analysis, where the contribution of the glottal source is attenuated in optimizing the inverse model of the vocal tract. By expressing the optimization problem in terms of the output of a filter, we can apply physically motivated optimization such as flatness of the closed phase. The proposed method was objectively evaluated using a synthetic Liljencrants-Fant model based test set of sustained vowels, as well as a real speech test set where the glottal flow estimates' closed phases were compared in terms of their flatness. The results based on synthetic speech indicate that the proposed method is robust to changes in f0, and state-of-the-art quality results were obtained for high-pitched voices, when f0 is in the range 330-450 Hz. The results based on real speech indicate that the proposed method produces glottal flow estimates that have flatter closed phases with less formant ripple in comparison to estimates computed with known reference methods.},
  keywords = {Closed phase analysis,Computational modeling,glottal inverse filtering,IEEE transactions,Lips,Quadratic programming,quadratic programming (QPR),Speech,Speech processing},
  file = {/home/marnix/Zotero/storage/7J39IASS/Airaksinen et al. - 2017 - Quadratic Programming Approach to Glottal Inverse .pdf;/home/marnix/Zotero/storage/DJWH3UTQ/7636994.html}
}

@article{Akamatsu2015,
  title = {Some {{Thoughts}} on the {{Term}} '{{Signeme}}'},
  author = {Akamatsu, Tsutomu},
  year = {2015},
  journal = {Moenia},
  volume = {21},
  file = {/home/marnix/Zotero/storage/L8DCVDCG/Akamatsu - 2015 - Some thoughts on the term'signeme'.pdf;/home/marnix/Zotero/storage/4VH66PT4/2311.html}
}

@book{Akram2010,
  title = {Interbank Overnight Interest Rates - Gains from Systemic Importance},
  booktitle = {Norges Bank Working Paper},
  author = {Akram, Q. Farooq and Christophersen, Casper},
  year = {2010},
  abstract = {We study overnight interbank interest rates paid by banks in Norway over the period 2006-2009. We observe large variations in interest rates across banks and over time. During the financial crisis, the interest rates are found to be substantially below indicative quotes of interest rates provided by major banks. Our econometric model attributes the interest rate variation partly to differences in banks' characteristics including relative size and connectedness, implying favorable terms for banks of systemic importance. Moreover, interest rates are found to depend not only on overall liquidity in the interbank market, but possibly on its distribution among banks as well, suggesting exploitation of market power by banks with surplus liquidity. There is also evidence of stronger effects on interest rates of systemic importance, credit ratings and liquidity demand and supply since the start of the current financial crisis. [ABSTRACT FROM AUTHOR] Copyright of Norges Bank: Working Papers is the property of Norges Bank and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
  isbn = {978-92-5-108151-8},
  pmid = {53897569},
  file = {/home/marnix/Zotero/storage/6SZRGPYT/Akram, Christophersen - 2010 - Interbank overnight interest rates-gains from systemic importance.pdf}
}

@article{AlBawab2008,
  title = {Analysis-by-Synthesis Features for Speech Recognition},
  author = {Al Bawab, Z and Raj, B and Stern, R M},
  year = {2008},
  journal = {Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on},
  number = {March},
  pages = {4185--4188},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2008.4518577},
  isbn = {978-1-4244-1483-3},
  keywords = {Gaussian processes,speech recognition,speech synth},
  file = {/home/marnix/Zotero/storage/ANLNKBVQ/AlBawab2008 Analysis-by-synthesis features for speech recognition.pdf}
}

@article{Albert2000,
  title = {Error and Attack Tolerance of Complex Networks},
  author = {Albert, Reka and Jeong, Hawoong and Barabasi, Albert-Laszlo},
  year = {2000},
  month = jul,
  journal = {Nature},
  volume = {406},
  number = {6794},
  pages = {378--382},
  issn = {1476-4687},
  doi = {10.1038/35019019},
  abstract = {Many complex systems display a surprising degree of tolerance against errors. For example, relatively simple organisms grow, persist and reproduce despite drastic pharmaceutical or environmental interventions, an error tolerance attributed to the robustness of the underlying metabolic network. Complex communication networks display a surprising degree of robustness: although key components regularly malfunction, local failures rarely lead to the loss of the global information-carrying ability of the network. The stability of these and other complex systems is often attributed to the redundant wiring of the functional web defined by the systems' components. Here we demonstrate that error tolerance is not shared by all redundant systems: it is displayed only by a class of inhomogeneously wired networks, called scale-free networks, which include the \{World-Wide\} Web, the Internet, social networks and cells. We find that such networks display an unexpected degree of robustness, the ability of their nodes to communicate being unaffected even by unrealistically high failure rates. However, error tolerance comes at a high price in that these networks are extremely vulnerable to attacks (that is, to the selection and removal of a few nodes that play a vital role in maintaining the network's connectivity). Such error tolerance and attack vulnerability are generic properties of communication networks},
  pmid = {10935628},
  keywords = {error\{{\textbackslash}\_\}tolerance},
  file = {/home/marnix/Zotero/storage/68TPTGNJ/Albert, Jeong, Barabasi - 2000 - Error and attack tolerance of complex networks.pdf}
}

@article{Albert2002,
  title = {Statistical Mechanics of Complex Networks},
  author = {Albert, R{\'e}ka and Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  year = {2002},
  journal = {Reviews of Modern Physics},
  volume = {74},
  number = {1},
  eprintclass = {arXiv:cond-mat},
  pages = {47--97},
  issn = {0034-6861},
  doi = {10.1103/RevModPhys.74.47},
  abstract = {Complex networks describe a wide range of systems in nature and society. Frequently cited examples include the cell, a network of chemicals linked by chemical reactions, and the Internet, a network of routers and computers connected by physical links. While traditionally these systems have been modeled as random graphs, it is increasingly recognized that the topology and evolution of real networks are governed by robust organizing principles. This article reviews the recent advances in the field of complex networks, focusing on the statistical mechanics of network topology and dynamics. After reviewing the empirical data that motivated the recent interest in networks, the authors discuss the main models and analytical tools, covering random graphs, small-world and scale-free networks, the emerging theory of evolving networks, and the interplay between topology and the network's robustness against failures and attacks.},
  arxiv = {0106096v1},
  arxivid = {arXiv:cond-mat/0106096v1},
  isbn = {0034-6861},
  pmid = {16204838},
  keywords = {Complexity,Introduction,Network},
  file = {/home/marnix/Zotero/storage/JNZJPZXJ/Albert, Barabási - 2002 - Statistical mechanics of complex networks.pdf}
}

@article{Albert2019,
  title = {Gaussian Processes for Data Fulfilling Linear Differential Equations},
  author = {Albert, Christopher G.},
  year = {2019},
  month = sep,
  journal = {arXiv:1909.03447 [physics]},
  eprint = {1909.03447},
  primaryclass = {physics},
  urldate = {2020-09-16},
  abstract = {A method to reconstruct fields, source strengths and physical parameters based on Gaussian process regression is presented for the case where data are known to fulfill a given linear differential equation with localized sources. The approach is applicable to a wide range of data from physical measurements and numerical simulations. It is based on the well-known invariance of the Gaussian under linear operators, in particular differentiation. Instead of using a generic covariance function to represent data from an unknown field, the space of possible covariance functions is restricted to allow only Gaussian random fields that fulfil the homogeneous differential equation. The resulting tailored kernel functions lead to more reliable regression compared to using a generic kernel and makes some hyperparameters directly interpretable. For differential equations representing laws of physics such a choice limits realizations of random fields to physically possible solutions. Source terms are added by superposition and their strength estimated in a probabilistic fashion, together with possibly unknown hyperparameters with physical meaning in the differential operator.},
  archiveprefix = {arXiv},
  keywords = {{Physics - Data Analysis, Statistics and Probability}},
  file = {/home/marnix/Zotero/storage/7FSUN2DV/Albert - 2019 - Gaussian processes for data fulfilling linear diff.pdf;/home/marnix/Zotero/storage/UNJBZPUU/1909.html}
}

@article{Albert2020,
  title = {{{JAXNS}}: A High-Performance Nested Sampling Package Based on {{JAX}}},
  shorttitle = {{{JAXNS}}},
  author = {Albert, Joshua G.},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.15286 [astro-ph]},
  eprint = {2012.15286},
  primaryclass = {astro-ph},
  urldate = {2022-04-07},
  abstract = {Since its debut by John Skilling in 2004, nested sampling has proven a valuable tool to the scientist, providing hypothesis evidence calculations and parameter inference for complicated posterior distributions, particularly in the field of astronomy. Due to its computational complexity and long-running nature, in the past, nested sampling has been reserved for offline-type Bayesian inference, leaving tools such as variational inference and MCMC for online-type, time-constrained, Bayesian computations. These tools do not easily handle complicated multi-modal posteriors, discrete random variables, and posteriors lacking gradients, nor do they enable practical calculations of the Bayesian evidence. An opening thus remains for a high-performance out-of-the-box nested sampling package that can close the gap in computational time, and let nested sampling become common place in the data science toolbox. We present JAX-based nested sampling (JAXNS), a high-performance nested sampling package written in XLA-primitives using JAX, and show that it is several orders of magnitude faster than the currently available nested sampling implementations of PolyChord, MultiNEST, and dynesty, while maintaining the same accuracy of evidence calculation. The JAXNS package is publically available at {\textbackslash}url\{https://github.com/joshuaalbert/jaxns\}.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
  file = {/home/marnix/Zotero/storage/V6PTPC38/Albert - 2020 - JAXNS a high-performance nested sampling package .pdf;/home/marnix/Zotero/storage/SKJPBZCI/2012.html}
}

@misc{Albert2023,
  title = {Phantom-{{Powered Nested Sampling}}},
  author = {Albert, Joshua G.},
  year = {2023},
  month = dec,
  number = {arXiv:2312.11330},
  eprint = {2312.11330},
  primaryclass = {astro-ph, stat},
  publisher = {arXiv},
  urldate = {2024-02-17},
  abstract = {We introduce a novel technique within the Nested Sampling framework to enhance efficiency of the computation of Bayesian evidence, a critical component in scientific data analysis. In higher dimensions, Nested Sampling relies on Markov Chain-based likelihood-constrained prior samplers, which generate numerous 'phantom points' during parameter space exploration. These points are too auto-correlated to be used in the standard Nested Sampling scheme and so are conventionally discarded, leading to waste. Our approach discovers a way to integrate these phantom points into the evidence calculation, thereby improving the efficiency of Nested Sampling without sacrificing accuracy. This is achieved by ensuring the points within the live set remain asymptotically i.i.d. uniformly distributed, allowing these points to contribute meaningfully to the final evidence estimation. We apply our method on several models, demonstrating substantial enhancements in sampling efficiency, that scales well in high-dimension. Our findings suggest that this approach can reduce the number of required likelihood evaluations by at least a factor of 5. This advancement holds considerable promise for improving the robustness and speed of statistical analyses over a wide range of fields, from astrophysics and cosmology to climate modelling.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Statistics - Computation},
  file = {/home/marnix/Zotero/storage/NX8S3XDX/Albert - 2023 - Phantom-Powered Nested Sampling.pdf;/home/marnix/Zotero/storage/WUF7GZIJ/2312.html}
}

@article{Albuquerque2011,
  title = {Synthetic Generation of High-Dimensional Datasets},
  author = {Albuquerque, Georgia and Lowe, Thomas and Magnor, Marcus},
  year = {2011},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {17},
  number = {12},
  pages = {2317--2324},
  issn = {10772626},
  doi = {10.1109/TVCG.2011.237},
  abstract = {Generation of synthetic datasets is a common practice in many research areas. Such data is often generated to meet specific needs or certain conditions that may not be easily found in the original, real data. The nature of the data varies according to the application area and includes text, graphs, social or weather data, among many others. The common process to create such synthetic datasets is to implement small scripts or programs, restricted to small problems or to a specific application. In this paper we propose a framework designed to generate high dimensional datasets. Users can interactively create and navigate through multi dimensional datasets using a suitable graphical user-interface. The data creation is driven by statistical distributions based on a few user-defined parameters. First, a grounding dataset is created according to given inputs, and then structures and trends are included in selected dimensions and orthogonal projection planes. Furthermore, our framework supports the creation of complex non-orthogonal trends and classified datasets. It can successfully be used to create synthetic datasets simulating important trends as multidimensional clusters, correlations and outliers.},
  isbn = {1077-2626},
  pmid = {22034352},
  keywords = {high-dimensional data,interaction,multivariate data,Synthetic data generation},
  file = {/home/marnix/Zotero/storage/RWW23HQL/Albuquerque, Lowe, Magnor - 2011 - Synthetic generation of high-dimensional datasets.pdf}
}

@article{Aldasoro2016,
  title = {Multiplex Interbank Networks and Systemic Importance: {{An}} Application to {{European}} Data},
  author = {Aldasoro, I{\~n}aki and Alves, Iv{\'a}n},
  year = {2016},
  publisher = {SAFE Working Paper}
}

@article{Alku1992,
  title = {Glottal Wave Analysis with {{Pitch Synchronous Iterative Adaptive Inverse Filtering}}},
  author = {Alku, Paavo},
  year = {1992},
  volume = {11},
  abstract = {A new glottal wave analysis method, Pitch Synchronous lterative Adaptive Inverse Filtering (PSIAIF) is presented. The algorithm is based on a previously developed method, Iterative Adaptive Inverse Filtering (IAIF). In the IAIF-method the glottal contribution to the speech spectrum is first estimated with an iterative structure. The vocal tract transfer function is modeled after eliminating the average glottal contribution. The glottal excitation is obtained by cancelling the effects of the vocal tract and lip radiation by inverse filtering. In the new PSIAIF-method the glottal pulseform is computed by applying the IAIF-algorithm twice to the same signal. The first IAIF-analysis gives as a result a glottal excitation that spans over several pitch periods. This pulseform is used in order to determine positions and lengths of frames for the pitch synchronous analysis. The final result is obtained by analysing the original speech signal with the IAIF-algorithm one fundamental period at a time. The PSlAIF-algorithm was applied in glottal wave analysis using both synthetic and natural vowels. The results show that the method is able to give a fairly accurate estimate for the glottal flow excluding the analysis of vowels with a low first formant that are produced with a pressed phonation type.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/2ABAX2JN/Alku - 1992 - Glottal wave analysis with Pitch Synchronous Itera.pdf}
}

@article{Alku2002,
  title = {Normalized Amplitude Quotient for Parametrization of the Glottal Flow},
  author = {Alku, Paavo and B{\"a}ckstr{\"o}m, Tom and Vilkman, Erkki},
  year = {2002},
  month = aug,
  journal = {The Journal of the Acoustical Society of America},
  volume = {112},
  number = {2},
  pages = {701--710},
  issn = {0001-4966},
  doi = {10.1121/1.1490365},
  urldate = {2019-06-24},
  langid = {english},
  file = {/home/marnix/Zotero/storage/SB4F9HBJ/Alku et al. - 2002 - Normalized amplitude quotient for parametrization .pdf}
}

@article{Alku2006,
  title = {Estimation of the {{Voice Source}} from {{Speech Pressure Signals}}: {{Evaluation}} of an {{Inverse Filtering Technique Using Physical Modelling}} of {{Voice Production}}},
  shorttitle = {Estimation of the {{Voice Source}} from {{Speech Pressure Signals}}},
  author = {Alku, Paavo and Story, Brad and Airas, Matti},
  year = {2006},
  journal = {Folia Phoniatrica et Logopaedica},
  volume = {58},
  number = {2},
  pages = {102--113},
  issn = {1021-7762, 1421-9972},
  doi = {10.1159/000089611},
  urldate = {2023-02-28},
  abstract = {\emph{Objective:} The goal of the study is to use physical modelling of voice production to assess the performance of an inverse filtering method in estimating the glottal flow from acoustic speech pressure signals. \emph{Methods:} An automatic inverse filtering method is presented, and speech pressure signals are generated using physical modelling of voice production so as to obtain test vowels with a known shape of the glottal excitation waveform. The speech sounds produced consist of 4 different vowels, each with 10 different values of the fundamental frequency. Both the original glottal flows given by physical modelling and their estimates computed by inverse filtering were parametrised with two robust voice source parameters: the normalized amplitude quotient and the difference (in decibels) between the levels of the first and second harmonics. \emph{Results:}The results show that for both extracted parameters the error introduced by inverse filtering was, in general, small. The effect of the distortion caused by inverse filtering on the parameter values was clearly smaller than the change in the corresponding parameters when the phonation type was altered. The distortion was largest for high-pitched vowels with the lowest value of the first formant. \emph{Conclusions:} The study shows that the proposed inverse filtering technique combined with the extracted parameters constitutes a voice source analysis tool that is able to measure the voice source dynamics automatically with satisfactory accuracy.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/HR7KBAN4/Alku et al. - 2006 - Estimation of the Voice Source from Speech Pressur.pdf}
}

@article{Alku2011,
  title = {Glottal Inverse Filtering Analysis of Human Voice Production --- {{A}} Review of Estimation and Parameterization Methods of the Glottal Excitation and Their Applications},
  author = {Alku, Paavo},
  year = {2011},
  month = oct,
  journal = {Sadhana},
  volume = {36},
  number = {5},
  pages = {623--650},
  issn = {0256-2499, 0973-7677},
  doi = {10.1007/s12046-011-0041-5},
  urldate = {2019-06-24},
  abstract = {Glottal inverse filtering (GIF) refers to methods of estimating the source of voiced speech, the glottal volume velocity waveform. GIF is based on the idea of inversion, in which the effects of the vocal tract and lip radiation are cancelled from the output of the voice production mechanism, the speech signal. This article provides a review on GIF research by examining an era spanning five decades during which this topic has been under development. The topic is handled from three main perspectives: the estimation methods of the glottal source, the parameterization techniques that have been developed to express the estimated glottal excitations in numerical forms, and the application areas of GIF. Finally, the strengths and limitations of the GIF approach are discussed.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/RE3YNLCJ/Alku - 2011 - Glottal inverse filtering analysis of human voice .pdf}
}

@article{Alku2013,
  title = {Formant Frequency Estimation of High-Pitched Vowels Using Weighted Linear Prediction},
  author = {Alku, Paavo and Pohjalainen, Jouni and Vainio, Martti and Laukkanen, Anne-Maria and Story, Brad H.},
  year = {2013},
  month = aug,
  journal = {The Journal of the Acoustical Society of America},
  volume = {134},
  number = {2},
  pages = {1295--1313},
  issn = {0001-4966},
  doi = {10.1121/1.4812756},
  urldate = {2023-04-11},
  langid = {english},
  file = {/home/marnix/Zotero/storage/E2Q4V3YQ/Alku et al. - 2013 - Formant frequency estimation of high-pitched vowel.pdf}
}

@article{Alku2019,
  title = {{{OPENGLOT}} -- {{An}} Open Environment for the Evaluation of Glottal Inverse Filtering},
  author = {Alku, Paavo and Murtola, Tiina and Malinen, Jarmo and Kuortti, Juha and Story, Brad and Airaksinen, Manu and Salmi, Mika and Vilkman, Erkki and Geneid, Ahmed},
  year = {2019},
  month = feb,
  journal = {Speech Communication},
  volume = {107},
  pages = {38--47},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2019.01.005},
  urldate = {2023-10-18},
  abstract = {Glottal inverse filtering (GIF) refers to technology to estimate the source of voiced speech, the glottal flow, from speech signals. When a new GIF algorithm is proposed, its accuracy needs to be evaluated. However, the evaluation of GIF is problematic because the ground truth, the real glottal volume velocity signal generated by the vocal folds, cannot be recorded non-invasively from natural speech. This absence of the ground truth has been circumvented in most previous GIF studies by using simple linear source-filter synthesis techniques with known artificial glottal flow models and all-pole vocal tract filters. Moreover, in a few previous studies, physical modeling of speech production has been utilized in synthesis of the test data for GIF evaluation. The evaluation strategy in previous GIF studies is, however, scattered between individual investigations and there is currently a lack of a coherent, common platform to be used in GIF evaluation. In order to address this shortcoming, the current study introduces a new environment, called OPENGLOT, for GIF evaluation. The key ideas of OPENGLOT are twofold: the environment is versatile (i.e., it provides different types of test signals for GIF evaluation) and open (i.e., the system can be used by anyone who wants to evaluate her or his new GIF method and compare it objectively to previously developed benchmark techniques). OPENGLOT consists of four main parts, Repositories I--IV, that contain data and sound synthesis software. Repository I contains a large set of synthetic glottal flow waveforms, and speech signals generated by using the Liljencrants--Fant (LF) waveform as an artificial excitation, and a digital all-pole filter to model the vocal tract. Repository II contains glottal flow and speech pressure signals generated using physical modeling of human speech production. Repository III contains pairs of glottal excitation and speech pressure signal generated by exciting 3D printed plastic vocal tract replica with LF excitations via a loudspeaker. Finally, Repository IV contains multichannel recordings (speech pressure signal, electroglottogram, high-speed video of the vocal folds) from natural production of speech. After presenting these four core parts of OPENGLOT, the article demonstrates the platform by presenting a typical use case.},
  keywords = {Evaluation tool,Glottal flow,Glottal inverse filtering,Speech production},
  file = {/home/marnix/Zotero/storage/8BZZ7ZKB/Alku et al. - 2019 - OPENGLOT – An open environment for the evaluation .pdf;/home/marnix/Zotero/storage/WYDZH5E7/S0167639318303509.html}
}

@article{Allen2000,
  title = {Financial {{Contagion}}},
  author = {Allen, Franklin and Gale, Douglas},
  year = {2000},
  month = feb,
  journal = {Journal of Political Economy},
  volume = {108},
  number = {1},
  eprint = {1011.1669v3},
  pages = {1--33},
  publisher = {The University of Chicago Press},
  issn = {0022-3808},
  doi = {10.1086/262109},
  abstract = {Financial contagion is modeled as an equilibrium phenomenon. Because liquidity preference shocks are imperfectly correlated across regions, banks hold interregional claims on other banks to provide insurance against liquidity preference shocks. When there is no aggregate uncertainty, the first-best allocation of risk sharing can be achieved. However, this arrangement is financially fragile. A small liquidity preference shock in one region can spread by contagion throughout the economy. The possibility of contagion depends strongly on the completeness of the structure of interregional claims. Complete claims structures are shown to be more robust than incomplete structures.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1011.1669v3},
  isbn = {9780123978752},
  pmid = {25246403},
  keywords = {bank,contagion,financial},
  file = {/home/marnix/Zotero/storage/Z69CKTPP/Allen, Gale - 2000 - Financial Contagion.pdf}
}

@article{Allison2012,
  title = {Application of a {{Bayesian Method}} to {{Absorption Spectral-Line Finding}} in {{Simulated ASKAP Data}}},
  author = {Allison, J. R. and Sadler, E. M. and Whiting, M. T.},
  year = {2012/ed},
  journal = {Publications of the Astronomical Society of Australia},
  volume = {29},
  number = {3},
  pages = {221--228},
  publisher = {Cambridge University Press},
  issn = {1448-6083, 1323-3580},
  doi = {10.1071/AS11040},
  urldate = {2021-03-19},
  abstract = {The large spectral bandwidth and wide field of view of the Australian SKA Pathfinder radio telescope will open up a completely new parameter space for large extragalactic HI surveys. Here we focus on identifying and parametrising HI absorption lines which occur in the line of sight towards strong radio continuum sources. We have developed a method for simultaneously finding and fitting HI absorption lines in radio data by using multi-nested sampling, a Bayesian Monte Carlo algorithm. The method is tested on a simulated ASKAP data cube, and is shown to be reliable at detecting absorption lines in low signal-to-noise data without the need to smooth or alter the data. Estimation of the local Bayesian evidence statistic provides a quantitative criterion for assigning significance to a detection and selecting between competing analytical line-profile models.},
  langid = {english},
  keywords = {methods: data analysis,methods: statistical,radio lines: galaxies},
  file = {/home/marnix/Zotero/storage/26B2QEIL/Allison et al. - 2012 - Application of a Bayesian Method to Absorption Spe.pdf;/home/marnix/Zotero/storage/B7USZJD3/128E89378741B752FAAB976713E62276.html}
}

@article{Alsing2021,
  title = {Nested Sampling with Any Prior You Like},
  author = {Alsing, Justin and Handley, Will},
  year = {2021},
  month = jun,
  journal = {Monthly Notices of the Royal Astronomical Society: Letters},
  volume = {505},
  number = {1},
  eprint = {2102.12478},
  primaryclass = {astro-ph, physics:physics, stat},
  pages = {L95-L99},
  issn = {1745-3925, 1745-3933},
  doi = {10.1093/mnrasl/slab057},
  urldate = {2022-07-20},
  abstract = {Nested sampling is an important tool for conducting Bayesian analysis in Astronomy and other fields, both for sampling complicated posterior distributions for parameter inference, and for computing marginal likelihoods for model comparison. One technical obstacle to using nested sampling in practice is the requirement (for most common implementations) that prior distributions be provided in the form of transformations from the unit hyper-cube to the target prior density. For many applications - particularly when using the posterior from one experiment as the prior for another - such a transformation is not readily available. In this letter we show that parametric bijectors trained on samples from a desired prior density provide a general-purpose method for constructing transformations from the uniform base density to a target prior, enabling the practical use of nested sampling under arbitrary priors. We demonstrate the use of trained bijectors in conjunction with nested sampling on a number of examples from cosmology.},
  archiveprefix = {arXiv},
  keywords = {{Physics - Data Analysis, Statistics and Probability},Astrophysics - Cosmology and Nongalactic Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/5LNC7XD8/Alsing and Handley - 2021 - Nested sampling with any prior you like.pdf;/home/marnix/Zotero/storage/828U7PDU/2102.html}
}

@article{Alvarado2016,
  title = {Gaussian {{Processes}} for {{Music Audio Modelling}} and {{Content Analysis}}},
  author = {Alvarado, Pablo A. and Stowell, Dan},
  year = {2016},
  month = jun,
  journal = {arXiv:1606.01039 [cs, stat]},
  eprint = {1606.01039},
  primaryclass = {cs, stat},
  urldate = {2020-05-01},
  abstract = {Real music signals are highly variable, yet they have strong statistical structure. Prior information about the underlying physical mechanisms by which sounds are generated and rules by which complex sound structure is constructed (notes, chords, a complete musical score), can be naturally unified using Bayesian modelling techniques. Typically algorithms for Automatic Music Transcription independently carry out individual tasks such as multiple-F0 detection and beat tracking. The challenge remains to perform joint estimation of all parameters. We present a Bayesian approach for modelling music audio, and content analysis. The proposed methodology based on Gaussian processes seeks joint estimation of multiple music concepts by incorporating into the kernel prior information about non-stationary behaviour, dynamics, and rich spectral content present in the modelled music signal. We illustrate the benefits of this approach via two tasks: pitch estimation, and inferring missing segments in a polyphonic audio recording.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Sound,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/6PJ6446R/Alvarado and Stowell - 2016 - Gaussian Processes for Music Audio Modelling and C.pdf;/home/marnix/Zotero/storage/DZLSVLNJ/1606.html}
}

@article{Alvarado2018,
  title = {Sparse {{Gaussian Process Audio Source Separation Using Spectrum Priors}} in the {{Time-Domain}}},
  author = {Alvarado, Pablo A. and {\'A}lvarez, Mauricio A. and Stowell, Dan},
  year = {2018},
  month = nov,
  journal = {arXiv:1810.12679 [cs, eess, stat]},
  eprint = {1810.12679},
  primaryclass = {cs, eess, stat},
  urldate = {2020-05-01},
  abstract = {Gaussian process (GP) audio source separation is a time-domain approach that circumvents the inherent phase approximation issue of spectrogram based methods. Furthermore, through its kernel, GPs elegantly incorporate prior knowledge about the sources into the separation model. Despite these compelling advantages, the computational complexity of GP inference scales cubically with the number of audio samples. As a result, source separation GP models have been restricted to the analysis of short audio frames. We introduce an efficient application of GPs to time-domain audio source separation, without compromising performance. For this purpose, we used GP regression, together with spectral mixture kernels, and variational sparse GPs. We compared our method with LD-PSDTF (positive semi-definite tensor factorization), KL-NMF (Kullback-Leibler non-negative matrix factorization), and IS-NMF (Itakura-Saito NMF). Results show that the proposed method outperforms these techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/ZGIWFCQB/Alvarado et al. - 2018 - Sparse Gaussian Process Audio Source Separation Us.pdf;/home/marnix/Zotero/storage/MM7RDGHW/1810.html}
}

@misc{Alvarez-Melis2018,
  title = {Gromov-{{Wasserstein Alignment}} of {{Word Embedding Spaces}}},
  author = {{Alvarez-Melis}, David and Jaakkola, Tommi S.},
  year = {2018},
  month = aug,
  number = {arXiv:1809.00013},
  eprint = {1809.00013},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-19},
  abstract = {Cross-lingual or cross-domain correspondences play key roles in tasks ranging from machine translation to transfer learning. Recently, purely unsupervised methods operating on monolingual embeddings have become effective alignment tools. Current state-of-theart methods, however, involve multiple steps, including heuristic post-hoc refinement strategies. In this paper, we cast the correspondence problem directly as an optimal transport (OT) problem, building on the idea that word embeddings arise from metric recovery algorithms. Indeed, we exploit the GromovWasserstein distance that measures how similarities between pairs of words relate across languages. We show that our OT objective can be estimated efficiently, requires little or no tuning, and results in performance comparable with the state-of-the-art in various unsupervised word translation tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/marnix/Zotero/storage/77X3MIEP/Alvarez-Melis and Jaakkola - 2018 - Gromov-Wasserstein Alignment of Word Embedding Spa.pdf}
}

@techreport{Alves2013,
  title = {The Structure and Resilience of the {{European}} Interbank Market},
  booktitle = {European Systemic Risk Board Occasional Paper Series},
  author = {Alves, Ivan and Ferrari, Stijn and Franchini, Pietro and Heam, Jean-Cyprien and Jurca, Pavol and {Others} and Langfield, Sam and Laviola, Sabastiano and Liedorp, Franka and S{\'a}nchez, Antonio and Tavolaro, Santiago and Vuillemey, Guillaume},
  year = {2013},
  number = {3},
  institution = {European Systemic Risk Board},
  abstract = {Financial institutions are connected to each other by a series of bilateral transactions. In normal times, institutions' connections may result in efficient risk transfer. But in crises, connections can facilitate contagion -- as initial problems lead to chains of defaults and liquidity shortages -- sparked by shocks which might arise within the financial system or from the real economy.{\textbackslash}nInstitutions are also interconnected in indirect ways, since they are exposed to common risk factors that can result in concurrent losses. For example, most banks extend loans secured by real estate: they are thus collectively exposed to falls in house prices. Resulting bank distress can then exacerbate initial problems: banks might simultaneously sell collateral (houses), thus worsening downward price spirals. Less tangibly, institutions can also be connected through perceptions of counterparties' creditworthiness. Given uncertainty, financial institutions may in general become reluctant to lend to each other and hoard liquidity.{\textbackslash}nPotential for contagion due to interconnectedness is a key component of systemic risk. As a first step towards understanding the mechanisms of contagion, this paper abstracts from complex indirect connections between banks, and rather focuses on direct linkages between 53 large EU banks, based on unique data on interbank exposures collected by national regulators as of the end of 2011.},
  file = {/home/marnix/Zotero/storage/TZ2QNHL9/Alves et al. - 2013 - The structure and resilience of the European interbank market.pdf}
}

@article{Alzamendi2017,
  title = {Modeling and Joint Estimation of Glottal Source and Vocal Tract Filter by State-Space Methods},
  author = {Alzamendi, Gabriel A. and Schlotthauer, Gast{\'o}n},
  year = {2017},
  month = aug,
  journal = {Biomedical Signal Processing and Control},
  series = {Study of the Human Voice: New Methodologies for a Current Challenge},
  volume = {37},
  pages = {5--15},
  issn = {1746-8094},
  doi = {10.1016/j.bspc.2016.12.022},
  urldate = {2020-04-06},
  abstract = {Accurate estimation of the glottal source from a voiced sound is a difficult blind separation problem in speech signal processing. In this work, state-space methods are investigated to enhance the joint estimation of the glottal source and the vocal tract information. The aim of this paper is twofold. First, a stochastic glottal source is proposed, based on deterministic Liljencrants--Fant model and ruled by a stochastic difference equation. Such a representation allows to accurately capture any perturbation occurring at glottal level in real voices. A state-space voice model is formulated considering the stochastic glottal source. Then, combining this voice model and the state-space framework, an inverse filtering method is developed that allows to jointly estimate both glottal source and vocal tract filter. The performance of this method is studied by means of experiments with voices synthesized by applying both the source-filter theory and a physical based voice model. The method is also test using human voice signals. The results demonstrate that accurate estimates of the glottal source and the vocal tract filter can be obtained over several scenarios. Moreover, the method is shown to be robust with respect to different phonation types.},
  langid = {english},
  keywords = {Glottal inverse filtering,Joint source-filter estimation,State-space voice model,Stochastic glottal source},
  file = {/home/marnix/Zotero/storage/DYDZYV7G/Alzamendi and Schlotthauer - 2017 - Modeling and joint estimation of glottal source an.pdf;/home/marnix/Zotero/storage/SAY4MJSS/S1746809416302348.html}
}

@article{Anand2011,
  title = {Shannon and von {{Neumann}} Entropy of Random Networks with Heterogeneous Expected Degree},
  author = {Anand, Kartik and Bianconi, Ginestra and Severini, Simone},
  year = {2011},
  month = mar,
  journal = {Physical Review E},
  volume = {83},
  number = {3},
  eprint = {1011.1565},
  pages = {036109+},
  publisher = {American Physical Society},
  issn = {15393755},
  doi = {10.1103/physreve.83.036109},
  abstract = {Entropic measures of complexity are able to quantify the information encoded in complex network structures. Several entropic measures have been proposed in this respect. Here we study the relation between the Shannon entropy and the von Neumann entropy of networks with given expected degree sequence. We find in different examples of network topologies that when the degree distribution contains some heterogeneity, an intriguing correlation emerges between the two entropic quantities. This results seems to suggest that heterogeneity in the expected degree distribution is implying an equivalence between a quantum and a classical description of networks, which respectively corresponds to the von Neumann and the Shannon entropy.},
  archiveprefix = {arXiv},
  arxivid = {1011.1565},
  isbn = {1550-2376 (Electronic)\r1539-3755 (Linking)},
  pmid = {21517560},
  keywords = {entropy,information\{{\textbackslash}\_\}theory},
  file = {/home/marnix/Zotero/storage/DUVTNRC9/Anand, Bianconi, Severini - 2011 - Shannon and von Neumann entropy of random networks with heterogeneous expected degree.pdf}
}

@article{Ananthapadmanabha1982,
  title = {Calculation of True Glottal Flow and Its Components},
  author = {Ananthapadmanabha, T. V. and Fant, G.},
  year = {1982},
  month = dec,
  journal = {Speech Communication},
  volume = {1},
  number = {3},
  pages = {167--184},
  issn = {0167-6393},
  doi = {10.1016/0167-6393(82)90015-2},
  urldate = {2023-05-24},
  abstract = {An iterative numerical algorithm is presented for computing the true glottal flow given the glottal area function and lung pressure. The effects of glottal inertance and sub- and supraglottal impedance are discussed. It is shown that the effect of glottal inertance is small and that it is adequate to consider subglottal and supraglottal systems by one-formant loads only. An equivalent circuit for the glottis considering the nonlinear and time-varying pressure-flow relation is derived. In addition to the dynamic glottal resistance, there exists a hypothetical inductance in the equivalent circuit which is mainly responsible for the skewing of the source pulses under load. An analytical equation for the main pulse shape of the true glottal flow is derived as the source residue. The role of ripple components in the true glottal flow is discussed. Zusammenfassung Beschrieben wird ein iterativer Algorithmus zur Bestimmung des Luftstroms in der Glottis f{\"u}r den Fall, dass die Querschnittsfunktion der Glottis und der Luftdruck in der Lunge bekannt sind. Die Auswirkung der Tr{\"a}gheit der Stimmb{\"a}nder und der Impedanz des supraglotten Systems werden diskuteirt. Wie sich zeigt, istder Einfluss der Tr{\"a}gheit gering, und das subglottale wie auch das supraglottale System ist als Lastimpedanz mit nur einem Formanten dartellbar. Ein Ersatzschaltbild f{\"u}r die Glottis wird abgeleitet, welches die nichtlineare und zeitver{\"a}nderliche Beziehung zwischen Luftdruck and Luftstrom beschreibt. Zus{\"a}tzlich zum dynamischen Widerstand der Glottis existiert im Ersatzschaltbild eine hypothetische Induktivit{\"a}t; diese ist letztlich verantwortlich f{\"u}r die Abschr{\"a}gung der Anregungsimplulse bei angekoppelter Lastimpedanz. Eine analytische Formel f{\"u}r den zeitlichen Verlauf desLuftstroms durch die Glottis wird in Form des Quellenresiduums hergeletet. Diskutiert wird ausserdem die Frage von Welligkeien im zeetlichen Verlauf des Luftstroms. R{\'e}sum{\'e} On pr{\'e}nte un algorithmic num{\'e}rique it{\'e}ratif pour le calcul du flux glottique r{\'e}el, compete tenu de la fonction d'aire glottique et de la pression pulmonaire. Les effets de l'inertance de la glotte et de l'imp{\'e}dance sub- et supra-glottique sont discut{\'e}s. Il est montr{\'e} que l'effet de l'inertance de la glotte est faile et qu'il est ad{\'e}quat de consid{\'e}rer les syst{\`m}es sub- et supra-glottique comme soumis {\`a} la charge d'un seul formant. Un circuit {\'e}quivalent pour la glotte est d{\'e}riv{\'e}, circuit qui prend en compte la relation non lin{\'e}aire et temporellement variable existant entre la pression et le flux glottiques. En plus de la r{\'e}sistance dynamique de la glotte, une hypoth{\'e}tique inductance est introduite dans le circuit {\'e}quivalent, inductance qui est principalement responsible de la d{\'e}formation de l'onde de source, quand le syst{\`e}me est charg{\'e}. Une {\'e}quation analytique pour la forme de l'impulsion principale du flux glottique r{\'e}el est d{\'e}riv{\'e}e en tant que r{\'e}sidu de la fonction de source. Le r{\^o}le des components oscillantes du flux glottique r{\'e}el est discut{\'e}.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/I34XSPWA/Ananthapadmanabha and Fant - 1982 - Calculation of true glottal flow and its component.pdf;/home/marnix/Zotero/storage/RV2GZH92/0167639382900152.html}
}

@article{Anatolyev2003,
  title = {The Term Structure of {{Russian}} Interest Rates},
  author = {Anatolyev, Stanislav and Korepanov, Sergey},
  year = {2003},
  journal = {Applied Economics Letters},
  volume = {10},
  number = {13},
  pages = {867--870},
  publisher = {Taylor & Francis},
  file = {/home/marnix/Zotero/storage/XNTWCDL4/Anatolyev, Korepanov - 2003 - The term structure of Russian interest rates.pdf}
}

@book{Andersen2023,
  title = {The Metainterface: {{The}} Art of Platforms, Cities, and Clouds},
  author = {Andersen, Christian Ulrik and Pold, Soren Bro},
  year = {2023},
  publisher = {MIT Press},
  file = {/home/marnix/Zotero/storage/VSQ5LIWA/the-metainterface.html}
}

@article{Andrew2013,
  title = {Using Articulatory Measurements to Learn Better Acoustic Features},
  author = {Andrew, Galen and Arora, Raman and Bharadwaj, Sujeeth and Bilmes, Jeff and {Hasegawa-johnson}, Mark and Livescu, Karen and Washington, U},
  year = {2013},
  pages = {1--2},
  file = {/home/marnix/Zotero/storage/QLEAQLS6/Andrew2013 Using articulatory measurements to learn better acoustic features.pdf}
}

@article{Andrieu1999,
  title = {Joint {{Bayesian}} Model Selection and Estimation of Noisy Sinusoids via Reversible Jump {{MCMC}}},
  author = {Andrieu, C. and Doucet, A.},
  year = {1999},
  month = oct,
  journal = {IEEE Transactions on Signal Processing},
  volume = {47},
  number = {10},
  pages = {2667--2676},
  issn = {1941-0476},
  doi = {10.1109/78.790649},
  abstract = {In this paper, the problem of joint Bayesian model selection and parameter estimation for sinusoids in white Gaussian noise is addressed. An original Bayesian model is proposed that allows us to define a posterior distribution on the parameter space. All Bayesian inference is then based on this distribution. Unfortunately, a direct evaluation of this distribution and of its features, including posterior model probabilities, requires evaluation of some complicated high-dimensional integrals. We develop an efficient stochastic algorithm based on reversible jump Markov chain Monte Carlo methods to perform the Bayesian computation. A convergence result for this algorithm is established. In simulation, it appears that the performance of detection based on posterior model probabilities outperforms conventional detection schemes.},
  keywords = {Bayesian methods,Computational modeling,Convergence,Frequency estimation,Gaussian noise,Inference algorithms,Maximum likelihood estimation,Parameter estimation,Spectral analysis,Stochastic processes},
  file = {/home/marnix/Zotero/storage/C8HL9UXU/Andrieu and Doucet - 1999 - Joint Bayesian model selection and estimation of n.pdf;/home/marnix/Zotero/storage/XKBHG2FD/790649.html}
}

@phdthesis{Andringa2002,
  title = {Continuity {{Preserving Signal Processing}}},
  author = {Andringa, Tjeerd},
  year = {2002},
  abstract = {This thesis is the product of a research project that is simple to formulate: apply the rich temporal information of a model of the human inner ear (or cochlea) to automatic speech recognition. This project seemed, and proved to be, an interesting combination of physics, engineering, and cognitive science.},
  file = {/home/marnix/Zotero/storage/2H3PS3KU/Andringa2002 Continuity preserving signal processing.pdf}
}

@book{Antsaklis2006,
  title = {Linear Systems},
  author = {Antsaklis, Panos J and Michel, Anthony N},
  year = {2006},
  publisher = {Springer Science \& Business Media},
  file = {/home/marnix/Zotero/storage/3KXE3B87/Antsaklis and Michel - 2006 - Linear systems.pdf}
}

@techreport{Archakov2018,
  title = {A New Parametrization of Correlation Matrices},
  author = {Archakov, I and Hansen, {\relax PR}},
  year = {2018},
  institution = {Working paper},
  file = {/home/marnix/Zotero/storage/KY84DQJE/Archakov and Hansen - 2018 - A new parametrization of correlation matrices.pdf}
}

@book{Arfken2005,
  title = {Mathematical Methods for Physicists},
  author = {Arfken, George B. and Weber, Hans-Jurgen},
  year = {2005},
  edition = {6th ed},
  publisher = {Elsevier},
  address = {Boston},
  isbn = {978-0-12-059876-2 978-0-12-088584-8},
  langid = {english},
  lccn = {QA37.3 .A74 2005},
  keywords = {Mathematical physics,Mathematics},
  file = {/home/marnix/Zotero/storage/2WURZ9KY/Arfken and Weber - 2005 - Mathematical methods for physicists.pdf}
}

@book{Arnold1989,
  title = {Mathematical Methods of Classical Mechanics},
  author = {Arnol'd, V. I.},
  year = {1989},
  series = {Graduate Texts in Mathematics},
  edition = {2nd ed},
  number = {60},
  publisher = {Springer-Verlag},
  address = {New York},
  isbn = {978-0-387-96890-2},
  langid = {english},
  lccn = {QA805 .A6813 1989},
  keywords = {{Mechanics, Analytic}},
  file = {/home/marnix/Zotero/storage/VAZYHZCN/Arnolʹd - 1989 - Mathematical methods of classical mechanics.pdf}
}

@misc{Arora2019,
  title = {A {{Latent Variable Model Approach}} to {{PMI-based Word Embeddings}}},
  author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  year = {2019},
  month = jun,
  number = {arXiv:1502.03520},
  eprint = {1502.03520},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1502.03520},
  urldate = {2024-07-23},
  abstract = {Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparameters and reweighting methods. This paper proposes a new generative model, a dynamic version of the log-linear topic model of{\textasciitilde}{\textbackslash}citet\{mnih2007three\}. The methodological novelty is to use the prior to compute closed form expressions for word statistics. This provides a theoretical justification for nonlinear models like PMI, word2vec, and GloVe, as well as some hyperparameter choices. It also helps explain why low-dimensional semantic embeddings contain linear algebraic structure that allows solution of word analogies, as shown by{\textasciitilde}{\textbackslash}citet\{mikolov2013efficient\} and many subsequent papers. Experimental support is provided for the generative model assumptions, the most important of which is that latent word vectors are fairly uniformly dispersed in space.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/DMZS3X9E/Arora et al. - 2019 - A Latent Variable Model Approach to PMI-based Word.pdf}
}

@article{Aryal2012,
  title = {Boosting {{Automatic Speech Recognition Through Articulatory Inversion}}},
  author = {Aryal, Sandesh and Huang, Jin and Felps, Daniel and {Gutierrez-osuna}, Ricardo},
  year = {2012},
  pages = {6--10},
  keywords = {and engineering,articulatory inversion,artment of computer science,boosting automatic speech recognition,daniel felps,jin huang,m university,ricardo gutierrez-osuna,sandesh aryal,technical report tamu-cs-tr-2012-12-3,texas a,through},
  file = {/home/marnix/Zotero/storage/T4VSU47Q/Aryal2012 Boosting ASR through articulatory inversion.pdf}
}

@article{Ashton2022,
  title = {Nested Sampling for Physical Scientists},
  author = {Ashton, Greg and Bernstein, Noam and Buchner, Johannes and Chen, Xi and Cs{\'a}nyi, G{\'a}bor and Fowlie, Andrew and Feroz, Farhan and Griffiths, Matthew and Handley, Will and Habeck, Michael and Higson, Edward and Hobson, Michael and Lasenby, Anthony and Parkinson, David and P{\'a}rtay, Livia B. and Pitkin, Matthew and Schneider, Doris and Speagle, Joshua S. and South, Leah and Veitch, John and Wacker, Philipp and Wales, David J. and Yallup, David},
  year = {2022},
  month = may,
  journal = {Nature Reviews Methods Primers},
  volume = {2},
  number = {1},
  pages = {1--22},
  publisher = {Nature Publishing Group},
  issn = {2662-8449},
  doi = {10.1038/s43586-022-00121-x},
  urldate = {2024-01-05},
  abstract = {This Primer examines Skilling's nested sampling algorithm for Bayesian inference and, more broadly, multidimensional integration. The principles of nested sampling are summarized and recent developments using efficient nested sampling algorithms in high dimensions~surveyed, including methods for sampling from the constrained prior. Different ways of applying nested sampling are outlined, with detailed examples from three scientific fields: cosmology, gravitational-wave astronomy and materials science. Finally, the Primer includes recommendations for best practices and a discussion of potential limitations and optimizations of nested sampling.},
  copyright = {2022 Springer Nature Limited},
  langid = {english},
  keywords = {Statistical physics,Statistics},
  file = {/home/marnix/Zotero/storage/DSSEM6QR/Ashton et al. - 2022 - Nested sampling for physical scientists.pdf}
}

@article{Atal1970,
  title = {Adaptive Predictive Coding of Speech Signals},
  author = {Atal, B. S. and Schroeder, M. R.},
  year = {1970},
  month = oct,
  journal = {The Bell System Technical Journal},
  volume = {49},
  number = {8},
  pages = {1973--1986},
  issn = {0005-8580},
  doi = {10.1002/j.1538-7305.1970.tb04297.x},
  abstract = {We describe in this paper a method for efficient encoding of speech signals, based on predictive coding. In this coding method, both the transmitter and the receiver estimate the signal's current value by linear prediction on the previously transmitted signal. The difference between this estimate and the true value of the signal is quantized, coded and transmitted to the receiver. At the receiver, the decoded difference signal is added to the predicted signal to reproduce the input speech signal. Because of the nonstationary nature of the speech signals, an adaptive linear predictor is used, which is readjusted periodically to minimize the mean-square error between the predicted and the true value of the signals. The predictive coding system was simulated on a digital computer. The predictor parameters, comprising one delay and nine other coefficients related to the signal spectrum, were readjusted every 5 milliseconds. The speech signal was sampled at a rate of 6.67 kHz, and the difference signal was quantized by a two-level quantizer with variable step size. Subjective comparisons with speech from a logarithmic PCM encoder (log-PCM) indicate that the quality of the synthesized speech signal from the predictive coding system is approximately equal to that of log-PCM speech encoded at 6 bits/sample. Preliminary studies suggest that the binary difference signal and the predictor parameters together can be transmitted at approximately 10 kilobits/second which is several times less than the bit rate required for log-PCM encoding with comparable speech quality.},
  file = {/home/marnix/Zotero/storage/LNLZ7YSE/Atal and Schroeder - 1970 - Adaptive predictive coding of speech signals.pdf;/home/marnix/Zotero/storage/7NAYYWQM/6769531.html}
}

@article{Atal1971,
  title = {Speech Analysis and Synthesis by Linear Prediction of the Speech Wave},
  author = {Atal, Bishnu S and Hanauer, Suzanne L},
  year = {1971},
  journal = {The journal of the acoustical society of America},
  volume = {50},
  number = {2B},
  pages = {637--655},
  publisher = {ASA},
  file = {/home/marnix/Zotero/storage/CLP6CKDU/Atal and Hanauer - Speech Analysis and Synthesis by Linear Prediction.pdf}
}

@inproceedings{Atal1982,
  title = {A New Model of {{LPC}} Excitation for Producing Natural-Sounding Speech at Low Bit Rates},
  booktitle = {{{ICASSP}} '82. {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Atal, B. and Remde, J.},
  year = {1982},
  month = may,
  volume = {7},
  pages = {614--617},
  issn = {null},
  doi = {10.1109/ICASSP.1982.1171649},
  abstract = {The excitation for LPC speech synthesis usually consists of two separate signals - a delta-function pulse once every pitch period for voiced speech and white noise for unvoiced speech. This manner of representing excitation requires that speech segments be classified accurately into voiced and unvoiced categories and the pitch period of voiced segments be known. It is now well recognized that such a rigid idealization of the vocal excitation is often responsible for the unnatural quality associated with synthesized speech. This paper describes a new approach to the excitation problem that does not require a priori knowledge of either the voiced-unvoiced decision or the pitch period. All classes of sounds are generated by exciting the LPC filter with a sequence of pulses; the amplitudes and locations of the pulses are determined using a non-iterative analysis-by-synthesis procedure. This procedure minimizes a perceptual-distance metric representing subjectively-important differences between the waveforms of the original and the synthetic speech signals. The distance metric takes account of the finite-frequency resolution as well as the differential sensitivity of the human ear to errors in the formant and inter-formant regions of the speech spectrum.},
  keywords = {Bit rate,Filters,Humans,Linear predictive coding,Pulse generation,Signal resolution,Speech enhancement,Speech recognition,Speech synthesis,White noise},
  file = {/home/marnix/Zotero/storage/2F5EARZT/Atal and Remde - 1982 - A new model of LPC excitation for producing natura.pdf;/home/marnix/Zotero/storage/JAIDJDWI/1171649.html}
}

@article{Atal2006,
  title = {The History of Linear Prediction},
  author = {Atal, Bishnu S},
  year = {2006},
  journal = {IEEE Signal Processing Magazine},
  volume = {23},
  number = {2},
  pages = {154--161},
  publisher = {IEEE},
  file = {/home/marnix/Zotero/storage/V88CFGG7/Atal - 2006 - The history of linear prediction.pdf}
}

@article{Au2003,
  title = {Important Sampling in High Dimensions},
  author = {Au, S. K. and Beck, J. L.},
  year = {2003},
  month = apr,
  journal = {Structural Safety},
  volume = {25},
  number = {2},
  pages = {139--163},
  issn = {0167-4730},
  doi = {10.1016/S0167-4730(02)00047-4},
  urldate = {2022-04-29},
  abstract = {This paper draws attention to a fundamental problem that occurs in applying importance sampling to `high-dimensional' reliability problems, i.e., those with a large number of uncertain parameters. This question of applicability carries an important bearing on the potential use of importance sampling for solving dynamic first-excursion problems and static reliability problems for structures with a large number of uncertain structural model parameters. The conditions under which importance sampling is applicable in high dimensions are investigated, where the focus is put on the common case of standard Gaussian uncertain parameters. It is found that importance sampling densities using design points are applicable if the covariance matrix associated with each design point does not deviate significantly from the identity matrix. The study also suggests that importance sampling densities using random pre-samples are generally not applicable in high dimensions.},
  langid = {english},
  keywords = {Importance sampling,Monte Carlo simulation,Relative entropy,Reliability},
  file = {/home/marnix/Zotero/storage/ZFLC78TN/Au and Beck - 2003 - Important sampling in high dimensions.pdf;/home/marnix/Zotero/storage/VI5H6VD6/S0167473002000474.html}
}

@article{Auvinen2014,
  title = {Automatic Glottal Inverse Filtering with the {{Markov}} Chain {{Monte Carlo}} Method},
  author = {Auvinen, Harri and Raitio, Tuomo and Airaksinen, Manu and Siltanen, Samuli and Story, Brad H. and Alku, Paavo},
  year = {2014},
  month = sep,
  journal = {Computer Speech \& Language},
  volume = {28},
  number = {5},
  pages = {1139--1155},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2013.09.004},
  urldate = {2023-03-28},
  abstract = {This paper presents a new glottal inverse filtering (GIF) method that utilizes a Markov chain Monte Carlo (MCMC) algorithm. First, initial estimates of the vocal tract and glottal flow are evaluated by an existing GIF method, iterative adaptive inverse filtering (IAIF). Simultaneously, the initially estimated glottal flow is synthesized using the Rosenberg--Klatt (RK) model and filtered with the estimated vocal tract filter to create a synthetic speech frame. In the MCMC estimation process, the first few poles of the initial vocal tract model and the RK excitation parameter are refined in order to minimize the error between the synthetic and original speech signals in the time and frequency domain. MCMC approximates the posterior distribution of the parameters, and the final estimate of the vocal tract is found by averaging the parameter values of the Markov chain. Experiments with synthetic vowels produced by a physical modeling approach show that the MCMC-based GIF method gives more accurate results compared to two known reference methods.},
  langid = {english},
  keywords = {Glottal inverse filtering,Markov chain Monte Carlo},
  file = {/home/marnix/Zotero/storage/AIIQMDQ2/Auvinen et al. - 2014 - Automatic glottal inverse filtering with the Marko.pdf;/home/marnix/Zotero/storage/6G25J75L/S0885230813000752.html}
}

@article{Badiu2017,
  title = {Variational {{Bayesian Inference}} of {{Line Spectra}}},
  author = {Badiu, Mihai-Alin and Hansen, Thomas Lundgaard and Fleury, Bernard Henri},
  year = {2017},
  month = may,
  journal = {IEEE Transactions on Signal Processing},
  volume = {65},
  number = {9},
  eprint = {1604.03744},
  pages = {2247--2261},
  issn = {1053-587X, 1941-0476},
  doi = {10.1109/TSP.2017.2655489},
  urldate = {2021-03-04},
  abstract = {In this paper, we address the fundamental problem of line spectral estimation in a Bayesian framework. We target model order and parameter estimation via variational inference in a probabilistic model in which the frequencies are continuous-valued, i.e., not restricted to a grid; and the coefficients are governed by a Bernoulli-Gaussian prior model turning model order selection into binary sequence detection. Unlike earlier works which retain only point estimates of the frequencies, we undertake a more complete Bayesian treatment by estimating the posterior probability density functions (pdfs) of the frequencies and computing expectations over them. Thus, we additionally capture and operate with the uncertainty of the frequency estimates. Aiming to maximize the model evidence, variational optimization provides analytic approximations of the posterior pdfs and also gives estimates of the additional parameters. We propose an accurate representation of the pdfs of the frequencies by mixtures of von Mises pdfs, which yields closed-form expectations. We define the algorithm VALSE in which the estimates of the pdfs and parameters are iteratively updated. VALSE is a gridless, convergent method, does not require parameter tuning, can easily include prior knowledge about the frequencies and provides approximate posterior pdfs based on which the uncertainty in line spectral estimation can be quantified. Simulation results show that accounting for the uncertainty of frequency estimates, rather than computing just point estimates, significantly improves the performance. The performance of VALSE is superior to that of state-of-the-art methods and closely approaches the Cram{\textbackslash}'er-Rao bound computed for the true model order.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/CWJY5AZZ/Badiu et al. - 2017 - Variational Bayesian Inference of Line Spectra.pdf;/home/marnix/Zotero/storage/4HGSE8LX/1604.html}
}

@article{Bagaev2023,
  title = {{{RxInfer}}: {{A Julia}} Package for Reactive Real-{{timeBayesian}} Inference},
  shorttitle = {{{RxInfer}}},
  author = {Bagaev, Dmitry and Podusenko, Albert and De Vries, Bert},
  year = {2023},
  month = apr,
  journal = {Journal of Open Source Software},
  volume = {8},
  number = {84},
  pages = {5161},
  issn = {2475-9066},
  doi = {10.21105/joss.05161},
  urldate = {2024-02-22},
  abstract = {Bayesian inference realizes optimal information processing through a full commitment to reasoning by probability theory. The Bayesian framework is positioned at the core of modern AI technology for applications such as speech and image recognition and generation, medical analysis, robot navigation, and more. The framework describes how a rational agent should update its beliefs when new information is revealed by the agent's environment. Unfortunately, perfect Bayesian reasoning is generally intractable, since calculations of (often) very high-dimensional integrals are required for many models of interest. As a result, a number of numerical algorithms for approximating Bayesian inference have been developed and implemented in probabilistic programming packages. Successful methods include the Laplace approximation (Gelman et al., 2015), variants of Monte Carlo (MC) sampling (Salimans et al., n.d.), Variational Inference (VI) (Blei et al., 2017), Automatic-Differentiation Variational Inference (ADVI) (Kucukelbir et al., 2017), and Black-Box Variational Inference (BBVI) (Bamler \& Mandt, 2017).},
  langid = {english},
  file = {/home/marnix/Zotero/storage/BFNUBYS4/Bagaev et al. - 2023 - RxInfer A Julia package for reactive real-timeBay.pdf}
}

@article{Baggenstoss2015,
  title = {Maximum {{Entropy PDF Design Using Feature Density Constraints}}: {{Applications}} in {{Signal Processing}}},
  shorttitle = {Maximum {{Entropy PDF Design Using Feature Density Constraints}}},
  author = {Baggenstoss, P. M.},
  year = {2015},
  month = jun,
  journal = {IEEE Transactions on Signal Processing},
  volume = {63},
  number = {11},
  pages = {2815--2825},
  issn = {1941-0476},
  doi = {10.1109/TSP.2015.2419189},
  abstract = {This paper revisits an existing method of constructing high-dimensional probability density functions (PDFs) based on the PDF at the output of a dimension-reducing feature transformation. We show how to modify the method so that it can provide the PDF with the highest entropy among all PDFs that generate the given low-dimensional PDF. The method is completely general and applies to arbitrary feature transformations. The chain-rule is described for multi-stage feature calculations typically used in signal processing. Examples are given including MFCC and auto-regressive features. Experimental verification of the results using simulated data is provided including a comparison with competing generative methods.},
  keywords = {auto-regressive features,dimension-reducing feature transformation,Entropy,Estimation,feature density constraints,Government,high-dimensional probability density functions,Kernel,Materials,Maximum entropy,maximum entropy PDF design,MFCC,PDF estimation,probability,Probability density function,signal processing,Signal processing,statistical distributions,statistical learning},
  file = {/home/marnix/Zotero/storage/WPBTUGGH/Baggenstoss - 2015 - Maximum Entropy PDF Design Using Feature Density C.pdf;/home/marnix/Zotero/storage/EBL5IEBE/7078839.html}
}

@article{Baggenstoss2018,
  title = {Beyond {{Moments}}: {{Extending}} the {{Maximum Entropy Principle}} to {{Feature Distribution Constraints}}},
  shorttitle = {Beyond {{Moments}}},
  author = {Baggenstoss, Paul M.},
  year = {2018},
  month = sep,
  journal = {Entropy},
  volume = {20},
  number = {9},
  pages = {650},
  publisher = {Multidisciplinary Digital Publishing Institute},
  doi = {10.3390/e20090650},
  urldate = {2021-01-15},
  abstract = {The maximum entropy principle introduced by Jaynes proposes that a data distribution should maximize the entropy subject to constraints imposed by the available knowledge. Jaynes provided a solution for the case when constraints were imposed on the expected value of a set of scalar functions of the data. These expected values are typically moments of the distribution. This paper describes how the method of maximum entropy PDF projection can be used to generalize the maximum entropy principle to constraints on the joint distribution of this set of functions.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {maximum entropy principle,PDF projection,statistical inference},
  file = {/home/marnix/Zotero/storage/762SMQUC/Baggenstoss - 2018 - Beyond Moments Extending the Maximum Entropy Prin.pdf;/home/marnix/Zotero/storage/XZL5DNGA/650.html}
}

@article{Bailey1994,
  title = {A Fast Method for the Numerical Evaluation of Continuous {{Fourier}} and {{Laplace}} Transforms},
  author = {Bailey, David H and Swarztrauber, Paul N},
  year = {1994},
  journal = {SIAM Journal on Scientific Computing},
  volume = {15},
  number = {5},
  pages = {1105--1110},
  publisher = {SIAM},
  file = {/home/marnix/Zotero/storage/FKXVR2PK/Bailey and Swarztrauber - 1994 - A fast method for the numerical evaluation of cont.pdf}
}

@article{Banerjee2008,
  title = {On the Spectrum of the Normalized Graph {{Laplacian}}},
  author = {Banerjee, Anirban and Jost, J{\"u}rgen},
  year = {2008},
  month = may,
  journal = {Linear Algebra and Its Applications},
  volume = {428},
  number = {11-12},
  eprint = {0705.3772},
  pages = {3015--3022},
  issn = {00243795},
  doi = {10.1016/j.laa.2008.01.029},
  abstract = {We investigate how the spectrum of the normalized (geometric) graph Laplacian is affected by operations like motif dougling, graph splitting or joining. The multiplicity of the eigenvalue 1, or equivalently, the dimension of the kernel of the adjacency matrix of the graph is of particular interest. This multiplicity can be increased, for instance, by motif doubling. {\copyright} 2008 Elsevier Inc. All rights reserved.},
  archiveprefix = {arXiv},
  arxivid = {0705.3772},
  keywords = {discrete-laplacian,Eigenvalue 1,graph,Graph Laplacian,Graph spectrum,Motif doubling,review},
  file = {/home/marnix/Zotero/storage/TKF678DF/Banerjee, Jost - 2008 - On the spectrum of the normalized graph Laplacian.pdf}
}

@article{Barabasi2000,
  title = {Scale-Free Characteristics of Random Networks: The Topology of the World-Wide Web},
  author = {Barabasi, Albert-Laszlo and Albert, Reka and Jeong, Hawoong},
  year = {2000},
  month = jun,
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {281},
  number = {1-4},
  pages = {69--77},
  doi = {10.1016/s0378-4371(00)00018-2},
  abstract = {The world-wide web forms a large directed graph, whose vertices are documents and edges are links pointing from one document to another. Here we demonstrate that despite its apparent random character, the topology of this graph has a number of universal scale-free characteristics. We introduce a model that leads to a scale-free network, capturing in a minimal fashion the self-organization processes governing the world-wide web.},
  keywords = {networks,power-law,scale-free,webgraph}
}

@article{Barabasi2004,
  title = {Network Biology: Understanding the Cell's Functional Organization},
  author = {Barabasi, Albert-Laszlo and Oltvai, Zoltan N},
  year = {2004},
  journal = {Nature reviews genetics},
  volume = {5},
  number = {2},
  pages = {101--113},
  publisher = {Nature Publishing Group UK London}
}

@article{Bardoscia2016,
  title = {Distress Propagation in Complex Networks: {{The}} Case of Non-Linear {{DebtRank}}},
  author = {Bardoscia, Marco and Caccioli, Fabio and Perotti, Juan Ignacio and Vivaldo, Gianna and Caldarelli, Guido},
  year = {2016},
  journal = {PLoS ONE},
  volume = {11},
  number = {10},
  eprint = {1512.04460},
  pages = {1--8},
  issn = {19326203},
  doi = {10.1371/journal.pone.0163825},
  abstract = {We consider a dynamical model of distress propagation on complex networks, which we apply to the study of financial contagion in networks of banks connected to each other by direct exposures. The model that we consider is an extension of the DebtRank algorithm, recently introduced in the literature. The mechanics of distress propagation is very simple: When a bank suffers a loss, distress propagates to its creditors, who in turn suffer losses, and so on. The original DebtRank assumes that losses are propagated linearly between connected banks. Here we relax this assumption and introduce a one-parameter family of non-linear propagation functions. As a case study, we apply this algorithm to a data-set of 183 European banks, and we study how the stability of the system depends on the non-linearity parameter under different stress-test scenarios. We find that the system is characterized by a transition between a regime where small shocks can be amplified and a regime where shocks do not propagate, and that the overall stability of the system increases between 2008 and 2013.},
  archiveprefix = {arXiv},
  arxivid = {1512.04460},
  pmid = {27701457},
  file = {/home/marnix/Zotero/storage/4QEK3LED/Bardoscia et al. - 2016 - Distress propagation in complex networks The case of non-linear DebtRank.pdf}
}

@article{Bargigli2015,
  title = {The Multiplex Structure of Interbank Networks},
  author = {Bargigli, Leonardo and Di Iasio, Giovanni and Infante, Luigi and Lillo, Fabrizio and Pierobon, Federico},
  year = {2015},
  journal = {Quantitative Finance},
  volume = {15},
  number = {4},
  pages = {673--691},
  publisher = {Taylor & Francis},
  file = {/home/marnix/Zotero/storage/6XTUE7V9/Bargigli et al. - 2015 - The multiplex structure of interbank networks.pdf}
}

@article{Barney2007,
  title = {The Effect of Glottal Opening on the Acoustic Response of the Vocal Tract},
  author = {Barney, Anna and De Stefano, Antonio and Henrich, Nathalie},
  year = {2007},
  journal = {Acta Acustica united with Acustica},
  volume = {93},
  number = {6},
  pages = {1046--1056},
  publisher = {S. Hirzel Verlag},
  file = {/home/marnix/Zotero/storage/4L92YFH3/Barney et al. - 2007 - The effect of glottal opening on the acoustic resp.pdf}
}

@article{Barreda2021,
  title = {Fast {{Track}}: Fast (Nearly) Automatic Formant-Tracking Using {{Praat}}},
  shorttitle = {Fast {{Track}}},
  author = {Barreda, Santiago},
  year = {2021},
  month = jan,
  journal = {Linguistics Vanguard},
  volume = {7},
  number = {1},
  publisher = {De Gruyter Mouton},
  issn = {2199-174X},
  doi = {10.1515/lingvan-2020-0051},
  urldate = {2022-10-29},
  abstract = {Fast Track is a formant tracker implemented in Praat that attempts to automatically select the best analysis from a set of candidates. The best track is selected by modeling smooth formant contours across the entirety of the sound, providing the researcher with rich information about static and dynamic formant properties. Fast Track returns text files containing acoustic information (formant frequencies, formant bandwidths, fundamental frequency, etc.) sampled every 2~ms, generates images showing the winning analysis and comparing alternate analyses, and creates log files detailing analysis information for each file. Fast Track features a modular workflow that allows for analysis steps to be run (and re-run) independently as necessary, and is designed to allow for easy correction of tracking errors by allowing the user to override the automatic analysis, or manually edit tracks where necessary. In addition, Fast Track includes tools to aggregate data across tokens, and to easily create vowel plots of mean values or time-varying formant contours. The design and use of Fast Track are outlined using a re-analysis of the Hillenbrand et~al. (1995) dataset, which suggests that Fast Track can be very accurate in cases where signal properties allow for reliable formant estimates.},
  langid = {english},
  keywords = {formant tracking,formants,Praat,vowels}
}

@book{Barthes1981,
  title = {Camera Lucida: {{Reflections}} on Photography},
  author = {Barthes, Roland},
  year = {1981},
  publisher = {Macmillan}
}

@article{Barucca2016,
  title = {Disentangling Bipartite and Core-Periphery Structure in Financial Networks},
  author = {Barucca, Paolo and Lillo, Fabrizio},
  year = {2016},
  journal = {Chaos, Solitons and Fractals},
  volume = {88},
  eprint = {1511.08830},
  pages = {244--253},
  publisher = {Elsevier Ltd},
  issn = {09600779},
  doi = {10.1016/j.chaos.2016.02.004},
  abstract = {A growing number of systems are represented as networks whose architecture conveys significant information and determines many of their properties. Examples of network architecture include modular, bipartite, and core-periphery structures. However inferring the network structure is a non trivial task and can depend sometimes on the chosen null model. Here we propose a method for classifying network structures and ranking its nodes in a statistically well-grounded fashion. The method is based on the use of Belief Propagation for learning through Entropy Maximization on both the Stochastic Block Model (SBM) and the degree-corrected Stochastic Block Model (dcSBM). As a specific application we show how the combined use of the two ensembles - SBM and dcSBM - allows to disentangle the bipartite and the core-periphery structure in the case of the e-MID interbank network. Specifically we find that, taking into account the degree, this interbank network is better described by a bipartite structure, while using the SBM the core-periphery structure emerges only when data are aggregated for more than a week.},
  archiveprefix = {arXiv},
  arxivid = {1511.08830},
  keywords = {Belief propagation,Complex networks,Interbank markets,Statistical inference},
  file = {/home/marnix/Zotero/storage/7PLUIUKM/Barucca2016 Disentangling bipartite and core-petiphery structure in financial networks.pdf}
}

@article{Bassett2017,
  title = {Network Neuroscience},
  author = {Bassett, Danielle S and Sporns, Olaf},
  year = {2017},
  journal = {Nature neuroscience},
  volume = {20},
  number = {3},
  pages = {353--364},
  publisher = {Nature Publishing Group US New York}
}

@article{Battiston2014,
  title = {Structural Measures for Multiplex Networks},
  author = {Battiston, F and Nicosia, V and Latora, V},
  year = {2014},
  month = mar,
  journal = {Physical Review E},
  volume = {89},
  number = {3},
  eprint = {1308.3182},
  eprintclass = {physics.soc-ph},
  pages = {32804},
  doi = {10.1103/PhysRevE.89.032804},
  archiveprefix = {arXiv},
  arxivid = {physics.soc-ph/1308.3182},
  keywords = {Dynamics of social systems,Networks and genealogical trees},
  file = {/home/marnix/Zotero/storage/RZQUBIN7/Battiston, Nicosia, Latora - 2014 - Structural measures for multiplex networks.pdf}
}

@article{Battiston2016,
  title = {The New Challenges of Multiplex Networks: Measures and Models},
  author = {Battiston, F and Nicosia, V and Latora, V},
  year = {2016},
  month = jun,
  journal = {ArXiv e-prints},
  eprint = {1606.09221},
  eprintclass = {physics.soc-ph},
  archiveprefix = {arXiv},
  arxivid = {physics.soc-ph/1606.09221},
  keywords = {Computer Science - Social and Information Network,Computer Science - Social and Information Networks,Condensed Matter - Disordered Systems and Neural,Condensed Matter - Disordered Systems and Neural N,Condensed Matter - Statistical Mechanics,Physics - Physics and Society},
  file = {/home/marnix/Zotero/storage/Z2DIK6XK/Battiston, Nicosia, Latora - 2016 - The new challenges of multiplex networks measures and models.pdf}
}

@incollection{Bauer2016,
  title = {Understanding {{Probabilistic Sparse Gaussian Process Approximations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Bauer, Matthias and {van der Wilk}, Mark and Rasmussen, Carl Edward},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  year = {2016},
  pages = {1533--1541},
  publisher = {Curran Associates, Inc.},
  urldate = {2020-04-22},
  file = {/home/marnix/Zotero/storage/DM6R2LR4/Bauer et al. - 2016 - Understanding Probabilistic Sparse Gaussian Proces.pdf;/home/marnix/Zotero/storage/4X4BSU6H/6477-understanding-probabilistic-sparse-gaussian-process-approximations.html}
}

@inproceedings{Becker2008,
  title = {Forensic Speaker Verification Using Formant Features and {{Gaussian}} Mixture Models},
  booktitle = {Ninth {{Annual Conference}} of the {{International Speech Communication Association}}},
  author = {Becker, Timo and Jessen, Michael and Grigoras, Catalin},
  year = {2008},
  file = {/home/marnix/Zotero/storage/49P6D5NF/Becker et al. - Forensic Speaker Veriﬁcation Using Formant Feature.pdf}
}

@article{Bect2024,
  title = {Rational Kernel-Based Interpolation for Complex-Valued Frequency Response Functions},
  author = {Bect, Julien and Georg, Niklas and R{\"o}mer, Ulrich and Sch{\"o}ps, Sebastian},
  year = {2024},
  month = dec,
  journal = {SIAM Journal on Scientific Computing},
  volume = {46},
  number = {6},
  eprint = {2307.13484},
  primaryclass = {cs},
  pages = {A3727-A3755},
  issn = {1064-8275, 1095-7197},
  doi = {10.1137/23M1588901.},
  urldate = {2025-04-24},
  abstract = {This work is concerned with the kernel-based approximation of a complex-valued function from data, where the frequency response function of a partial differential equation in the frequency domain is of particular interest. In this setting, kernel methods are employed more and more frequently, however, standard kernels do not perform well. Moreover, the role and mathematical implications of the underlying pair of kernels, which arises naturally in the complex-valued case, remain to be addressed. We introduce new reproducing kernel Hilbert spaces of complex-valued functions, and formulate the problem of complex-valued interpolation with a kernel pair as minimum norm interpolation in these spaces. Moreover, we combine the interpolant with a low-order rational function, where the order is adaptively selected based on a new model selection criterion. Numerical results on examples from different fields, including electromagnetics and acoustic examples, illustrate the performance of the method, also in comparison to available rational approximation methods.},
  archiveprefix = {arXiv},
  keywords = {{Computer Science - Computational Engineering, Finance, and Science},Computer Science - Machine Learning,Computer Science - Numerical Analysis,Mathematics - Numerical Analysis},
  file = {/home/marnix/Zotero/storage/2DF65SI3/Bect et al. - 2024 - Rational kernel-based interpolation for complex-valued frequency response functions.pdf;/home/marnix/Zotero/storage/MXUWY9NR/2307.html}
}

@article{Bell1961,
  title = {Reduction of {{Speech Spectra}} by {{Analysis}}-by-{{Synthesis Techniques}}},
  author = {Bell, C. G. and Fujisaki, H. and Heinz, J. M. and Stevens, K. N. and House, A. S.},
  year = {1961},
  month = dec,
  journal = {The Journal of the Acoustical Society of America},
  volume = {33},
  number = {12},
  pages = {1725--1736},
  issn = {0001-4966},
  doi = {10.1121/1.1908556},
  urldate = {2019-11-11},
  langid = {english},
  file = {/home/marnix/Zotero/storage/5XAJWUTN/Bell et al. - 1961 - Reduction of Speech Spectra by Analysis‐by‐Synthes.pdf}
}

@article{Ben-Jamaa2008,
  title = {Analytical Tools for Optimizing the Error Correction Performance of Arithmetic Codes},
  author = {{Ben--Jamaa}, Salma and Kieffer, Michel and Weidmann, Claudio},
  year = {2008},
  month = sep,
  journal = {IEEE Transactions on Communications},
  volume = {56},
  number = {9},
  pages = {1458--1468},
  urldate = {2020-03-05},
  abstract = {In joint source-channel arithmetic coding (JSCAC) schemes, additional redundancy may be introduced into an arithmetic source code in order to be more robust against transmission errors. The purpose of this work is to provide analytical tools to predict and evaluate the effectiveness of that redundancy. Integer binary Arithmetic Coding (AC) is modeled by a reduced-state automaton in order to obtain a bit-clock trellis describing the encoding process. Considering AC as a trellis code, distance spectra are then derived. In particular, an algorithm to compute the free distance of an arithmetic code is proposed. The obtained code properties allow to compute upper bounds on both bit error and symbol error probabilities and thus provide an objective criterion to analyze the behavior of JSCAC schemes when used on noisy channels. This criterion is then exploited to design efficient error-correcting arithmetic codes. Simulation results highlight the validity of the theoretical error bounds and show that for equivalent rate and complexity, a simple optimization yields JSCACs that outperform classical tandem schemes at low to medium SNR.},
  file = {/home/marnix/Zotero/storage/SWEP8G27/Ben–Jamaa et al. - 2008 - Analytical tools for optimizing the error correcti.pdf}
}

@article{Benavoli2016,
  title = {State {{Space}} Representation of Non-Stationary {{Gaussian Processes}}},
  author = {Benavoli, Alessio and Zaffalon, Marco},
  year = {2016},
  month = jan,
  journal = {arXiv:1601.01544 [cs, stat]},
  eprint = {1601.01544},
  primaryclass = {cs, stat},
  urldate = {2021-12-08},
  abstract = {The state space (SS) representation of Gaussian processes (GP) has recently gained a lot of interest. The main reason is that it allows to compute GPs based inferences in O(n), where \$n\$ is the number of observations. This implementation makes GPs suitable for Big Data. For this reason, it is important to provide a SS representation of the most important kernels used in machine learning. The aim of this paper is to show how to exploit the transient behaviour of SS models to map non-stationary kernels to SS models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/7NE3CCKI/Benavoli and Zaffalon - 2016 - State Space representation of non-stationary Gauss.pdf;/home/marnix/Zotero/storage/8P2PDTWY/1601.html}
}

@article{Bengio2013,
  title = {Representation {{Learning}}: {{A Review}} and {{New Perspectives}}},
  shorttitle = {Representation {{Learning}}},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  year = {2013},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {35},
  number = {8},
  pages = {1798--1828},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2013.50},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
  keywords = {Abstracts,AI,Algorithms,artificial intelligence,Artificial Intelligence,autoencoder,autoencoders,Boltzmann machine,data representation,data structures,Deep learning,density estimation,Feature extraction,feature learning,geometrical connections,Humans,Learning systems,Machine learning,machine learning algorithms,manifold learning,Manifolds,neural nets,Neural networks,Neural Networks (Computer),probabilistic models,probability,representation learning,Speech recognition,unsupervised feature learning,unsupervised learning},
  file = {/home/marnix/Zotero/storage/V6D9ISJR/Bengio et al. - 2013 - Representation Learning A Review and New Perspect.pdf;/home/marnix/Zotero/storage/GD48U3FX/6472238.html}
}

@article{Bengio2021,
  title = {Deep Learning for {{AI}}},
  author = {Bengio, Yoshua and Lecun, Yann and Hinton, Geoffrey},
  year = {2021},
  month = jul,
  journal = {Communications of the ACM},
  volume = {64},
  number = {7},
  pages = {58--65},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3448250},
  urldate = {2025-10-03},
  abstract = {How can neural networks learn the rich internal representations required for difficult tasks such as recognizing objects or understanding language?},
  langid = {english},
  file = {/home/marnix/Zotero/storage/ZFFK3KMN/Bengio et al. - 2021 - Deep learning for AI.pdf}
}

@incollection{Benton2019,
  title = {Function-{{Space Distributions}} over {{Kernels}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Benton, Gregory and Maddox, Wesley J and Salkey, Jayson and Albinati, Julio and Wilson, Andrew Gordon},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {14965--14976},
  publisher = {Curran Associates, Inc.},
  urldate = {2020-09-14},
  file = {/home/marnix/Zotero/storage/NB6EZR2I/Benton et al. - 2019 - Function-Space Distributions over Kernels.pdf;/home/marnix/Zotero/storage/8XDS56SP/9634-function-space-distributions-over-kernels.html}
}

@article{Berger2006,
  title = {The Measurement of Bank Liquidity Creation and the Effect of Capital},
  author = {Berger, Allen},
  year = {2006},
  file = {/home/marnix/Zotero/storage/WKUEZEHW/Berger - 2006 - The measurement of bank liquidity creation and the effect of capital.pdf}
}

@book{Bernardo1994,
  title = {Bayesian Theory},
  author = {Bernardo, Jose M and Smith, Adrian F M},
  year = {1994},
  series = {Wiley Series in Probability and Mathematical Statistics},
  publisher = {Wiley},
  address = {Chichester},
  file = {/home/marnix/Zotero/storage/KG4QSSEE/Bernardo and Smith - 1994 - Bayesian theory.pdf}
}

@article{Bernardo1997,
  title = {Noninformative Priors Do Not Exist: A Discussion with {{Jos{\'e} M Bernardo}}},
  author = {Bernardo, {\relax JM}},
  year = {1997},
  journal = {Bernardo. J. Statist. Planning and Inference},
  file = {/home/marnix/Zotero/storage/3I937IKX/Bernardo - Noninformative Priors Do Not Exist A Discussion w.pdf}
}

@article{Betancourt2017,
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  year = {2017},
  eprint = {1701.02434},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous under- standing of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is con- fined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any ex- haustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
  archiveprefix = {arXiv},
  arxivid = {1701.02434},
  file = {/home/marnix/Zotero/storage/SPCZHH4B/Betancourt - 2017 - A Conceptual Introduction to Hamiltonian Monte Carlo.pdf}
}

@article{Bhat2010,
  title = {On the Derivation of the {{Bayesian Information Criterion}}},
  author = {Bhat, H S and Kumar, N},
  year = {2010},
  pages = {4},
  abstract = {We present a careful derivation of the Bayesian Inference Criterion (BIC) for model selection. The BIC is viewed here as an approximation to the Bayes Factor. One of the main ingredients in the approximation, the use of Laplace's method for approximating integrals, is explained well in the literature. Our derivation sheds light on this and other steps in the derivation, such as the use of a flat prior and the invocation of the weak law of large numbers, that are not often discussed in detail.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/L62YP9IQ/Bhat and Kumar - On the derivation of the Bayesian Information Crit.pdf}
}

@book{Binder2022,
  title = {Race, Class, and Mobility in {{US}} Marriage Markets},
  author = {Binder, Ariel J and Walker, Caroline and Eggleston, Jonathan and {Murray-Close}, Marta},
  year = {2022},
  publisher = {US Census Bureau, Center for Economic Studies}
}

@article{Blasques2015,
  title = {A {{Dynamic Network Model}} of the {{Unsecured Interbank Lending Market}}},
  author = {Blasques, Francisco and Br{\"a}uning, Falk and ! Lelyveld, Iman},
  year = {2015},
  month = feb,
  journal = {Social Science Research Network Working Paper Series},
  abstract = {We introduce a structural dynamic network model of the formation of lending relationships in the unsecured interbank market. Banks are subject to random liquidity shocks and can form links with potential trading partners to bilaterally Nash bargain about loan conditions. To reduce credit risk uncertainty, banks can engage in costly peer monitoring of counterparties. We estimate the structural model parameters by indirect inference using network statistics of the Dutch interbank market from 2008 to 2011. The estimated model accurately explains the high sparsity and stability of the lending network. In particular, peer monitoring and credit risk uncertainty are key factors in the formation of stable interbank lending relationships that are associated with improved credit conditions. Moreover, the estimated degree distribution of the lending network is highly skewed with a few very interconnected core banks and many peripheral banks that trade mainly with core banks. Shocks to credit risk uncertainty can lead to extended periods of low market activity, amplified by a reduction in peer monitoring. Finally, our monetary policy analysis shows that a wider interest rate corridor leads to a more active market through a direct effect on the outside options and an indirect multiplier effect by increasing banks' monitoring and search efforts.},
  file = {/home/marnix/Zotero/storage/HMNT7T66/Blasques, Bräuning, !van Lelyveld - 2015 - A Dynamic Network Model of the Unsecured Interbank Lending Market.pdf}
}

@article{Blei2017,
  title = {Variational {{Inference}}: {{A Review}} for {{Statisticians}}},
  shorttitle = {Variational {{Inference}}},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  year = {2017},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {112},
  number = {518},
  eprint = {1601.00670},
  pages = {859--877},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2017.1285773},
  urldate = {2020-07-01},
  abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/W8XU2WG6/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf;/home/marnix/Zotero/storage/QPUZ62IN/1601.html}
}

@article{Bleyer2017,
  title = {Alternating Minimisation for Glottal Inverse Filtering},
  author = {Bleyer, Ismael Rodrigo and Lybeck, Lasse and Auvinen, Harri and Airaksinen, Manu and Alku, Paavo and Siltanen, Samuli},
  year = {2017},
  month = may,
  journal = {Inverse Problems},
  volume = {33},
  number = {6},
  pages = {065005},
  publisher = {IOP Publishing},
  issn = {0266-5611},
  doi = {10.1088/1361-6420/aa6eb8},
  urldate = {2023-04-17},
  abstract = {A new method is proposed for solving the glottal inverse filtering (GIF) problem. The goal of GIF is to separate an acoustical speech signal into two parts: the glottal airflow excitation and the vocal tract filter. To recover such information one has to deal with a blind deconvolution problem. This ill-posed inverse problem is solved under a deterministic setting, considering unknowns on both sides of the underlying operator equation. A stable reconstruction is obtained using a double regularization strategy, alternating between fixing either the glottal source signal or the vocal tract filter. This enables not only splitting the nonlinear and nonconvex problem into two linear and convex problems, but also allows the use of the best parameters and constraints to recover each variable at a time. This new technique, called alternating minimization glottal inverse filtering (AM-GIF), is compared with two other approaches: Markov chain Monte Carlo glottal inverse filtering (MCMC-GIF), and iterative adaptive inverse filtering (IAIF), using synthetic speech signals. The recent MCMC-GIF has good reconstruction quality but high computational cost. The state-of-the-art IAIF method is computationally fast but its accuracy deteriorates, particularly for speech signals of high fundamental frequency (F0). The results show the competitive performance of the new method: With high F0, the reconstruction quality is better than that of IAIF and close to MCMC-GIF while reducing the computational complexity by two orders of magnitude.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/UPIDY3FD/Bleyer et al. - 2017 - Alternating minimisation for glottal inverse filte.pdf}
}

@article{Blitzstein2011,
  title = {A Sequential Importance Sampling Algorithm for Generating Random Graphs with Prescribed Degrees},
  author = {Blitzstein, Joseph and Diaconis, Persi},
  year = {2011},
  journal = {Internet Mathematics},
  volume = {6},
  number = {4},
  pages = {489--522},
  issn = {15427951},
  doi = {10.1080/15427951.2010.557277},
  abstract = {Random graphs with a given degree sequence are a useful model capturing several features absent in the classical Erd{\textacutedbl} os-R{\textasciiacute} enyi model, such as dependent edges and non-binomial degrees. In this paper, we use a characterization due to Erd{\textacutedbl} os and Gallai to develop a sequential algorithm for generating a random labeled graph with a given degree sequence. The algorithm is easy to implement and allows surprisingly efficient sequential importance sampling. Applications are given, in- cluding simulating a biological network and estimating the number of graphs with a given degree sequence.},
  keywords = {and phrases,graphical degree sequences,random graphs,random networks,randomized gen-},
  file = {/home/marnix/Zotero/storage/KZ335MIN/SIS for generating graphs with given degrees.pdf}
}

@article{Blomgren1998,
  title = {How Steady Are Vowel Steady-States?},
  author = {Blomgren, Michael and Robb, Michael},
  year = {1998},
  month = jan,
  journal = {Clinical Linguistics \& Phonetics},
  volume = {12},
  number = {5},
  pages = {405--415},
  issn = {0269-9206},
  doi = {10.1080/02699209808985234},
  urldate = {2019-05-20},
  abstract = {The duration of vowel steady-states (VSS) was examined acoustically in the speech production of 40 normal young adults. VSS was assessed according to formant frequency changes in sustained /i/ productions and consonant +/i/ + /d/(/Cid/) productions. The duration of the VSS was measured for the first and second formants (Fl and F2) by incorporating a fixed rate-of-change criterion. Results indicated no significant differences in VSS duration according to gender or vowel context. VSS duration based on Fl was significantly longer than F2 VSS duration. The duration of VSS was also found to be correlated to the overall vowel duration in /Cid/ contexts. Discussion focuses on the analysis and application of VSS in acoustic studies of normal and disordered speech production.},
  keywords = {acoustics,formant frequency,linear predictive coding,vowel steady-state},
  file = {/home/marnix/Zotero/storage/G6QMLUM2/Blomgren and Robb - 1998 - How steady are vowel steady-states.pdf;/home/marnix/Zotero/storage/L4VJ55LC/02699209808985234.html}
}

@article{Blondel2021,
  title = {Efficient and {{Modular Implicit Differentiation}}},
  author = {Blondel, Mathieu and Berthet, Quentin and Cuturi, Marco and Frostig, Roy and Hoyer, Stephan and {Llinares-L{\'o}pez}, Felipe and Pedregosa, Fabian and Vert, Jean-Philippe},
  year = {2021},
  month = oct,
  journal = {arXiv:2105.15183 [cs, math, stat]},
  eprint = {2105.15183},
  primaryclass = {cs, math, stat},
  urldate = {2022-04-23},
  abstract = {Automatic differentiation (autodiff) has revolutionized machine learning. It allows expressing complex computations by composing elementary ones in creative ways and removes the burden of computing their derivatives by hand. More recently, differentiation of optimization problem solutions has attracted widespread attention with applications such as optimization layers, and in bi-level problems such as hyper-parameter optimization and meta-learning. However, so far, implicit differentiation remained difficult to use for practitioners, as it often required case-by-case tedious mathematical derivations and implementations. In this paper, we propose a unified, efficient and modular approach for implicit differentiation of optimization problems. In our approach, the user defines directly in Python a function \$F\$ capturing the optimality conditions of the problem to be differentiated. Once this is done, we leverage autodiff of \$F\$ and implicit differentiation to automatically differentiate the optimization problem. Our approach thus combines the benefits of implicit differentiation and autodiff. It is efficient as it can be added on top of any state-of-the-art solver and modular as the optimality condition specification is decoupled from the implicit differentiation mechanism. We show that seemingly simple principles allow to recover many exiting implicit differentiation methods and create new ones easily. We demonstrate the ease of formulating and solving bi-level optimization problems using our framework. We also showcase an application to the sensitivity analysis of molecular dynamics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/IWM6NJTV/Blondel et al. - 2021 - Efficient and Modular Implicit Differentiation.pdf;/home/marnix/Zotero/storage/A6CWZITA/2105.html}
}

@inproceedings{Blythe2023,
  title = {Artificial Design Fiction: {{Using AI}} as a Material for Pastiche Scenarios},
  booktitle = {Proceedings of the 26th International Academic Mindtrek Conference},
  author = {Blythe, Mark},
  year = {2023},
  pages = {195--206},
  publisher = {ACM},
  address = {Tampere, Finland},
  doi = {10.1145/3616961.3616987}
}

@article{Boccaletti2014,
  title = {The Structure and Dynamics of Multilayer Networks},
  author = {Boccaletti, S and Bianconi, G and Criado, R and {del Genio}, C and {G{\'o}mez-Garde{\~n}es}, J and Romance, M and {Sendi{\~n}a-Nadal}, I and Wang, Z and Zanin, M},
  year = {2014},
  month = nov,
  journal = {Physics Reports},
  volume = {544},
  number = {1},
  eprint = {1407.0742},
  pages = {1--122},
  issn = {0370-1573},
  doi = {10.1016/j.physrep.2014.07.001},
  abstract = {In the past years, network theory has successfully characterized the interaction among the constituents of a variety of complex systems, ranging from biological to technological, and social systems. However, up until recently, attention was almost exclusively given to networks in which all components were treated on equivalent footing, while neglecting all the extra information about the temporal- or context-related properties of the interactions under study. Only in the last years, taking advantage of the enhanced resolution in real data sets, network scientists have directed their interest to the multiplex character of real-world systems, and explicitly considered the time-varying and multilayer nature of networks. We offer here a comprehensive review on both structural and dynamical organization of graphs made of diverse relationships (layers) between its constituents, and cover several relevant issues, from a full redefinition of the basic structural measures, to understanding how the multilayer nature of the network affects processes and dynamics.},
  archiveprefix = {arXiv},
  arxivid = {1407.0742},
  keywords = {multilayer-networks,review},
  file = {/home/marnix/Zotero/storage/89P3B3QA/Boccaletti et al. - 2014 - The structure and dynamics of multilayer networks.pdf}
}

@article{Boersma2001,
  title = {Praat, a System for Doing Phonetics by Computer},
  author = {Boersma, Paul},
  year = {2001},
  volume = {5},
  number = {9},
  pages = {341--345}
}

@article{Boersma2009,
  title = {Should Jitter Be Measured by Peak Picking or by Waveform Matching},
  author = {Boersma, Paul},
  year = {2009},
  journal = {Folia Phoniatrica et Logopaedica},
  volume = {61},
  number = {5},
  pages = {305--308},
  publisher = {Karger Publishers},
  file = {/home/marnix/Zotero/storage/I3BI6KVV/Boersma and others - 2009 - Should jitter be measured by peak picking or by wa.pdf}
}

@article{Bonastre2015,
  title = {Forensic Speaker Recognition: {{Mirages}} and Reality},
  author = {Bonastre, Jean-Fran{\c c}ois and Kahn, Juliette and Rossato, Solange and Ajili, Moez},
  year = {2015},
  journal = {S. Fuchs/D},
  pages = {255},
  file = {/home/marnix/Zotero/storage/KFCP9CHM/Forensic Speaker Recognition Mirages and Reality.pdf}
}

@inproceedings{Bontekoe2006,
  title = {Scheduling for Schools},
  booktitle = {{{AIP}} Conference Proceedings},
  author = {Bontekoe, Tj Romke and Kester, Do and Skilling, John},
  year = {2006},
  volume = {872},
  pages = {525--532},
  organization = {American Institute of Physics},
  file = {/home/marnix/Zotero/storage/2R3LPVZS/Bontekoe et al. - 2006 - Scheduling for schools.pdf}
}

@inproceedings{Botts2013,
  title = {Nested Sampling in Practice},
  booktitle = {{{ICA}} 2013 {{Montreal}}},
  author = {Botts, Jonathan},
  year = {2013},
  pages = {055091--055091},
  address = {Montreal, Canada},
  doi = {10.1121/1.4800875},
  urldate = {2020-03-06},
  langid = {english},
  file = {/home/marnix/Zotero/storage/JKQVXXEH/Botts - 2013 - Nested sampling in practice.pdf}
}

@article{Botts2013a,
  title = {Design of {{IIR Filters With Bayesian Model Selection}} and {{Parameter Estimation}}},
  author = {Botts, Jonathan and Escolano, Jos{\'e} and Xiang, Ning},
  year = {2013},
  month = mar,
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {21},
  number = {3},
  pages = {669--674},
  issn = {1558-7924},
  doi = {10.1109/TASL.2012.2226159},
  abstract = {Bayesian model selection and parameter estimation are used to address the problem of choosing the most concise filter order for a given application while simultaneously determining the associated filter coefficients. This approach is validated against simulated data and used to generate pole-zero representations of head-related transfer functions.},
  keywords = {Autoregressive processes,Bayesian methods,Bayesian model selection,concise Illter order,Data models,Frequency domain analysis,head-related transfer function,head-related transfer functions,IIR filters,IIR filters design,Mathematical model,model comparison,Monte Carlo methods,parameter estimation,Parameter estimation,pole-zero representations,Transfer functions},
  file = {/home/marnix/Zotero/storage/I694SDTE/Botts et al. - 2013 - Design of IIR Filters With Bayesian Model Selectio.pdf;/home/marnix/Zotero/storage/M8AVN7VW/6338273.html}
}

@misc{Bowman2023,
  title = {Eight {{Things}} to {{Know}} about {{Large Language Models}}},
  author = {Bowman, Samuel R.},
  year = {2023},
  month = apr,
  number = {arXiv:2304.00612},
  eprint = {2304.00612},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.00612},
  urldate = {2023-04-12},
  abstract = {The widespread public deployment of large language models (LLMs) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This paper surveys the evidence for eight potentially surprising such points: 1. LLMs predictably get more capable with increasing investment, even without targeted innovation. 2. Many important LLM behaviors emerge unpredictably as a byproduct of increasing investment. 3. LLMs often appear to learn and use representations of the outside world. 4. There are no reliable techniques for steering the behavior of LLMs. 5. Experts are not yet able to interpret the inner workings of LLMs. 6. Human performance on a task isn't an upper bound on LLM performance. 7. LLMs need not express the values of their creators nor the values encoded in web text. 8. Brief interactions with LLMs are often misleading.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/marnix/Zotero/storage/XJTWPQP8/Bowman - 2023 - Eight Things to Know about Large Language Models.pdf;/home/marnix/Zotero/storage/MVRQLM6U/2304.html}
}

@article{Box1973,
  title = {Bayesian Inference in Statistical Analysis},
  author = {Box, George EP and Tiao, George C},
  year = {1973},
  journal = {Reading, MA},
  file = {/home/marnix/Zotero/storage/8NYZVBK9/Box and Tiao - 1973 - Bayesian inference in statistical analysis.pdf}
}

@article{Boyd1997,
  title = {Integrating Error Detection into Arithmetic Coding},
  author = {Boyd, C. and Cleary, J.G. and Irvine, S.A. and {Rinsma-Melchert}, I. and Witten, I.H.},
  year = {1997},
  month = jan,
  journal = {IEEE Transactions on Communications},
  volume = {45},
  number = {1},
  pages = {1--3},
  issn = {00906778},
  doi = {10.1109/26.554275},
  urldate = {2020-03-06},
  abstract = {A technique to implement error detection as part of the arithmetic coding process is described. Heuristic arguments are given to show that a small amount of extra redundancy can be very effective in detecting errors very quickly, and practical tests confirm this prediction.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/MBU8GV87/Boyd et al. - 1997 - Integrating error detection into arithmetic coding.pdf}
}

@article{Bozkurt2007,
  title = {Chirp Group Delay Analysis of Speech Signals},
  author = {Bozkurt, Baris and Couvreur, Laurent and Dutoit, Thierry},
  year = {2007},
  volume = {49},
  pages = {159--176},
  doi = {10.1016/j.specom.2006.12.004},
  keywords = {automatic speech recognition,group delay processing,phase processing,spectral analysis,windowing},
  file = {/home/marnix/Zotero/storage/TPPBIJK5/Bozkurt2007 Chirp group delay analysis of speech signals.pdf}
}

@article{Bradbury2020,
  title = {{{JAX}}: Composable Transformations of {{Python}}+ {{NumPy}} Programs, 2018},
  author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and {Wanderman-Milne}, Skye},
  year = {2020},
  journal = {URL http://github. com/google/jax},
  volume = {4},
  pages = {16}
}

@article{Braunstein2006,
  title = {The {{Laplacian}} of a Graph as a Density {{Matrix}}: {{A}} Basic Combinatorial Approach to Separability of Mixed States},
  author = {Braunstein, Samuel L. and Ghosh, Sibasish and Severini, Simone},
  year = {2006},
  journal = {Annals of Combinatorics},
  volume = {10},
  number = {3},
  eprintclass = {quant-ph},
  pages = {291--317},
  publisher = {Springer},
  issn = {02180006},
  doi = {10.1007/s00026-006-0289-3},
  abstract = {We study entanglement properties of mixed density matrices obtained from combinatorial Laplacians. This is done by introducing the notion of the density matrix of a graph. We characterize the graphs with pure density matrices and show that the density matrix of a graph can be always written as a uniform mixture of pure density matrices of graphs. We consider the von Neumann entropy of these matrices and we characterize the graphs for which the minimum and maximum values are attained. We then discuss the problem of separability by pointing out that separability of density matrices of graphs does not always depend on the labelling of the vertices. We consider graphs with a tensor product structure and simple cases for which combinatorial properties are linked to the entanglement of the state. We calculate the concurrence of all graph on four vertices representing entangled states. It turns out that for some of these graphs the value of the concurrence is exactly fractional.},
  arxiv = {0406165},
  arxivid = {quant-ph/0406165},
  keywords = {Density matrix,Entanglement,Graph laplacian},
  file = {/home/marnix/Zotero/storage/XTQ35CV7/Braunstein, Ghosh, Severini - 2006 - The Laplacian of a graph as a density Matrix A basic combinatorial approach to separability of mixe.pdf}
}

@book{Bretthorst1988,
  title = {Bayesian Spectrum Analysis and Parameter Estimation},
  author = {Bretthorst, G. Larry},
  year = {1988},
  file = {/home/marnix/Zotero/storage/7EIPFPZ4/Bretthorst1988 Bayesian spectrum analysis and parameter estimation.pdf}
}

@incollection{Bretthorst1989,
  title = {Bayesian Spectrum Analysis on Quadrature {{NMR}} Data with Noise Correlations},
  booktitle = {Maximum {{Entropy}} and {{Bayesian Methods}}},
  author = {Bretthorst, G Larry},
  year = {1989},
  pages = {261--273},
  publisher = {Springer}
}

@article{Bretthorst1990,
  title = {Bayesian {{Analysis}}. {{III}}. {{Applications}} to {{NMR Signal Detection}}, {{Model Selection}} and {{Parameter Estimation}} d},
  author = {Bretthorst, G. Larry},
  year = {1990},
  journal = {Journal of Magnetic Resonance},
  volume = {88},
  pages = {571--595},
  file = {/home/marnix/Zotero/storage/B5DNELDC/iii.pdf}
}

@article{Bretthorst1990a,
  title = {Bayesian Analysis. {{II}}. {{Signal}} Detection and Model Selection},
  author = {Bretthorst, G Larry},
  year = {1990},
  journal = {Journal of Magnetic Resonance},
  volume = {88},
  number = {3},
  pages = {552--570},
  publisher = {Elsevier Science},
  file = {/home/marnix/Zotero/storage/BEHAP6WX/ii.pdf}
}

@article{Bretthorst1990b,
  title = {Bayesian Analysis. {{I}}. {{Parameter}} Estimation Using Quadrature {{NMR}} Models},
  author = {Bretthorst, G. Larry},
  year = {1990},
  month = jul,
  journal = {Journal of Magnetic Resonance (1969)},
  volume = {88},
  number = {3},
  pages = {533--551},
  issn = {00222364},
  doi = {10.1016/0022-2364(90)90287-J},
  urldate = {2022-07-15},
  langid = {english},
  file = {/home/marnix/Zotero/storage/TYDFT9LA/Bretthorst - 1990 - Bayesian analysis. I. Parameter estimation using q.pdf}
}

@article{Bretthorst1991,
  title = {Bayesian Analysis. {{IV}}. {{Noise}} and Computing Time Considerations},
  author = {Bretthorst, G Larry},
  year = {1991},
  journal = {Journal of Magnetic Resonance (1969)},
  volume = {93},
  number = {2},
  pages = {369--394},
  publisher = {Elsevier},
  file = {/home/marnix/Zotero/storage/DGDSHAQH/Bretthorst - 1991 - Bayesian analysis. IV. Noise and computing time co.pdf}
}

@inproceedings{Bretthorst1999,
  title = {The {{Near-Irrelevance}} of {{Sampling Frequency Distributions}}},
  booktitle = {Maximum {{Entropy}} and {{Bayesian Methods Garching}}, {{Germany}} 1998},
  author = {Bretthorst, G. Larry},
  editor = {{von der Linden}, Wolfgang and Dose, Volker and Fischer, Rainer and Preuss, Roland},
  year = {1999},
  series = {Fundamental {{Theories}} of {{Physics}}},
  pages = {21--46},
  publisher = {Springer Netherlands},
  abstract = {Jaynes, in his unfinished work on probability theory [2], discusses the Gaussian distribution extensively. In that discussion he demonstrates that when using a Gaussian distribution to estimate a location parameter, the only properties of the noise that enter the calculation are the first and second moments of the true noise; the underlying ensemble sampling distribution for the noise, the distribution from which the noise was actually sampled, completely cancels out of the inference. Exactly the same inferences will be made regardless of the underlying ensemble sampling distribution for the noise, provided the first and second moments of the true noise are the same. In this paper we review Jaynes' calculation, demonstrate it explicitly, generalize it to more complex models, and show that for regression style models the underlying ensemble sampling distribution for the noise is irrelevant to our inferences provided the mean and mean-square projection of the model onto the true noise are the same.},
  isbn = {978-94-011-4710-1},
  langid = {english},
  keywords = {Gaussian White Noise,Joint Probability,Maximum Entropy,Posterior Probability,Probability Density Function},
  file = {/home/marnix/Zotero/storage/PX78XQBF/Bretthorst - 1999 - The Near-Irrelevance of Sampling Frequency Distrib.pdf}
}

@article{Bretthorst2013,
  title = {The Maximum Entropy Method of Moments and {{Bayesian}} Probability Theory},
  author = {Bretthorst, G. Larry},
  year = {2013},
  volume = {3},
  number = {2013},
  pages = {3--15},
  doi = {10.1063/1.4819977},
  isbn = {9780735411791},
  keywords = {bayesian probability the-,density estimation,maximum entropy method of,moments},
  file = {/home/marnix/Zotero/storage/5QBH7734/Bretthorst2013 The Maximum Entropy Method Of Moments.pdf}
}

@article{Brewer2015,
  title = {Inference for {{Trans-dimensional Bayesian Models}} with {{Diffusive Nested Sampling}}},
  author = {Brewer, Brendon J.},
  year = {2015},
  month = jan,
  journal = {arXiv:1411.3921 [astro-ph, physics:physics, stat]},
  eprint = {1411.3921},
  primaryclass = {astro-ph, physics:physics, stat},
  urldate = {2021-09-06},
  abstract = {Many inference problems involve inferring the number \$N\$ of components in some region, along with their properties \${\textbackslash}\{{\textbackslash}mathbf\{x\}\_i{\textbackslash}\}\_\{i=1\}{\textasciicircum}N\$, from a dataset \${\textbackslash}mathcal\{D\}\$. A common statistical example is finite mixture modelling. In the Bayesian framework, these problems are typically solved using one of the following two methods: i) by executing a Monte Carlo algorithm (such as Nested Sampling) once for each possible value of \$N\$, and calculating the marginal likelihood or evidence as a function of \$N\$; or ii) by doing a single run that allows the model dimension \$N\$ to change (such as Markov Chain Monte Carlo with birth/death moves), and obtaining the posterior for \$N\$ directly. In this paper we present a general approach to this problem that uses trans-dimensional MCMC embedded within a Nested Sampling algorithm, allowing us to explore the posterior distribution and calculate the marginal likelihood (summed over \$N\$) even if the problem contains a phase transition or other difficult features such as multimodality. We present two example problems, finding sinusoidal signals in noisy data, and finding and measuring galaxies in a noisy astronomical image. Both of the examples demonstrate phase transitions in the relationship between the likelihood and the cumulative prior mass, highlighting the need for Nested Sampling.},
  archiveprefix = {arXiv},
  keywords = {{Physics - Data Analysis, Statistics and Probability},Astrophysics - Instrumentation and Methods for Astrophysics,Statistics - Computation},
  file = {/home/marnix/Zotero/storage/WQB5REHS/Brewer - 2015 - Inference for Trans-dimensional Bayesian Models wi.pdf;/home/marnix/Zotero/storage/QANDLTRG/1411.html}
}

@article{Brewer2017,
  title = {Computing {{Entropies With Nested Sampling}}},
  author = {Brewer, Brendon J.},
  year = {2017},
  month = aug,
  journal = {Entropy},
  volume = {19},
  number = {8},
  eprint = {1707.03543},
  pages = {422},
  issn = {1099-4300},
  doi = {10.3390/e19080422},
  urldate = {2022-04-17},
  abstract = {The Shannon entropy, and related quantities such as mutual information, can be used to quantify uncertainty and relevance. However, in practice, it can be difficult to compute these quantities for arbitrary probability distributions, particularly if the probability mass functions or densities cannot be evaluated. This paper introduces a computational approach, based on Nested Sampling, to evaluate entropies of probability distributions that can only be sampled. I demonstrate the method on three examples: a simple gaussian example where the key quantities are available analytically; (ii) an experimental design example about scheduling observations in order to measure the period of an oscillating signal; and (iii) predicting the future from the past in a heavy-tailed scenario.},
  archiveprefix = {arXiv},
  keywords = {{Physics - Data Analysis, Statistics and Probability},Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Information Theory,Statistics - Computation},
  file = {/home/marnix/Zotero/storage/RW463W66/Brewer - 2017 - Computing Entropies With Nested Sampling.pdf;/home/marnix/Zotero/storage/5GL5P86E/1707.html}
}

@article{Bruce2018,
  title = {Conditional Adaptive {{Bayesian}} Spectral Analysis of Nonstationary Biomedical Time Series},
  author = {Bruce, Scott A. and Hall, Martica H. and Buysse, Daniel J. and Krafty, Robert T.},
  year = {2018},
  journal = {Biometrics},
  volume = {74},
  number = {1},
  eprint = {1609.00696v2},
  pages = {260--269},
  issn = {15410420},
  doi = {10.1111/biom.12719},
  abstract = {Many studies of biomedical time series signals aim to measure the association between frequency-domain properties of time series and clinical and behavioral covariates. However, the time-varying dynamics of these associations are largely ignored due to a lack of methods that can assess the changing nature of the relationship through time. This article introduces a method for the simultaneous and automatic analysis of the association between the time-varying power spectrum and covariates. The procedure adaptively partitions the grid of time and covariate values into an unknown number of approximately stationary blocks and nonparametrically estimates local spectra within blocks through penalized splines. The approach is formulated in a fully Bayesian framework, in which the number and locations of partition points are random, and fit using reversible jump Markov chain Monte Carlo techniques. Estimation and inference averaged over the distribution of partitions allows for the accurate analysis of spectra with both smooth and abrupt changes. The proposed methodology is used to analyze the association between the time-varying spectrum of heart rate variability and self-reported sleep quality in a study of older adults serving as the primary caregiver for their ill spouse.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1609.00696v2},
  keywords = {Heart rate variability,Locally stationary,Replicated time series,Reversible jump Markov chain Monte Carlo,Sleep quality,Spectrum analysis,Whittle likelihood},
  file = {/home/marnix/Zotero/storage/WLUHEYQ2/Bruce2018 Conditional adaptive Bayesian spectral analysis of nonstationary biomedical time series.pdf}
}

@misc{Bubeck2023,
  title = {Sparks of {{Artificial General Intelligence}}: {{Early}} Experiments with {{GPT-4}}},
  shorttitle = {Sparks of {{Artificial General Intelligence}}},
  author = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  year = {2023},
  month = apr,
  number = {arXiv:2303.12712},
  eprint = {2303.12712},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-04-25},
  abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/marnix/Zotero/storage/VKWF2LBP/Bubeck et al. - 2023 - Sparks of Artificial General Intelligence Early e.pdf;/home/marnix/Zotero/storage/I7DTVDC8/2303.html}
}

@article{Buchner2021,
  title = {Nested {{Sampling Methods}}},
  author = {Buchner, Johannes},
  year = {2021},
  month = jul,
  journal = {arXiv:2101.09675 [astro-ph, stat]},
  eprint = {2101.09675},
  primaryclass = {astro-ph, stat},
  urldate = {2021-08-19},
  abstract = {Nested sampling (NS) computes parameter posterior distributions and makes Bayesian model comparison computationally feasible. Its strengths are the unsupervised navigation of complex, potentially multi-modal posteriors until a well-defined termination point. A systematic literature review of nested sampling algorithms and variants is presented. We focus on complete algorithms, including solutions to likelihood-restricted prior sampling, parallelisation, termination and diagnostics. The relation between number of live points, dimensionality and computational cost is studied for two complete algorithms. A new formulation of NS is presented, which casts the parameter space exploration as a search on a tree. Previously published ways of obtaining robust error estimates and dynamic variations of the number of live points are presented as special cases of this formulation. A new on-line diagnostic test is presented based on previous insertion rank order work. The survey of nested sampling methods concludes with outlooks for future research.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Statistics - Computation},
  file = {/home/marnix/Zotero/storage/UGFUJD8D/Buchner - 2021 - Nested Sampling Methods.pdf;/home/marnix/Zotero/storage/65J3IIXC/2101.html}
}

@article{Buchner2023,
  title = {Snowballing {{Nested Sampling}}},
  author = {Buchner, Johannes},
  year = {2023},
  journal = {Physical Sciences Forum},
  volume = {9},
  number = {1},
  pages = {17},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2673-9984},
  doi = {10.3390/psf2023009017},
  urldate = {2024-01-05},
  abstract = {A new way to run nested sampling, combined with realistic MCMC proposals to generate new live points, is presented. Nested sampling is run with a fixed number of MCMC steps. Subsequently, snowballing nested sampling extends the run to more and more live points. This stabilizes the MCMC proposal of later MCMC proposals, and leads to pleasant properties, including that the number of live points and number of MCMC steps do not have to be calibrated, that the evidence and posterior approximation improve as more compute is added and can be diagnosed with convergence diagnostics from the MCMC community. Snowballing nested sampling converges to a ``perfect'' nested sampling run with an infinite number of MCMC steps.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Markov Chain Monte Carlo,nested sampling},
  file = {/home/marnix/Zotero/storage/5MRZCJK6/Buchner - 2023 - Snowballing Nested Sampling.pdf}
}

@book{Buck1991,
  title = {Maximum Entropy in Action: A Collection of Expository Essays},
  author = {Buck, Brian and Macaulay, Vincent A and others},
  year = {1991},
  publisher = {Oxford: Clarendon Press; New York: Oxford University Press},
  file = {/home/marnix/Zotero/storage/HD6CG8DR/Maximum Entropy in Action A Collection of Expository Essays by Brian Buck, Vincent A. Macaulay (eds) (z-lib.org).djvu}
}

@article{Bui2016,
  title = {A Unifying Framework for Sparse Gaussian Process Approximation Using Power Expectation Propagation},
  author = {Bui, Thang D and Yan, Josiah and Turner, Richard E},
  year = {2016},
  journal = {stat},
  volume = {23},
  number = {1050}
}

@article{Buscicchio2019,
  title = {Label {{Switching Problem}} in {{Bayesian Analysis}} for {{Gravitational Wave Astronomy}}},
  author = {Buscicchio, Riccardo and Roebber, Elinore and Goldstein, Janna M. and Moore, Christopher J.},
  year = {2019},
  month = oct,
  journal = {Physical Review D},
  volume = {100},
  number = {8},
  eprint = {1907.11631},
  pages = {084041},
  issn = {2470-0010, 2470-0029},
  doi = {10.1103/PhysRevD.100.084041},
  urldate = {2021-07-09},
  abstract = {The label switching problem arises in the Bayesian analysis of models containing multiple indistinguishable parameters with arbitrary ordering. Any permutation of these parameters is equivalent, therefore models with many such parameters have extremely multi-modal posterior distributions. It is difficult to sample efficiently from such posteriors. This paper discusses a solution to this problem which involves carefully mapping the input parameter space to a high dimensional hypertriangle. It is demonstrated that this solution is efficient even for large numbers of parameters and can be easily applied alongside any stochastic sampling algorithm. This method is illustrated using two example problems from the field of gravitational wave astronomy.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
  file = {/home/marnix/Zotero/storage/PJU3ZGDD/Buscicchio et al. - 2019 - Label Switching Problem in Bayesian Analysis for G.pdf;/home/marnix/Zotero/storage/7B9ZMZLF/1907.html}
}

@inproceedings{Cabezas2023,
  title = {Transport {{Elliptical Slice Sampling}}},
  booktitle = {Proceedings of {{The}} 26th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Cabezas, Alberto and Nemeth, Christopher},
  year = {2023},
  month = apr,
  pages = {3664--3676},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-02-13},
  abstract = {We propose a new framework for efficiently sampling from complex probability distributions using a combination of normalizing flows and elliptical slice sampling (Murray et al., 2010). The central idea is to learn a diffeomorphism, through normalizing flows, that maps the non-Gaussian structure of the target distribution to an approximately Gaussian distribution. We then use the elliptical slice sampler, an efficient and tuning-free Markov chain Monte Carlo (MCMC) algorithm, to sample from the transformed distribution. The samples are then pulled back using the inverse normalizing flow, yielding samples that approximate the stationary target distribution of interest. Our transport elliptical slice sampler (TESS) is optimized for modern computer architectures, where its adaptation mechanism utilizes parallel cores to rapidly run multiple Markov chains for a few iterations. Numerical demonstrations show that TESS produces Monte Carlo samples from the target distribution with lower autocorrelation compared to non-transformed samplers, and demonstrates significant improvements in efficiency when compared to gradient-based proposals designed for parallel computer architectures, given a flexible enough diffeomorphism.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/NW2JGHAW/Cabezas and Nemeth - 2023 - Transport Elliptical Slice Sampling.pdf}
}

@article{Cacciatore2020,
  title = {Potential {{Mechanisms}} of the {{Alexander Technique}}: {{Toward}} a {{Comprehensive Neurophysiological Model}}},
  author = {Cacciatore, Timothy W and Johnson, Patrick M and Cohen, Rajal G},
  year = {2020},
  pages = {15},
  abstract = {The Alexander technique (AT) has been practiced for over 125 years. Despite evidence of its clinical utility, a clear explanation of how AT works is lacking, as the foundational science needed to test the underlying ideas has only recently become available. The authors propose that the core changes brought about by Alexander training are improvements in the adaptivity and distribution of postural tone, along with changes in body schema, and that these changes underlie many of the reported benefits. They suggest that AT alters tone and body schema via spatial attention and executive processes, which in turn affect low-level motor elements. To engage these pathways, AT strategically engages attention, intention, and inhibition, along with haptic communication. The uniqueness of the approach comes from the way these elements are woven together. Evidence for the contribution of these elements is discussed, drawing on direct studies of AT and other relevant modern scientific literature.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/5WI7UY53/Cacciatore et al. - Potential Mechanisms of the Alexander Technique T.pdf}
}

@book{Caccioli2018,
  title = {Network Models of Financial Systemic Risk: A Review},
  booktitle = {Journal of Computational Social Science},
  author = {Caccioli, Fabio and Barucca, Paolo and Kobayashi, Teruyoshi},
  year = {2018},
  volume = {1},
  publisher = {Springer Singapore},
  issn = {2432-2725},
  doi = {10.1007/s42001-017-0008-3},
  isbn = {4200101700083},
  keywords = {Clearin,Contagion,Financial networks,Systemic risk},
  file = {/home/marnix/Zotero/storage/8A3AJ8MG/Caccioli2018 Network models of financial systems.pdf}
}

@misc{Caffagni2024,
  title = {The ({{R}}){{Evolution}} of {{Multimodal Large Language Models}}: {{A Survey}}},
  shorttitle = {The ({{R}}){{Evolution}} of {{Multimodal Large Language Models}}},
  author = {Caffagni, Davide and Cocchi, Federico and Barsellotti, Luca and Moratelli, Nicholas and Sarto, Sara and Baraldi, Lorenzo and Baraldi, Lorenzo and Cornia, Marcella and Cucchiara, Rita},
  year = {2024},
  month = feb,
  number = {arXiv:2402.12451},
  eprint = {2402.12451},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.12451},
  urldate = {2024-04-18},
  abstract = {Connecting text and visual modalities plays an essential role in generative intelligence. For this reason, inspired by the success of large language models, significant research efforts are being devoted to the development of Multimodal Large Language Models (MLLMs). These models can seamlessly integrate visual and textual modalities, both as input and output, while providing a dialogue-based interface and instruction-following capabilities. In this paper, we provide a comprehensive review of recent visual-based MLLMs, analyzing their architectural choices, multimodal alignment strategies, and training techniques. We also conduct a detailed analysis of these models across a wide range of tasks, including visual grounding, image generation and editing, visual understanding, and domain-specific applications. Additionally, we compile and describe training datasets and evaluation benchmarks, conducting comparisons among existing models in terms of performance and computational requirements. Overall, this survey offers a comprehensive overview of the current state of the art, laying the groundwork for future MLLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  file = {/home/marnix/Zotero/storage/LJQKZADX/Caffagni et al. - 2024 - The (R)Evolution of Multimodal Large Language Mode.pdf;/home/marnix/Zotero/storage/CHZKQGFH/2402.html}
}

@inproceedings{Calandra2016,
  title = {Manifold {{Gaussian Processes}} for Regression},
  booktitle = {2016 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Calandra, Roberto and Peters, Jan and Rasmussen, Carl Edward and Deisenroth, Marc Peter},
  year = {2016},
  month = jul,
  pages = {3338--3345},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2016.7727626},
  abstract = {Off-the-shelf Gaussian Process (GP) covariance functions encode smoothness assumptions on the structure of the function to be modeled. To model complex and non-differentiable functions, these smoothness assumptions are often too restrictive. One way to alleviate this limitation is to find a different representation of the data by introducing a feature space. This feature space is often learned in an unsupervised way, which might lead to data representations that are not useful for the overall regression task. In this paper, we propose Manifold Gaussian Processes, a novel supervised method that jointly learns a transformation of the data into a feature space and a GP regression from the feature space to observed space. The Manifold GP is a full GP and allows to learn data representations, which are useful for the overall regression task. As a proof-of-concept, we evaluate our approach on complex non-smooth functions where standard GPs perform poorly, such as step functions and robotics tasks with contacts.},
  keywords = {Bayes methods,complex nonsmooth functions,Computational modeling,data handling,data representation,Gaussian processes,GP regression,manifold Gaussian processes,Manifolds,regression analysis,Standards,Supervised learning,supervised method,Training},
  file = {/home/marnix/Zotero/storage/JFK2FD7V/Calandra et al. - 2016 - Manifold Gaussian Processes for regression.pdf;/home/marnix/Zotero/storage/NKIGN8L4/7727626.html}
}

@article{Calcaterra2008,
  title = {Approximating with {{Gaussians}}},
  author = {Calcaterra, Craig and Boldt, Axel},
  year = {2008},
  month = may,
  journal = {arXiv:0805.3795 [math]},
  eprint = {0805.3795},
  primaryclass = {math},
  urldate = {2019-09-04},
  abstract = {Linear combinations of translations of a single Gaussian, e{\textasciicircum}\{-x{\textasciicircum}2\}, are shown to be dense in L{\textasciicircum}2(R). Two algorithms for determining the coefficients for the approximations are given, using orthogonal Hermite functions and least squares. Taking the Fourier transform of this result shows low-frequency trigonometric series are dense in L{\textasciicircum}2 with Gaussian weight function.},
  archiveprefix = {arXiv},
  keywords = {41A30,42A32,42C10,Mathematics - Classical Analysis and ODEs,Mathematics - Functional Analysis},
  file = {/home/marnix/Zotero/storage/E2K8TPHP/Calcaterra and Boldt - 2008 - Approximating with Gaussians.pdf;/home/marnix/Zotero/storage/LFUR2HMC/0805.html}
}

@inproceedings{Calcaterra2008a,
  title = {Linear Combinations of Gaussians with a Single Variance Are Dense in L2},
  booktitle = {Proceedings of the World Congress on Engineering},
  author = {Calcaterra, Craig},
  year = {2008},
  volume = {2},
  file = {/home/marnix/Zotero/storage/M835F2DC/Calcaterra - 2008 - Linear combinations of gaussians with a single var.pdf}
}

@misc{Callaghan2024,
  title = {Dear {{Kelly}}},
  author = {Callaghan, Andrew and Liedgren, Elliot},
  year = {2024},
  publisher = {Channel 5},
  howpublished = {Film}
}

@article{Calvetti2018,
  title = {Inverse Problems: {{From}} Regularization to {{Bayesian}} Inference},
  shorttitle = {Inverse Problems},
  author = {Calvetti, D. and Somersalo, E.},
  year = {2018},
  journal = {WIREs Computational Statistics},
  volume = {10},
  number = {3},
  pages = {e1427},
  issn = {1939-0068},
  doi = {10.1002/wics.1427},
  urldate = {2023-07-28},
  abstract = {Inverse problems deal with the quest for unknown causes of observed consequences, based on predictive models, known as the forward models, that associate the former quantities to the latter in the causal order. Forward models are usually well-posed, as causes determine consequences in a unique and stable way. Inverse problems, on the other hand, are usually ill-posed: the data may be insufficient to identify the cause unambiguously, an exact solution may not exist, and, like in a mystery story, discovering the cause without extra information tends to be highly sensitive to measurement noise and modeling errors. The Bayesian methodology provides a versatile and natural way of incorporating extra information to supplement the noisy data by modeling the unknown as a random variable to highlight the uncertainty about its value. Presenting the solution in the form of a posterior distribution provides a wide range of possibilities to compute useful estimates. Inverse problems are traditionally approached from the point of view of regularization, a process whereby the ill-posed problem is replaced by a nearby well-posed one. While many of the regularization techniques can be reinterpreted in the Bayesian framework through prior design, the Bayesian formalism provides new techniques to enrich the paradigm of traditional inverse problems. In particular, inaccuracies and inadequacies of the forward model are naturally handled in the statistical framework. Similarly, qualitative information about the solution may be reformulated in the form of priors with unknown parameters that can be successfully handled in the hierarchical Bayesian context. This article is categorized under: Statistical and Graphical Methods of Data Analysis {$>$} Bayesian Methods and Theory Algorithms and Computational Methods {$>$} Numerical Methods Applications of Computational Statistics {$>$} Computational Mathematics},
  copyright = {{\copyright} 2018 Wiley Periodicals, Inc.},
  langid = {english},
  keywords = {hierarchical Bayesian models,Ill-posedness,likelihood,measurement and modeling errors,posterior estimates,sparsity-promoting and sample-based priors,structural,Tikhonov regularization},
  file = {/home/marnix/Zotero/storage/QD9J3E2A/Calvetti and Somersalo - 2018 - Inverse problems From regularization to Bayesian .pdf;/home/marnix/Zotero/storage/E5ETNRFS/wics.html}
}

@article{Canazza2010,
  title = {Restoration of {{Audio Documents}} by {{Means}} of {{Extended Kalman Filter}}},
  author = {Canazza, Sergio and De Poli, Giovanni and Mian, Gian Antonio},
  year = {2010},
  month = aug,
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {18},
  number = {6},
  pages = {1107--1115},
  issn = {1558-7924},
  doi = {10.1109/TASL.2009.2030005},
  abstract = {We present some results on audio restoration obtained with an algorithm that solves the problems of broadband noise filtering, signal parameters tracking, and impulsive noise removal by using the Extended Kalman Filter (EKF) theory. We show that, to achieve maximum performance, it is essential to optimize the EKF implementation. To this purpose, having to cope with the nonstationarity of the audio signal, we use two properly combined EKF filters (forward and backward), and introduce a bootstrapping procedure for model tracking. The careful combination of the proposed techniques and an accurate choice of some critical parameters, allows to improve the performance of the EKF algorithm. The presented procedure is validated by listening tests.},
  keywords = {audio document restoration,Audio restoration,audio signal processing,bootstrapping,broadband noise filtering,EKF theory,extended Kalman filter,Extended Kalman Filter (EKF),Filtering algorithms,Filtering theory,Frequency estimation,identification of time-varying autoregressive processes,impulse noise,impulsive noise removal,Kalman filtering,Kalman filters,Power system restoration,signal parameters tracking,Signal processing algorithms,Signal restoration,Signal to noise ratio,Time domain analysis,Wiener filter},
  file = {/home/marnix/Zotero/storage/DTPS785D/Canazza et al. - 2010 - Restoration of Audio Documents by Means of Extende.pdf;/home/marnix/Zotero/storage/SIDXX6PF/5200506.html}
}

@inproceedings{Cao2002,
  title = {Parallel {{Cholesky}} Factorization of a Block Tridiagonal Matrix},
  booktitle = {Proceedings. {{International Conference}} on {{Parallel Processing Workshop}}},
  author = {Cao, T.D. and Hall, J.F. and {van de Geijn}, R.A.},
  year = {2002},
  month = aug,
  pages = {327--335},
  issn = {1530-2016},
  doi = {10.1109/ICPPW.2002.1039748},
  abstract = {We discuss the parallel implementation of the Cholesky factorization of a positive definite symmetric matrix when that matrix is block tridiagonal. While parallel implementations for this problem, and closely related problems like the factorization of banded matrices, have been previously reported in the literature, those implementations dealt with the special cases where the block size (bandwidth) was either very large (wide) or very small (narrow). We present a solution that can be used for the entire spectrum of cases, ranging from extremely large (wide) to very small (narrow). Preliminary performance results collected on a Cray T3E-600 distributed memory supercomputer show that our implementation attains respectable performance. Indeed, factorization of a matrix with block size b=1000 and a total dimension of more than 500,000 takes about 3.6 minutes on 128 processors.},
  keywords = {Bandwidth,Bridges,Civil engineering,Concurrent computing,Linear algebra,Linear systems,Memory architecture,Packaging,Supercomputers,Symmetric matrices},
  file = {/home/marnix/Zotero/storage/E3R7U5CL/Cao et al. - 2002 - Parallel Cholesky factorization of a block tridiag.pdf;/home/marnix/Zotero/storage/UFJJXSNS/1039748.html}
}

@misc{Cao2024,
  title = {Personalized {{Steering}} of {{Large Language Models}}: {{Versatile Steering Vectors Through Bi-directional Preference Optimization}}},
  shorttitle = {Personalized {{Steering}} of {{Large Language Models}}},
  author = {Cao, Yuanpu and Zhang, Tianrong and Cao, Bochuan and Yin, Ziyi and Lin, Lu and Ma, Fenglong and Chen, Jinghui},
  year = {2024},
  month = jul,
  number = {arXiv:2406.00045},
  eprint = {2406.00045},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.00045},
  urldate = {2025-09-15},
  abstract = {Researchers have been studying approaches to steer the behavior of Large Language Models (LLMs) and build personalized LLMs tailored for various applications. While fine-tuning seems to be a direct solution, it requires substantial computational resources and may significantly affect the utility of the original LLM. Recent endeavors have introduced more lightweight strategies, focusing on extracting "steering vectors" to guide the model's output toward desired behaviors by adjusting activations within specific layers of the LLM's transformer architecture. However, such steering vectors are directly extracted from the activations of human preference data and thus often lead to suboptimal results and occasional failures, especially in alignment-related scenarios. This work proposes an innovative approach that could produce more effective steering vectors through bi-directional preference optimization. Our method is designed to allow steering vectors to directly influence the generation probability of contrastive human preference data pairs, thereby offering a more precise representation of the target behavior. By carefully adjusting the direction and magnitude of the steering vector, we enabled personalized control over the desired behavior across a spectrum of intensities. Extensive experimentation across various open-ended generation tasks, particularly focusing on steering AI personas, has validated the efficacy of our approach. Moreover, we comprehensively investigate critical alignment-concerning scenarios, such as managing truthfulness, mitigating hallucination, and addressing jailbreaking attacks. Remarkably, our method can still demonstrate outstanding steering effectiveness across these scenarios. Furthermore, we showcase the transferability of our steering vectors across different models/LoRAs and highlight the synergistic benefits of applying multiple vectors simultaneously.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/marnix/Zotero/storage/G6HPX5IJ/Cao et al. - 2024 - Personalized Steering of Large Language Models Versatile Steering Vectors Through Bi-directional Pr.pdf;/home/marnix/Zotero/storage/96RATQKI/2406.html}
}

@article{Carhart-Harris2019,
  title = {{{REBUS}} and the {{Anarchic Brain}}: {{Toward}} a {{Unified Model}} of the {{Brain Action}} of {{Psychedelics}}},
  shorttitle = {{{REBUS}} and the {{Anarchic Brain}}},
  author = {{Carhart-Harris}, R. L. and Friston, K. J.},
  editor = {Barker, Eric L.},
  year = {2019},
  month = jul,
  journal = {Pharmacological Reviews},
  volume = {71},
  number = {3},
  pages = {316--344},
  publisher = {{American Society for Pharmacology and Experimental Therapeutics}},
  issn = {0031-6997, 1521-0081},
  doi = {10.1124/pr.118.017160},
  urldate = {2023-04-24},
  abstract = {This paper formulates the action of psychedelics by integrating the free-energy principle and entropic brain hypothesis. We call this formulation relaxed beliefs under psychedelics (REBUS) and the anarchic brain, founded on the principle that---via their entropic effect on spontaneous cortical activity---psychedelics work to relax the precision of high-level priors or beliefs, thereby liberating bottom-up information flow, particularly via intrinsic sources such as the limbic system. We assemble evidence for this model and show how it can explain a broad range of phenomena associated with the psychedelic experience. With regard to their potential therapeutic use, we propose that psychedelics work to relax the precision weighting of pathologically overweighted priors underpinning various expressions of mental illness. We propose that this process entails an increased sensitization of high-level priors to bottom-up signaling (stemming from intrinsic sources), and that this heightened sensitivity enables the potential revision and deweighting of overweighted priors. We end by discussing further implications of the model, such as that psychedelics can bring about the revision of other heavily weighted high-level priors, not directly related to mental health, such as those underlying partisan and/or overly-confident political, religious, and/or philosophical perspectives. Significance Statement Psychedelics are capturing interest, with efforts underway to bring psilocybin therapy to marketing authorisation and legal access within a decade, spearheaded by the findings of a series of phase 2 trials. In this climate, a compelling unified model of how psychedelics alter brain function to alter consciousness would have appeal. Towards this end, we have sought to integrate a leading model of global brain function, hierarchical predictive coding, with an often-cited model of the acute action of psychedelics, the entropic brain hypothesis. The resulting synthesis states that psychedelics work to relax high-level priors, sensitising them to liberated bottom-up information flow, which, with the right intention, care provision and context, can help guide and cultivate the revision of entrenched pathological priors.},
  chapter = {Review Article},
  copyright = {Copyright {\copyright} 2019 The Author(s).. This is an open access article distributed under the CC BY Attribution 4.0 International license.},
  langid = {english},
  pmid = {31221820},
  file = {/home/marnix/Zotero/storage/HWPGGK3L/Carhart-Harris and Friston - 2019 - REBUS and the Anarchic Brain Toward a Unified Mod.pdf}
}

@article{Carpenter2017,
  title = {Stan : {{A Probabilistic Programming Language}}},
  shorttitle = {{\emph{Stan}}},
  author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  year = {2017},
  journal = {Journal of Statistical Software},
  volume = {76},
  number = {1},
  issn = {1548-7660},
  doi = {10.18637/jss.v076.i01},
  urldate = {2021-01-13},
  abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/WP69329W/Carpenter et al. - 2017 - Stan  A Probabilistic Programming Language.pdf}
}

@inproceedings{Celeux1998,
  title = {Bayesian {{Inference}} for {{Mixture}}: {{The Label Switching Problem}}},
  shorttitle = {Bayesian {{Inference}} for {{Mixture}}},
  booktitle = {{{COMPSTAT}}},
  author = {Celeux, Gilles},
  editor = {Payne, Roger and Green, Peter},
  year = {1998},
  pages = {227--232},
  publisher = {Physica-Verlag HD},
  address = {Heidelberg},
  doi = {10.1007/978-3-662-01131-7_26},
  abstract = {A K-component mixture distribution is invariant to permutations of the labels of the components. As a consequence, in a Bayesian framework, the posterior distribution of the mixture parameters has theoretically K! modes. This fact involves possible difficulties when interpreting this posterior distribution. In this paper, we discuss the problem of labelling and we propose a simple and general clustering-like tool to deal with this problem.},
  isbn = {978-3-662-01131-7},
  langid = {english},
  keywords = {k-means algorithm,labelling latent structure,MCMC algorithm},
  file = {/home/marnix/Zotero/storage/I8XT7253/Celeux - 1998 - Bayesian Inference for Mixture The Label Switchin.pdf}
}

@article{Celeux2018,
  title = {Computational {{Solutions}} for {{Bayesian Inference}} in {{Mixture Models}}},
  author = {Celeux, Gilles and Kamary, Kaniav and {Malsiner-Walli}, Gertraud and Marin, Jean-Michel and Robert, Christian P.},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.07240 [stat]},
  eprint = {1812.07240},
  primaryclass = {stat},
  urldate = {2021-03-26},
  abstract = {This chapter surveys the most standard Monte Carlo methods available for simulating from a posterior distribution associated with a mixture and conducts some experiments about the robustness of the Gibbs sampler in high dimensional Gaussian settings. This is a chapter prepared for the forthcoming 'Handbook of Mixture Analysis'.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation},
  file = {/home/marnix/Zotero/storage/UYZS2JYH/Celeux et al. - 2018 - Computational Solutions for Bayesian Inference in .pdf;/home/marnix/Zotero/storage/XXIL2JU8/1812.html}
}

@article{Celeux2018a,
  title = {Model {{Selection}} for {{Mixture Models}} - {{Perspectives}} and {{Strategies}}},
  author = {Celeux, Gilles and {Fruewirth-Schnatter}, Sylvia and Robert, Christian P.},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.09885 [stat]},
  eprint = {1812.09885},
  primaryclass = {stat},
  urldate = {2021-04-08},
  abstract = {Determining the number G of components in a finite mixture distribution is an important and difficult inference issue. This is a most important question, because statistical inference about the resulting model is highly sensitive to the value of G. Selecting an erroneous value of G may produce a poor density estimate. This is also a most difficult question from a theoretical perspective as it relates to unidentifiability issues of the mixture model. This is further a most relevant question from a practical viewpoint since the meaning of the number of components G is strongly related to the modelling purpose of a mixture distribution. We distinguish in this chapter between selecting G as a density estimation problem in Section 2 and selecting G in a model-based clustering framework in Section 3. Both sections discuss frequentist as well as Bayesian approaches. We present here some of the Bayesian solutions to the different interpretations of picking the "right" number of components in a mixture, before concluding on the ill-posed nature of the question.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/home/marnix/Zotero/storage/AAWXJHES/Celeux et al. - 2018 - Model Selection for Mixture Models - Perspectives .pdf;/home/marnix/Zotero/storage/M7T4BXLJ/1812.html}
}

@article{Chaabouni2019,
  title = {Anti-Efficient Encoding in Emergent Communication},
  author = {Chaabouni, Rahma and Kharitonov, Eugene and Dupoux, Emmanuel and Baroni, Marco},
  year = {2019},
  journal = {Advances in Neural Information Processing Systems},
  volume = {32}
}

@misc{Chai2019,
  title = {Improving {{Quadrature}} for {{Constrained Integrands}}},
  author = {Chai, Henry and Garnett, Roman},
  year = {2019},
  month = feb,
  number = {arXiv:1802.04782},
  eprint = {1802.04782},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-01-08},
  abstract = {We present an improved Bayesian framework for performing inference of affine transformations of constrained functions. We focus on quadrature with nonnegative functions, a common task in Bayesian inference. We consider constraints on the range of the function of interest, such as nonnegativity or boundedness. Although our framework is general, we derive explicit approximation schemes for these constraints, and argue for the use of a log transformation for functions with high dynamic range such as likelihood surfaces. We propose a novel method for optimizing hyperparameters in this framework: we optimize the marginal likelihood in the original space, as opposed to in the transformed space. The result is a model that better explains the actual data. Experiments on synthetic and real-world data demonstrate our framework achieves superior estimates using less wall-clock time than existing Bayesian quadrature procedures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/6K8G7LCC/Chai and Garnett - 2019 - Improving Quadrature for Constrained Integrands.pdf;/home/marnix/Zotero/storage/UPYWSNWU/1802.html}
}

@incollection{Champagnat2010,
  title = {Deconvolution of {{Spike Trains}}},
  booktitle = {Bayesian {{Approach}} to {{Inverse Problems}}},
  author = {Champagnat, Fr{\'e}d{\'e}ric and Goussard, Yves and Gautier, St{\'e}phane and Idier, J{\'e}r{\^o}me},
  editor = {Idier, J{\'e}r{\^o}me},
  year = {2010},
  month = jan,
  pages = {117--140},
  publisher = {ISTE},
  address = {London, UK},
  doi = {10.1002/9780470611197.ch5},
  urldate = {2019-08-08},
  isbn = {978-0-470-61119-7 978-1-84821-032-5},
  file = {/home/marnix/Zotero/storage/PA397ISY/Champagnat et al. - 2010 - Deconvolution of Spike Trains.pdf}
}

@article{Chan2023,
  title = {Evidential Value of Voice Quality Acoustics in Forensic Voice Comparison},
  author = {Chan, Ricky K. W.},
  year = {2023},
  month = jul,
  journal = {Forensic Science International},
  volume = {348},
  pages = {111725},
  issn = {0379-0738},
  doi = {10.1016/j.forsciint.2023.111725},
  urldate = {2025-10-01},
  abstract = {Voice recordings in forensic voice comparison casework typically involve speech style mismatch and are separated by days or weeks, but studies that aim to empirically validate the evidential value of speech features rarely include systematic comparisons on contemporaneous vs. non-contemporaneous recordings and match vs. mismatch in speech style. This study addresses this gap and focuses on the acoustics of laryngeal voice quality, since voice quality has been reported to be one of the most popular and useful features for forensic voice comparison. 75 male speakers aged 18--45 were selected from a forensically-oriented database of Australian English speakers in Sydney/New South Wales. The evidential strength of a number of spectral tilt and additive noise parameters were tested under the Bayesian likelihood-ratio framework. Results show that system performance using these parameters as input were stable across 50 replications. When speech style is controlled for, VQ parameters yielded promising results and better system validity was achieved when using more VQ parameters. However, they offered limited speaker-discriminatory value when speech style mismatch is involved, and non-contemporaneous recordings only led to a small decline in performance. Overall, forensic practitioners should be cautious when using spectral tilt measures and additive noise measures as speaker discriminants in forensic casework.},
  keywords = {Forensic voice comparison,Likelihood-ratio,Non-contemporaneous recordings,Speech style mismatch,Voice quality},
  file = {/home/marnix/Zotero/storage/9PGPG4C9/Chan - 2023 - Evidential value of voice quality acoustics in forensic voice comparison.pdf;/home/marnix/Zotero/storage/DGU3TK6P/S0379073823001755.html}
}

@article{Charpentier2022,
  title = {Differentiable {{DAG}} Sampling},
  author = {Charpentier, Bertrand and Kibler, Simon and G{\"u}nnemann, Stephan},
  year = {2022},
  pages = {25},
  abstract = {We propose a new differentiable probabilistic model over DAGs (DP-DAG). DPDAG allows fast and differentiable DAG sampling suited to continuous optimization. To this end, DP-DAG samples a DAG by successively (1) sampling a linear ordering of the node and (2) sampling edges consistent with the sampled linear ordering. We further propose VI-DP-DAG, a new method for DAG learning from observational data which combines DP-DAG with variational inference. Hence, VI-DP-DAG approximates the posterior probability over DAG edges given the observed data. VI-DP-DAG is guaranteed to output a valid DAG at any time during training and does not require any complex augmented Lagrangian optimization scheme in contrast to existing differentiable DAG learning approaches. In our extensive experiments, we compare VI-DP-DAG to other differentiable DAG learning baselines on synthetic and real datasets. VI-DP-DAG significantly improves DAG structure and causal mechanism learning while training faster than competitors.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/GGV74R8J/Charpentier et al. - 2022 - Differentiable DAG sampling.pdf}
}

@article{Chater2010,
  title = {Bayesian Models of Cognition},
  author = {Chater, Nick and Oaksford, Mike and Hahn, Ulrike and Heit, Evan},
  year = {2010},
  journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
  volume = {1},
  number = {6},
  pages = {811--823},
  issn = {19395078},
  doi = {10.1002/wcs.79},
  abstract = {Griffiths, Thomas L., Charles Kemp, and Joshua B. Tenenbaum. "Bayesian models of cognition." Cambridge handbook of computational cognitive modeling (2008): 59-100.},
  isbn = {9780521674102},
  pmid = {14907735},
  file = {/home/marnix/Zotero/storage/J4HZX4Q6/Chater et al. - 2010 - Bayesian models of cognition.pdf}
}

@book{Chen2009,
  title = {The {{Circuits}} and {{Filters Handbook}}},
  author = {Chen, Wai-Kai},
  year = {2009},
  edition = {3rd},
  publisher = {CRC Press, Inc.},
  address = {Boca Raton, FL, USA},
  isbn = {1-4200-5527-5 978-1-4200-5527-6},
  file = {/home/marnix/Zotero/storage/L3A8SGXK/Chen2009 The circuits and filters handbook.pdf}
}

@article{Chen2013,
  title = {Acoustics of {{Human Voice Production}}},
  author = {Chen, C Julian},
  year = {2013},
  month = jan,
  pages = {5},
  langid = {english},
  file = {/home/marnix/Zotero/storage/7NTZ4UXT/Chen - 2013 - Acoustics of Human Voice Production.pdf}
}

@book{Chen2016,
  title = {Elements of {{Human Voice}}},
  author = {Chen, C Julian},
  year = {2016},
  eprint = {https://www.worldscientific.com/doi/pdf/10.1142/9891},
  publisher = {WORLD SCIENTIFIC},
  doi = {10.1142/9891},
  file = {/home/marnix/Zotero/storage/5Z75N6MM/Chen2016 Elements of Human Voice (complete).pdf}
}

@article{Chen2019,
  title = {Pitch-{{Synchronous Analysis}} of {{Human Voice}}},
  author = {Chen, C Julian and Miller, Donald A},
  year = {2019},
  month = feb,
  journal = {Journal of Voice},
  volume = {0},
  number = {0},
  issn = {0892-1997, 1557-8658},
  doi = {10.1016/j.jvoice.2019.01.009},
  urldate = {2019-02-13},
  abstract = {{$<$}h2{$>$}Summary{$<$}/h2{$><$}h3{$>$}Objective{$<$}/h3{$><$}p{$>$}Based on simultaneous voice and electroglottograph (EGG) signals, to gain a better understanding of human voice production process, to make pitch-synchronous segmentation of voice signals, and to make visual representations of pitch marks and timbre spectra with high resolution.{$<$}/p{$><$}h3{$>$}Methods/Design{$<$}/h3{$><$}p{$>$}The traditional spectrogram segments the voice signals with a process window of fixed size and fixed shift, then performs fast Fourier transformation after multiplied with a window function, typically a Hamming window. Then display power spectrum in both frequency and time. Pitch information and timbre information are mixed. The new design segments the signals into pitch periods, either using the derivatives of the EGG signals or based on the voice signals, then performs Fourier analysis to the segment of signals in each pitch period without using a window function. The pitch information and the timbre information are cleanly separated. The graphical representations of both pitch marks and timbre spectra exhibit high resolution and high accuracy.{$<$}/p{$><$}h3{$>$}Results{$<$}/h3{$><$}p{$>$}Detailed analysis of simultaneously acquired voice and EGG signals provides a more precise understanding of human-voice production process. The transient theory of voice production, proposed by Leonhard Euler in early 18th century, is substantiated with modern data. Based on the transient theory of voice production, a pitch-synchronous spectrogram software is developed, which makes a visual representation of pitch marks and timbre spectra. In addition, the timbre spectrum and the power evolution pattern in each pitch period can be displayed individually.{$<$}/p{$><$}h3{$>$}Conclusions{$<$}/h3{$><$}p{$>$}Simultaneously acquired voice and EGG signals indicates that each glottal closing triggers a decaying elementary wave in the vocal tract. A superposition of those elementary waves constitutes voice. Based on that concept and using EGG data, a pitch-synchronous voice signal processing method is developed. The voice signal is first segmented into pitch periods, then the two ends are equalized. Fourier analysis is applied to obtain the timbre spectrum of each pitch period. High resolution display of timbre spectrum is generated. The power evolution pattern in each pitch period is also displayed.{$<$}/p{$>$}},
  langid = {english},
  pmid = {30744954},
  file = {/home/marnix/Zotero/storage/Q9IP2EQQ/Chen and Miller - 2019 - Pitch-Synchronous Analysis of Human Voice.pdf;/home/marnix/Zotero/storage/NU7IJGQ7/fulltext.html}
}

@article{Chen2020,
  title = {Multivariate {{Gaussian}} and {{Student}}\$-T\$ {{Process Regression}} for {{Multi-output Prediction}}},
  author = {Chen, Zexun and Wang, Bo and Gorban, Alexander N.},
  year = {2020},
  month = apr,
  journal = {Neural Computing and Applications},
  volume = {32},
  number = {8},
  eprint = {1703.04455},
  primaryclass = {stat},
  pages = {3005--3028},
  issn = {0941-0643, 1433-3058},
  doi = {10.1007/s00521-019-04687-8},
  urldate = {2022-11-05},
  abstract = {Gaussian process model for vector-valued function has been shown to be useful for multi-output prediction. The existing method for this model is to re-formulate the matrix-variate Gaussian distribution as a multivariate normal distribution. Although it is effective in many cases, re-formulation is not always workable and is difficult to apply to other distributions because not all matrix-variate distributions can be transformed to respective multivariate distributions, such as the case for matrix-variate Student\$-t\$ distribution. In this paper, we propose a unified framework which is used not only to introduce a novel multivariate Student\$-t\$ process regression model (MV-TPR) for multi-output prediction, but also to reformulate the multivariate Gaussian process regression (MV-GPR) that overcomes some limitations of the existing methods. Both MV-GPR and MV-TPR have closed-form expressions for the marginal likelihoods and predictive distributions under this unified framework and thus can adopt the same optimization approaches as used in the conventional GPR. The usefulness of the proposed methods is illustrated through several simulated and real data examples. In particular, we verify empirically that MV-TPR has superiority for the datasets considered, including air quality prediction and bike rent prediction. At last, the proposed methods are shown to produce profitable investment strategies in the stock markets.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/MQ3KVRZ6/Chen et al. - 2020 - Multivariate Gaussian and Student$-t$ Process Regr.pdf;/home/marnix/Zotero/storage/8XTCJC4V/1703.html}
}

@misc{Chen2022,
  title = {Bayesian Posterior Repartitioning for Nested Sampling},
  author = {Chen, Xi and Feroz, Farhan and Hobson, Michael},
  year = {2022},
  month = jul,
  number = {arXiv:1908.04655},
  eprint = {1908.04655},
  primaryclass = {astro-ph, stat},
  publisher = {arXiv},
  urldate = {2022-07-20},
  abstract = {Priors in Bayesian analyses often encode informative domain knowledge that can be useful in making the inference process more efficient. Occasionally, however, priors may be unrepresentative of the parameter values for a given dataset, which can result in inefficient parameter space exploration, or even incorrect inferences, particularly for nested sampling (NS) algorithms. Simply broadening the prior in such cases may be inappropriate or impossible in some applications. Hence our previous solution to this problem, known as posterior repartitioning (PR), redefines the prior and likelihood while keeping their product fixed, so that the posterior inferences and evidence estimates remain unchanged, but the efficiency of the NS process is significantly increased. In its most practical form, PR raises the prior to some power beta, which is introduced as an auxiliary variable that must be determined on a case-by-case basis, usually by lowering beta from unity according to some pre-defined `annealing schedule' until the resulting inferences converge to a consistent solution. Here we present a very simple yet powerful alternative Bayesian approach, in which beta is instead treated as a hyperparameter that is inferred from the data alongside the original parameters of the problem, and then marginalised over to obtain the final inference. We show through numerical examples that this Bayesian PR (BPR) method provides a very robust, self-adapting and computationally efficient `hands-off' solution to the problem of unrepresentative priors in Bayesian inference using NS. Moreover, unlike the original PR method, we show that even for representative priors BPR has a negligible computational overhead relative to standard nesting sampling, which suggests that it should be used as the default in all NS analyses.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Neural and Evolutionary Computing,Statistics - Computation},
  file = {/home/marnix/Zotero/storage/VIC82YV5/Chen et al. - 2022 - Bayesian posterior repartitioning for nested sampl.pdf;/home/marnix/Zotero/storage/LBDEAC92/1908.html}
}

@misc{Chen2023,
  title = {X-{{LLM}}: {{Bootstrapping Advanced Large Language Models}} by {{Treating Multi-Modalities}} as {{Foreign Languages}}},
  shorttitle = {X-{{LLM}}},
  author = {Chen, Feilong and Han, Minglun and Zhao, Haozhi and Zhang, Qingyang and Shi, Jing and Xu, Shuang and Xu, Bo},
  year = {2023},
  month = may,
  number = {arXiv:2305.04160},
  eprint = {2305.04160},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.04160},
  urldate = {2023-08-20},
  abstract = {Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X'' denotes multi-modalities such as image, speech, and videos, and ``L'' denotes languages. X-LLM's training consists of three stages: (1) Converting Multimodal Information: The first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimodal information into languages. (2) Aligning X2L representations with the LLM: single-modal encoders are aligned with the LLM through X2L interfaces independently. (3) Integrating multiple modalities: all single-modal encoders are aligned with the LLM through X2L interfaces to integrate multimodal capabilities into the LLM. Our experiments show that X-LLM demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 84.5{\textbackslash}\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. And we also conduct quantitative tests on using LLM for ASR and multimodal ASR, hoping to promote the era of LLM-based speech recognition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/marnix/Zotero/storage/6RCYZ9UC/Chen et al. - 2023 - X-LLM Bootstrapping Advanced Large Language Model.pdf;/home/marnix/Zotero/storage/V6ANE7LP/2305.html}
}

@misc{Cheng2018,
  title = {Variational {{Inference}} for {{Gaussian Process Models}} with {{Linear Complexity}}},
  author = {Cheng, Ching-An and Boots, Byron},
  year = {2018},
  month = jan,
  number = {arXiv:1711.10127},
  eprint = {1711.10127},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-02-29},
  abstract = {Large-scale Gaussian process inference has long faced practical challenges due to time and space complexity that is superlinear in dataset size. While sparse variational Gaussian process models are capable of learning from large-scale data, standard strategies for sparsifying the model can prevent the approximation of complex functions. In this work, we propose a novel variational Gaussian process model that decouples the representation of mean and covariance functions in reproducing kernel Hilbert space. We show that this new parametrization generalizes previous models. Furthermore, it yields a variational inference problem that can be solved by stochastic gradient ascent with time and space complexity that is only linear in the number of mean function parameters, regardless of the choice of kernels, likelihoods, and inducing points. This strategy makes the adoption of large-scale expressive Gaussian process models possible. We run several experiments on regression tasks and show that this decoupled approach greatly outperforms previous sparse variational Gaussian process inference procedures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/6A37UBQC/Cheng and Boots - 2018 - Variational Inference for Gaussian Process Models .pdf;/home/marnix/Zotero/storage/9GPBZEM6/1711.html}
}

@article{Chien2017,
  title = {Evaluation of {{Glottal Inverse Filtering Algorithms Using}} a {{Physiologically Based Articulatory Speech Synthesizer}}},
  author = {Chien, Yu-Ren and Mehta, Daryush D. and Gu{\dh}nason, J{\'o}n and Za{\~n}artu, Mat{\'i}as and Quatieri, Thomas F.},
  year = {2017},
  month = aug,
  journal = {IEEE/ACM transactions on audio, speech, and language processing},
  volume = {25},
  number = {8},
  pages = {1718--1730},
  issn = {2329-9290},
  doi = {10.1109/taslp.2017.2714839},
  urldate = {2023-03-27},
  abstract = {Glottal inverse filtering aims to estimate the glottal airflow signal from a speech signal for applications such as speaker recognition and clinical voice assessment. Nonetheless, evaluation of inverse filtering algorithms has been challenging due to the practical difficulties of directly measuring glottal airflow. Apart from this, it is acknowledged that the performance of many methods degrade in voice conditions that are of great interest, such as breathiness, high pitch, soft voice, and running speech. This paper presents a comprehensive, objective, and comparative evaluation of state-of-the-art inverse filtering algorithms that takes advantage of speech and glottal airflow signals generated by a physiological speech synthesizer. The synthesizer provides a physics-based simulation of the voice production process and thus an adequate test bed for revealing the temporal and spectral performance characteristics of each algorithm. Included in the synthetic data are continuous speech utterances and sustained vowels, which are produced with multiple voice qualities (pressed, slightly pressed, modal, slightly breathy, and breathy), fundamental frequencies, and subglottal pressures to simulate the natural variations in real speech. In evaluating the accuracy of a glottal flow estimate, multiple error measures are used, including an error in the estimated signal that measures overall waveform deviation, as well as an error in each of several clinically relevant features extracted from the glottal flow estimate. Waveform errors calculated from glottal flow estimation experiments exhibited mean values around 30\% for sustained vowels, and around 40\% for continuous speech, of the amplitude of true glottal flow derivative. Closed-phase approaches showed remarkable stability across different voice qualities and subglottal pressures. The algorithms of choice, as suggested by significance tests, are closed-phase covariance analysis for the analysis of sustained vowels, and sparse linear prediction for the analysis of continuous speech. Results of data subset analysis suggest that analysis of close rounded vowels is an additional challenge in glottal flow estimation.},
  pmcid = {PMC8279087},
  pmid = {34268444},
  file = {/home/marnix/Zotero/storage/MMP6TMCK/Chien et al. - 2017 - Evaluation of Glottal Inverse Filtering Algorithms.pdf}
}

@inproceedings{Cho2009,
  title = {Kernel {{Methods}} for {{Deep Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Cho, Youngmin and Saul, Lawrence},
  year = {2009},
  volume = {22},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-04-24},
  abstract = {We introduce a new family of positive-definite kernel functions that mimic the computation in large, multilayer neural nets.  These kernel functions can be used in shallow architectures, such as support vector machines (SVMs), or in deep kernel-based architectures that we call multilayer kernel machines (MKMs).  We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of deep architectures.  On several problems, we obtain better results than previous, leading benchmarks from both SVMs with Gaussian kernels as well as deep belief nets.},
  file = {/home/marnix/Zotero/storage/KHUEC2H6/Cho and Saul - 2009 - Kernel Methods for Deep Learning.pdf}
}

@article{Choi1986,
  title = {On the Relation between the Maximum Entropy Probability Density Function and the Autoregressive Model},
  author = {Choi, B.},
  year = {1986},
  month = dec,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {34},
  number = {6},
  pages = {1659--1661},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1986.1164968},
  abstract = {The problem of maximizing the entropy of an n-variate random vector subject to constraints on the first p + 1 autocovariance terms is examined. It is shown that the maximum is achieved by the Gaussian autoregressive process of order p satisfying the autocovariance constraints. This solution provides Burg's theorem about the maximum entropy spectral density as a special case.},
  keywords = {Digital signal processing,Discrete Fourier transforms,Entropy,Narrowband,Probability density function,Roundoff errors,Signal processing algorithms,Speech analysis,Speech processing,Testing},
  file = {/home/marnix/Zotero/storage/TBUGRMQJ/Choi - 1986 - On the relation between the maximum entropy probab.pdf;/home/marnix/Zotero/storage/SZ9DYFVW/1164968.html}
}

@book{Christensen2005,
  title = {Complexity and Criticality},
  author = {Christensen, Kim and Moloney, Nicholas R},
  year = {2005},
  volume = {1},
  publisher = {Imperial College Press}
}

@article{Chung2005,
  title = {Laplacians and the {{Cheeger Inequality}} for {{Directed Graphs}}},
  author = {Chung, Fan},
  year = {2005},
  month = apr,
  journal = {Annals of Combinatorics},
  volume = {9},
  number = {1},
  pages = {1--19},
  doi = {10.1007/s00026-005-0237-z},
  abstract = {We consider Laplacians for directed graphs and examine their eigenvalues. We introduce a notion of a circulation in a directed graph and its connection with the Rayleigh quotient. We then define a Cheeger constant and establish the Cheeger inequality for directed graphs. These relations can be used to deal with various problems that often arise in the study of non-reversible Markov chains including bounding the rate of convergence and deriving comparison theorems.},
  keywords = {directed,graph,math},
  file = {/home/marnix/Zotero/storage/JHKZ5UGX/Chung - 2005 - Laplacians and the Cheeger Inequality for Directed Graphs.pdf}
}

@article{Cimini2019,
  title = {The Statistical Physics of Real-World Networks},
  author = {Cimini, Giulio and Squartini, Tiziano and Saracco, Fabio and Garlaschelli, Diego and Gabrielli, Andrea and Caldarelli, Guido},
  year = {2019},
  month = jan,
  journal = {Nature Reviews Physics},
  volume = {1},
  number = {1},
  pages = {58--71},
  publisher = {Nature Publishing Group},
  issn = {2522-5820},
  doi = {10.1038/s42254-018-0002-6},
  urldate = {2022-05-25},
  abstract = {In the past 15 years, statistical physics has been successful as a framework for modelling complex networks. On the theoretical side, this approach has unveiled a variety of physical phenomena, such as the emergence of mixed distributions and ensemble non-equivalence, that are observed in heterogeneous networks but not in homogeneous systems. At the same time, thanks to the deep connection between the principle of maximum entropy and information theory, statistical physics has led to the definition of null models for networks that reproduce features of real-world systems but that are otherwise as random as possible. We review here the statistical physics approach and the null models for complex networks, focusing in particular on analytical frameworks that reproduce local network features. We show how these models have been used to detect statistically significant structural patterns in real-world networks and to reconstruct the network structure in cases of incomplete information. We further survey the statistical physics models that reproduce more complex, semilocal network features using Markov chain Monte Carlo sampling, as well as models of generalized network structures, such as multiplex networks, interacting networks and simplicial complexes.},
  copyright = {2019 Springer Nature Limited},
  langid = {english},
  keywords = {Complex networks,Statistical physics},
  file = {/home/marnix/Zotero/storage/6X28N46X/Cimini et al. - 2019 - The statistical physics of real-world networks.pdf;/home/marnix/Zotero/storage/E8QSJZYZ/s42254-018-0002-6.html}
}

@article{Cohen1995,
  title = {Vocal Tract Normalization in Speech Recognition: {{Compensating}} for Systematic Speaker Variability},
  shorttitle = {Vocal Tract Normalization in Speech Recognition},
  author = {Cohen, Jordan and Kamm, Terri and Andreou, Andreas G.},
  year = {1995},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {97},
  number = {5\_Supplement},
  pages = {3246--3247},
  issn = {0001-4966},
  doi = {10.1121/1.411700},
  urldate = {2023-07-03},
  abstract = {The performance of speech recognition systems is often improved by accounting explicitly for sources of variability in the data. In the SWITCHBOARD corpus, studied during the 1994 CAIP workshop [Frontiers in Speech Processing Workshop II, CAIP (August 1994)], an attempt was made to compensate for the systematic variability due to different vocal tract lengths of various speakers. The method found a maximum probability parameter for each speaker which mapped an acoustic model to the mean of the models taken from a homogeneous speaker population. The underlying acoustic model was that of a straight tube, and the parameter estimation was accomplished by warping the spectrum of each speaker linearly over a 20\% range (actually accomplished by digitally resampling the data), and finding the maximum a\hphantom{,}posteriori probability of the data given the warp. The technique produces statistically significant improvements in accuracy on a speech transcription task using each of four different speech recognition systems. The best parametrizations were later found to correlate well with vocal tract estimates computed manually from spectrograms.},
  file = {/home/marnix/Zotero/storage/QQ9XGUUP/Cohen et al. - 1995 - Vocal tract normalization in speech recognition C.pdf;/home/marnix/Zotero/storage/PWC5SIP5/Vocal-tract-normalization-in-speech-recognition.html}
}

@article{Conrad2004,
  title = {Probability Distributions and Maximum Entropy},
  author = {Conrad, Keith},
  year = {2004},
  journal = {Entropy},
  pages = {1--27},
  issn = {10994300},
  doi = {10.3390/e13101746},
  file = {/home/marnix/Zotero/storage/Y7IKHU9Z/Conrad PROBABILITY DISTRIBUTIONS AND MAXIMUM ENTROPY.pdf}
}

@article{Coupe2019,
  title = {Different Languages, Similar Encoding Efficiency: {{Comparable}} Information Rates across the Human Communicative Niche},
  shorttitle = {Different Languages, Similar Encoding Efficiency},
  author = {Coup{\'e}, Christophe and Oh, Yoon Mi and Dediu, Dan and Pellegrino, Fran{\c c}ois},
  year = {2019},
  month = sep,
  journal = {Science Advances},
  volume = {5},
  number = {9},
  pages = {eaaw2594},
  issn = {2375-2548},
  doi = {10.1126/sciadv.aaw2594},
  urldate = {2020-02-27},
  abstract = {Language is universal, but it has few indisputably universal characteristics, with cross-linguistic variation being the norm. For example, languages differ greatly in the number of syllables they allow, resulting in large variation in the Shannon information per syllable. Nevertheless, all natural languages allow their speakers to efficiently encode and transmit information. We show here, using quantitative methods on a large cross-linguistic corpus of 17 languages, that the coupling between language-level (information per syllable) and speaker-level (speech rate) properties results in languages encoding similar information rates ({\textasciitilde}39 bits/s) despite wide differences in each property individually: Languages are more similar in information rates than in Shannon information or speech rate. These findings highlight the intimate feedback loops between languages' structural properties and their speakers' neurocognition and biology under communicative pressures. Thus, language is the product of a multiscale communicative niche construction process at the intersection of biology, environment, and culture. Human languages encode similar average information rates ({\textasciitilde}39 bits/s) despite their remarkable differences. Human languages encode similar average information rates ({\textasciitilde}39 bits/s) despite their remarkable differences.},
  copyright = {Copyright {\copyright} 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution NonCommercial License 4.0 (CC BY-NC).. This is an open-access article distributed under the terms of the Creative Commons Attribution-NonCommercial license, which permits use, distribution, and reproduction in any medium, so long as the resultant use is not for commercial advantage and provided the original work is properly cited.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/PIH4K3YP/Coupé et al. - 2019 - Different languages, similar encoding efficiency .pdf;/home/marnix/Zotero/storage/8DMM3CHW/tab-pdf.html}
}

@article{Cox1946,
  title = {Probability, {{Frequency}} and {{Reasonable Expectation}}},
  author = {Cox, Richard T},
  year = {1946},
  journal = {Physics},
  file = {/home/marnix/Zotero/storage/B8KNJ3GA/Cox1946 Probability, Frequency and Reasonable Expectation.pdf}
}

@incollection{Cox2024,
  title = {Conversational {{Interactions}} with {{NPCs}} in {{LLM-Driven Gaming}}: {{Guidelines}} from a {{Content Analysis}} of {{Player Feedback}}},
  shorttitle = {Conversational {{Interactions}} with {{NPCs}} in {{LLM-Driven Gaming}}},
  booktitle = {Chatbot {{Research}} and {{Design}}},
  author = {Cox, Samuel Rhys and Ooi, Wei Tsang},
  editor = {F{\o}lstad, Asbj{\o}rn and Araujo, Theo and Papadopoulos, Symeon and Law, Effie L.-C. and Luger, Ewa and Goodwin, Morten and Hobert, Sebastian and Brandtzaeg, Petter Bae},
  year = {2024},
  volume = {14524},
  pages = {167--184},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-54975-5_10},
  urldate = {2025-09-15},
  abstract = {The growing capability and availability of large language models (LLMs) have led to their adoption in a number of domains. One application domain that could prove fruitful is to video games, where LLMs could be used to provide conversational responses from non-playable characters (NPCs) that are more dynamic and diverse. Additionally, LLMs could allow players the autonomy to converse in open-ended conversations potentially improving player immersion and agency. However, due to their recent commercial popularity, the consequences (both negative and positive) of using LLMs in video games from a player's perspective is currently unclear. On from this, we analyse player feedback to the use of LLM-driven NPC responses in a commercially available video game. We discuss findings and implications, and generate guidelines for designers incorporating LLMs into NPC dialogue.},
  isbn = {978-3-031-54974-8 978-3-031-54975-5},
  langid = {english},
  file = {/home/marnix/Zotero/storage/T4JECWGC/Cox and Ooi - 2024 - Conversational Interactions with NPCs in LLM-Driven Gaming Guidelines from a Content Analysis of Pl.pdf}
}

@article{Craig2014,
  title = {Interbank Tiering and Money Center Banks},
  author = {Craig, Ben and Von Peter, Goetz},
  year = {2014},
  journal = {Journal of Financial Intermediation},
  volume = {23},
  number = {3},
  pages = {322--347},
  publisher = {Elsevier},
  file = {/home/marnix/Zotero/storage/LUEK7A58/Craig, von Peter - 2014 - Interbank tiering and money center banks.pdf}
}

@article{Cremers2014,
  title = {Default {{Cascades}} in {{Interbank Networks}}},
  author = {Cremers, V{\'e}ronique and Ryckebusch, Jan and Schoors, Koen},
  year = {2014}
}

@inproceedings{Cuevas2021,
  title = {Bayesian Autoregressive Spectral Estimation},
  booktitle = {2021 {{IEEE Latin American Conference}} on {{Computational Intelligence}} ({{LA-CCI}})},
  author = {Cuevas, Alejandro and L{\'o}pez, Sebasti{\'a}n and Mandic, Danilo and Tobar, Felipe},
  year = {2021},
  month = nov,
  pages = {1--7},
  doi = {10.1109/LA-CCI48322.2021.9769834},
  urldate = {2023-12-18},
  abstract = {Autoregressive (AR) time series models are widely used in parametric spectral estimation (SE), where the power spectral density (PSD) of the time series is approximated by that of the best-fit AR model, which is available in closed form. Since AR parameters are usually found via maximum-likelihood, least squares or the method of moments, AR-based SE fails to account for the uncertainty of the approximate PSD, and thus only yields point estimates. We propose to handle the uncertainty related to the AR approximation by finding the full posterior distribution of the AR parameters to then propagate this uncertainty to the PSD approximation by integrating out the AR parameters; we implement this concept by assuming two different priors over the model noise. Through practical experiments, we show that the proposed Bayesian autoregressive spectral estimation (BASE) provides point estimates that follow closely those of standard autoregressive spectral estimation (ASE), while also providing error bars. BASE is validated against ASE and the Periodogram on both synthetic and real-world signals.},
  file = {/home/marnix/Zotero/storage/XINJDYHQ/Cuevas et al. - 2021 - Bayesian autoregressive spectral estimation.pdf;/home/marnix/Zotero/storage/4TQCJ8FL/9769834.html}
}

@article{DallAsta2006,
  title = {Non-Equilibrium Dynamics of Language Games on Complex Networks},
  author = {Dall'Asta, Luca and Baronchelli, Andrea and Barrat, Alain and Loreto, Vittorio},
  year = {2006},
  journal = {Physical Review E : Statistical, Nonlinear, and Soft Matter Physics},
  volume = {74},
  pages = {036105},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.74.036105},
  urldate = {2022-08-22},
  abstract = {The Naming Game is a model of non-equilibriun dynamics for the self-organized emergence of a linguistic convention or a communication system in a population of agents with pairwise local interactions. We present an extensive study of its dynamics on complex networks, that can be considered as the most natural topological embedding for agents involved in language games and opinion dynamics. Except for some community structured networks on which metastable phases can be observed, agents playing the Naming Game always manage to reach a global consensus. This convergence is obtained after a time generically scaling with the population's size \$N\$ as \$t\_\{conv\} {\textbackslash}sim N{\textasciicircum}\{1.4 {\textbackslash}pm 0.1\}\$, i.e. much faster than for agents embedded on regular lattices. Moreover, the memory capacity required by the system scales only linearly with its size. Particular attention is given to heterogenous networks, in which the dynamical activity pattern of a node depends on its degree. High degree nodes have a fundamental role, but require larger memory capacity. They govern the dynamics acting as spreaders of (linguistic) conventions. The effects of other properties, such as the average degree and the clustering, are also discussed.},
  file = {/home/marnix/Zotero/storage/B5FIGTYC/Dall'Asta et al. - 2006 - Non-equilibrium dynamics of language games on comp.pdf}
}

@misc{Damianou2014,
  title = {Variational {{Inference}} for {{Uncertainty}} on the {{Inputs}} of {{Gaussian Process Models}}},
  author = {Damianou, Andreas C. and Titsias, Michalis K. and Lawrence, Neil D.},
  year = {2014},
  month = sep,
  number = {arXiv:1409.2287},
  eprint = {1409.2287},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-02-29},
  abstract = {The Gaussian process latent variable model (GP-LVM) provides a flexible approach for non-linear dimensionality reduction that has been widely applied. However, the current approach for training GP-LVMs is based on maximum likelihood, where the latent projection variables are maximized over rather than integrated out. In this paper we present a Bayesian method for training GP-LVMs by introducing a non-standard variational inference framework that allows to approximately integrate out the latent variables and subsequently train a GP-LVM by maximizing an analytic lower bound on the exact marginal likelihood. We apply this method for learning a GP-LVM from iid observations and for learning non-linear dynamical systems where the observations are temporally correlated. We show that a benefit of the variational Bayesian procedure is its robustness to overfitting and its ability to automatically select the dimensionality of the nonlinear latent space. The resulting framework is generic, flexible and easy to extend for other purposes, such as Gaussian process regression with uncertain inputs and semi-supervised Gaussian processes. We demonstrate our method on synthetic data and standard machine learning benchmarks, as well as challenging real world datasets, including high resolution video data.},
  archiveprefix = {arXiv},
  keywords = {{60G15 (Primary), 58E30},Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,G.1.2,G.3,I.2.6,I.5.4,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/AVJ4UEGL/Damianou et al. - 2014 - Variational Inference for Uncertainty on the Input.pdf;/home/marnix/Zotero/storage/HFQZMS6W/1409.html}
}

@article{Dandekar2012,
  title = {Biased {{Assimilation}}, {{Homophily}} and the {{Dynamics}} of {{Polarization}}},
  author = {Dandekar, Pranav and Goel, Ashish and Lee, David},
  year = {2012},
  eprint = {1209.5998},
  issn = {0027-8424},
  doi = {10.1073/pnas.1217220110},
  abstract = {Are we as a society getting more polarized, and if so, why? We try to answer this question through a model of opinion formation. Empirical studies have shown that homophily results in polarization. However, we show that DeGroot's well-known model of opinion formation based on repeated averaging can never be polarizing, even if individuals are arbitrarily homophilous. We generalize DeGroot's model to account for a phenomenon well-known in social psychology as biased assimilation: when presented with mixed or inconclusive evidence on a complex issue, individuals draw undue support for their initial position thereby arriving at a more extreme opinion. We show that in a simple model of homophilous networks, our biased opinion formation process results in either polarization, persistent disagreement or consensus depending on how biased individuals are. In other words, homophily alone, without biased assimilation, is not sufficient to polarize society. Quite interestingly, biased assimilation also provides insight into the following related question: do internet based recommender algorithms that show us personalized content contribute to polarization? We make a connection between biased assimilation and the polarizing effects of some random-walk based recommender algorithms that are similar in spirit to some commonly used recommender algorithms.},
  archiveprefix = {arXiv},
  arxivid = {1209.5998},
  isbn = {1091-6490 (Electronic)\r0027-8424 (Linking)},
  pmid = {23536293},
  file = {/home/marnix/Zotero/storage/2H9G9ICH/Dandekar, Goel, Lee - 2012 - Biased Assimilation, Homophily and the Dynamics of Polarization.pdf}
}

@book{Datta2018,
  title = {Time {{Domain Representation}} of {{Speech Sounds}}: {{A Case Study}} in {{Bangla}}},
  shorttitle = {Time {{Domain Representation}} of {{Speech Sounds}}},
  author = {Datta, Asoke Kumar},
  year = {2018},
  publisher = {Springer Singapore},
  urldate = {2019-03-15},
  abstract = {The book presents the history of time-domain representation and the extent of its development along with that of spectral domain representation in the cognitive and technology domains. It discusses all the cognitive experiments related to this development, along with details of technological developments related to both automatic speech recognition (ASR) and text to speech synthesis (TTS), and introduces a viable time-domain representation for both objective and subjective analysis, as an alternative to the well-known spectral representation.The book also includes a new cohort study on the use of lexical knowledge in ASR. India has numerous official dialects, and spoken-language technology development is a burgeoning area. In fact TTS and ASR taken together constitute the most important technology for empowering people. As such, the book describes time domain representation in such a way that it can be easily and seamlessly incorporated into ASR and TTS research and development. In short, it is a valuable guidebook for the development of ASR and TTS in all the Indian Standard Dialects using signal domain parameters.},
  isbn = {978-981-13-2302-7},
  langid = {english},
  file = {/home/marnix/Zotero/storage/3AFQYJZL/9789811323027.html}
}

@misc{De2024,
  title = {Griffin: {{Mixing Gated Linear Recurrences}} with {{Local Attention}} for {{Efficient Language Models}}},
  shorttitle = {Griffin},
  author = {De, Soham and Smith, Samuel L. and Fernando, Anushan and Botev, Aleksandar and {Cristian-Muraru}, George and Gu, Albert and Haroun, Ruba and Berrada, Leonard and Chen, Yutian and Srinivasan, Srivatsan and Desjardins, Guillaume and Doucet, Arnaud and Budden, David and Teh, Yee Whye and Pascanu, Razvan and De Freitas, Nando and Gulcehre, Caglar},
  year = {2024},
  month = feb,
  number = {arXiv:2402.19427},
  eprint = {2402.19427},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.19427},
  urldate = {2024-07-05},
  abstract = {Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/marnix/Zotero/storage/7DQ3EDZY/De et al. - 2024 - Griffin Mixing Gated Linear Recurrences with Loca.pdf;/home/marnix/Zotero/storage/PWV5ZRME/2402.html}
}

@article{DeBacco2017,
  title = {Community Detection, Link Prediction, and Layer Interdependence in Multilayer Networks},
  author = {De Bacco, Caterina and Power, Eleanor A. and Larremore, Daniel B. and Moore, Cristopher},
  year = {2017},
  journal = {Physical Review E},
  volume = {95},
  number = {4},
  eprint = {1701.01369},
  pages = {1--11},
  issn = {24700053},
  doi = {10.1103/PhysRevE.95.042317},
  abstract = {Complex systems are often characterized by distinct types of interactions between the same entities. These can be described as a multilayer network where each layer represents one type of interaction. These layers may be interdependent in complicated ways, revealing different kinds of structure in the network. In this work we present a generative model, and an efficient expectation-maximization algorithm, which allows us to perform inference tasks such as community detection and link prediction in this setting. Our model assumes overlapping communities that are common between the layers, while allowing these communities to affect each layer in a different way, including arbitrary mixtures of assortative, disassortative, or directed structure. It also gives us a mathematically principled way to define the interdependence between layers, by measuring how much information about one layer helps us predict links in another layer. In particular, this allows us to bundle layers together to compress redundant information, and identify small groups of layers which suffice to predict the remaining layers accurately. We illustrate these findings by analyzing synthetic data and two real multilayer networks, one representing social support relationships among villagers in South India and the other representing shared genetic substrings material between genes of the malaria parasite.},
  archiveprefix = {arXiv},
  arxivid = {1701.01369},
  file = {/home/marnix/Zotero/storage/AYPA7KR3/DeBacco2017 Community detection, link prediction, and layer interdependence in multilayer networks.pdf}
}

@article{deBeaudrap2013,
  title = {Interpreting the von {{Neumann}} Entropy of Graph {{Laplacians}}, and Coentropic Graphs},
  author = {{de Beaudrap}, Niel and Giovannetti, Vittorio and Severini, Simone and Wilson, Richard},
  year = {2013},
  month = apr,
  eprint = {1304.7946},
  abstract = {For any graph, we define a rank-1 operator on a bipartite tensor product space, with components associated to the set of vertices and edges respectively. We show that the partial traces of the operator are the Laplacian and the \{edge-Laplacian\}. This provides an interpretation of the von Neumann entropy of the (normalized){\textbackslash} Laplacian as the amount of quantum entanglement between two systems corresponding to vertices and edges. In this framework, cospectral graphs correspond exactly to local unitarily equivalent pure states. Finally, we introduce the notion of coentropic graphs, that is, graphs with equal von Neumann entropy. The smallest coentropic (but not cospectral) graphs that we are able to construct have 8 vertices. The number of equivalence classes of coentropic graphs with n vertices and m edges is a lower bound to the number of (pure) bipartite entanglement classes with subsystems of corresponding dimension.},
  archiveprefix = {arXiv},
  arxivid = {1304.7946},
  file = {/home/marnix/Zotero/storage/83DY8SX6/de Beaudrap et al. - 2013 - Interpreting the von Neumann entropy of graph Lapl.pdf}
}

@article{deBoer2008,
  title = {Acoustic Tubes with Maximal and Minimal Resonance Frequencies},
  author = {{de Boer}, Bart},
  year = {2008},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {123},
  number = {5},
  pages = {3732--3732},
  issn = {0001-4966},
  doi = {10.1121/1.2935231},
  urldate = {2021-01-07},
  langid = {english},
  file = {/home/marnix/Zotero/storage/CEX4XSXH/De Boer - 2008 - Acoustic tubes with maximal and minimal resonance .pdf}
}

@article{Decelle2011,
  title = {Asymptotic Analysis of the Stochastic Block Model for Modular Networks and Its Algorithmic Applications},
  author = {Decelle, Aurelien and Krzakala, Florent and Moore, Cristopher and Zdeborov{\'a}, Lenka},
  year = {2011},
  month = dec,
  journal = {Physical Review E},
  volume = {84},
  number = {6},
  pages = {066106},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.84.066106},
  urldate = {2020-12-23},
  abstract = {In this paper we extend our previous work on the stochastic block model, a commonly used generative model for social and biological networks, and the problem of inferring functional groups or communities from the topology of the network. We use the cavity method of statistical physics to obtain an asymptotically exact analysis of the phase diagram. We describe in detail properties of the detectability-undetectability phase transition and the easy-hard phase transition for the community detection problem. Our analysis translates naturally into a belief propagation algorithm for inferring the group memberships of the nodes in an optimal way, i.e., that maximizes the overlap with the underlying group memberships, and learning the underlying parameters of the block model. Finally, we apply the algorithm to two examples of real-world networks and discuss its performance.},
  file = {/home/marnix/Zotero/storage/R7VHBI4G/Decelle et al. - 2011 - Asymptotic analysis of the stochastic block model .pdf;/home/marnix/Zotero/storage/JQ6WC8FA/PhysRevE.84.html}
}

@article{DeDecker2016,
  title = {An Evaluation of Noise on {{LPC-based}} Vowel Formant Estimates: {{Implications}} for Sociolinguistic Data Collection},
  shorttitle = {An Evaluation of Noise on {{LPC-based}} Vowel Formant Estimates},
  author = {De Decker, Paul},
  year = {2016},
  month = jan,
  journal = {Linguistics Vanguard},
  volume = {2},
  number = {1},
  issn = {2199-174X},
  doi = {10.1515/lingvan-2015-0010},
  urldate = {2020-02-05},
  abstract = {Abstract             Current trends in sociophonetic data analysis indicate a shift to entirely automatic measurements of spectral properties using programs like Praat. While such practices are useful for the rapid collection of acoustic data from large corpora, they, by default do not permit human analysts to provide quality control or make hand corrected measurements when needed. Under ideal signal-to-noise conditions, such as in a sound-proof room, this may not be a problem. However, analysis of audio recordings made in acoustically-uncontrolled environments, like many standard sociolinguistic interviews, are arguably susceptible to spurious estimates using automated routines. This paper presents the results of a highly controlled noise-interference experiment designed to examine the effects of different types of noise at varying signal-to-noise levels on automated LPC-based vowel formant measurements made in Praat. Findings indicate that some noises are more detrimental than others, affect some formant frequencies more than others and that louder noises make it inappropriate to conduct an analysis of F1 and F2. Results are discussed and suggestions for better practices in recording sociolinguistic interviews for sociophonetic data collection are presented.},
  file = {/home/marnix/Zotero/storage/NZQ3I88E/De Decker - 2016 - An evaluation of noise on LPC-based vowel formant .pdf}
}

@article{DeDomenico2014,
  title = {Mathematical Formulation of Multilayer Networks},
  author = {De Domenico, Manlio and {Sol{\'e}-Ribalta}, Albert and Cozzo, Emanuele and Kivel{\"a}, Mikko and Moreno, Yamir and Porter, Mason A. and G{\'o}mez, Sergio and Arenas, Alex},
  year = {2014},
  journal = {Physical Review X},
  volume = {3},
  number = {4},
  eprint = {1307.4977v2},
  pages = {1--15},
  issn = {21603308},
  doi = {10.1103/PhysRevX.3.041022},
  abstract = {A network representation is useful for describing the structure of a large variety of complex systems. However, most real and engineered systems have multiple subsystems and layers of connectivity, and the data produced by such systems are very rich. Achieving a deep understanding of such systems necessitates generalizing ``traditional'' network theory, and the newfound deluge of data now makes it possible to test increasingly general frameworks for the study of networks. In particular, although adjacency matrices are useful to describe traditional single-layer networks, such a representation is insufficient for the analysis and description of multiplex and time-dependent networks. One must therefore develop a more general mathematical framework to cope with the challenges posed by multilayer complex systems. In this paper, we introduce a tensorial framework to study multilayer networks, and we discuss the generalization of several important network descriptors and dynamical processes---including degree centrality, clustering coefficients, eigenvector centrality, modularity, von Neumann entropy, and diffusion---for this framework. We examine the impact of different choices in constructing these generalizations, and we illustrate how to obtain known results for the special cases of single-layer and multiplex networks. Our tensorial approach will be helpful for tackling pressing problems in multilayer complex systems, such as inferring who is influencing whom (and by which media) in multichannel social networks and developing routing techniques for multimodal transportation systems.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1307.4977v2},
  isbn = {2160-3308},
  pmid = {10313288},
  keywords = {interdisciplinary physics},
  file = {/home/marnix/Zotero/storage/8LYCBMJ8/De Domenico et al. - 2014 - Mathematical formulation of multilayer networks.pdf}
}

@article{DeDomenico2014a,
  title = {Layer Aggregation and Reducibility of Multilayer Interconnected Networks},
  author = {De Domenico, M. and Nicosia, V. and Arenas, A. and Latora, V.},
  year = {2014},
  journal = {Nature Communications},
  volume = {6},
  eprint = {1405.0425},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/ncomms7864},
  abstract = {Many complex systems can be represented as networks composed by distinct layers, interacting and depending on each others. For example, in biology, a good description of the full protein-protein interactome requires, for some organisms, up to seven distinct network layers, with thousands of protein-protein interactions each. A fundamental open question is then how much information is really necessary to accurately represent the structure of a multilayer complex system, and if and when some of the layers can indeed be aggregated. Here we introduce a method, based on information theory, to reduce the number of layers in multilayer networks, while minimizing information loss. We validate our approach on a set of synthetic benchmarks, and prove its applicability to an extended data set of protein-genetic interactions, showing cases where a strong reduction is possible and cases where it is not. Using this method we can describe complex systems with an optimal trade--off between accuracy and complexity.},
  archiveprefix = {arXiv},
  arxivid = {1405.0425},
  isbn = {2041-1723 (Electronic)\r2041-1723 (Linking)},
  pmid = {25904309},
  file = {/home/marnix/Zotero/storage/CT8ZGEYQ/De Domenico et al. - 2014 - Layer aggregation and reducibility of multilayer interconnected networks.pdf}
}

@article{DeDomenico2015,
  title = {Identifying Modular Flows on Multilayer Networks Reveals Highly Overlapping Organization in Interconnected Systems},
  author = {De Domenico, Manlio and Lancichinetti, Andrea and Arenas, Alex and Rosvall, Martin},
  year = {2015},
  journal = {Physical Review X},
  volume = {5},
  number = {1},
  eprint = {1408.2925},
  pages = {11027},
  publisher = {APS},
  issn = {21603308},
  doi = {10.1103/PhysRevX.5.011027},
  abstract = {Unveiling the community structure of networks is a powerful methodology to comprehend interconnected systems across the social and natural sciences. To identify different types of functional modules in interaction data aggregated in a single network layer, researchers have developed many powerful methods. For example, flow-based methods have proven useful for identifying modular dynamics in weighted and directed networks that capture constraints on flow in the systems they represent. However, many networked systems consist of agents or components that exhibit multiple layers of interactions. Inevitably, representing this intricate network of networks as a single aggregated network leads to information loss and may obscure the actual organization. Here we propose a method based on compression of network flows that can identify modular flows in non-aggregated multilayer networks. Our numerical experiments on synthetic networks show that the method can accurately identify modules that cannot be identified in aggregated networks or by analyzing the layers separately. We capitalize on our findings and reveal the community structure of two multilayer collaboration networks: scientists affiliated to the Pierre Auger Observatory and scientists publishing works on networks on the arXiv. Compared to conventional aggregated methods, the multilayer method reveals smaller modules with more overlap that better capture the actual organization.},
  archiveprefix = {arXiv},
  arxivid = {1408.2925},
  isbn = {2160-3308},
  keywords = {Complex systems,Interdisciplinary physics},
  file = {/home/marnix/Zotero/storage/K3JDXQL2/De Domenico et al. - 2015 - Identifying modular flows on multilayer networks reveals highly overlapping organization in interconnected s.pdf}
}

@article{DeDomenico2015b,
  title = {Structural Reducibility of Multilayer Networks},
  author = {De Domenico, Manlio and Nicosia, Vincenzo and Arenas, Alexandre and Latora, Vito},
  year = {2015},
  journal = {Nature communications},
  volume = {6},
  publisher = {Nature Publishing Group}
}

@phdthesis{Degottex2010,
  title = {Glottal Source and Vocal-Tract Separation},
  author = {Degottex, Gilles},
  year = {2010},
  school = {Universit{\'e} Pierre et Marie Curie-Paris VI},
  file = {/home/marnix/Zotero/storage/9XXL7NH5/Degottex - 2010 - Glottal source and vocal-tract separation.pdf}
}

@inproceedings{Degottex2014,
  title = {{{COVAREP}} --- {{A}} Collaborative Voice Analysis Repository for Speech Technologies},
  booktitle = {2014 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Degottex, Gilles and Kane, John and Drugman, Thomas and Raitio, Tuomo and Scherer, Stefan},
  year = {2014},
  month = may,
  pages = {960--964},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2014.6853739},
  abstract = {Speech processing algorithms are often developed demonstrating improvements over the state-of-the-art, but sometimes at the cost of high complexity. This makes algorithm reimplementations based on literature difficult, and thus reliable comparisons between published results and current work are hard to achieve. This paper presents a new collaborative and freely available repository for speech processing algorithms called COVAREP, which aims at fast and easy access to new speech processing algorithms and thus facilitating research in the field. We envisage that COVAREP will allow more reproducible research by strengthening complex implementations through shared contributions and openly available code which can be discussed, commented on and corrected by the community. Presently COVAREP contains contributions from five distinct laboratories and we encourage contributions from across the speech processing research field. In this paper, we provide an overview of the current offerings of COVAREP and also include a demonstration of the algorithms through an emotion classification experiment.},
  keywords = {Adaptation models,Estimation,Feature extraction,glottal source,Harmonic analysis,Signal processing algorithms,sinusoidal modeling,spectral envelope,Speech,Speech processing,toolkit,voice quality},
  file = {/home/marnix/Zotero/storage/ACIS67P9/Degottex et al. - 2014 - COVAREP — A collaborative voice analysis repositor.pdf;/home/marnix/Zotero/storage/DMK9FB2Y/6853739.html}
}

@inproceedings{Deistler2002,
  title = {System {{Identification}} and {{Time Series Analysis}}: {{Past}}, {{Present}}, and {{Future}}},
  shorttitle = {System {{Identification}} and {{Time Series Analysis}}},
  booktitle = {Stochastic {{Theory}} and {{Control}}},
  author = {Deistler, Manfred},
  editor = {{Pasik-Duncan}, Bozenna},
  year = {2002},
  series = {Lecture {{Notes}} in {{Control}} and {{Information Sciences}}},
  pages = {97--109},
  publisher = {Springer Berlin Heidelberg},
  abstract = {The aim of this contribution is to describe main features in the development of system identification, in the sense of modelling from time series data. Given the restrictions in space, such an effort is necessarely fragmentary. Clearly, subjective judgements cannot be avoided. System identification has been developed in a number of different scientific communities, the most important of which are econometrics, statistics and system- and control theory.},
  isbn = {978-3-540-48022-8},
  langid = {english},
  keywords = {Macro Model,Maximum Likelihood Estimation,Ordinary Little Square Estimator,State Space System,Time Series Analysis},
  file = {/home/marnix/Zotero/storage/I9TND25Y/Deistler - 2002 - System Identification and Time Series Analysis Pa.pdf}
}

@article{Deller1983,
  title = {On the Time Domain Properties of the Two-Pole Model of the Glottal Waveform and Implications for {{LPC}}},
  author = {Deller, John R},
  year = {1983},
  journal = {Speech Communication},
  volume = {2},
  number = {1},
  pages = {57--63},
  publisher = {Elsevier},
  file = {/home/marnix/Zotero/storage/FXYHYSG9/Deller - 1983 - On the time domain properties of the two-pole mode.pdf}
}

@article{Delpini2013,
  title = {Evolution of {{Controllability}} in {{Interbank Networks}}},
  author = {Delpini, Danilo and Battiston, Stefano and Riccaboni, Massimo and Gabbi, Giampaolo and Pammolli, Fabio and Caldarelli, Guido},
  year = {2013},
  journal = {Scientific Reports},
  volume = {3},
  number = {1},
  pages = {1626},
  issn = {2045-2322},
  doi = {10.1038/srep01626},
  abstract = {The Statistical Physics of Complex Networks has recently provided new theoretical tools for policy makers. Here we extend the notion of network controllability to detect the financial institutions, i.e. the drivers, that are most crucial to the functioning of an interbank market. The system we investigate is a paradigmatic case study for complex networks since it undergoes dramatic structural changes over time and links among nodes can be observed at several time scales. We find a scale-free decay of the fraction of drivers with increasing time resolution, implying that policies have to be adjusted to the time scales in order to be effective. Moreover, drivers are often not the most highly connected "hub" institutions, nor the largest lenders, contrary to the results of other studies. Our findings contribute quantitative indicators which can support regulators in developing more effective supervision and intervention policies.},
  isbn = {2045-2322 (Electronic)\n2045-2322 (Linking)},
  pmid = {23568033},
  file = {/home/marnix/Zotero/storage/S9QLK6JF/Delpini et al. - 2013 - Evolution of Controllability in Interbank Networks.pdf}
}

@article{DeMasi2006,
  title = {Fitness Model for the {{Italian}} Interbank Money Market},
  author = {De Masi, G. and Iori, G. and Caldarelli, G.},
  year = {2006},
  journal = {Physical Review E (Statistical, Nonlinear, and Soft Matter Physics)},
  volume = {74},
  number = {6},
  eprintclass = {physics},
  pages = {066112+},
  publisher = {APS},
  issn = {15393755},
  doi = {10.1103/physreve.74.066112},
  abstract = {We use the theory of complex networks in order to quantitatively characterize the formation of communities in a particular financial market. The system is composed by different banks exchanging on a daily basis loans and debts of liquidity. Through topological analysis and by means of a model of network growth we can determine the formation of different group of banks characterized by different business strategy. The model based on Pareto's law makes no use of growth or preferential attachment and it reproduces correctly all the various statistical properties of the system. We believe that this network modeling of the market could be an efficient way to evaluate the impact of different policies in the market of liquidity.},
  arxiv = {0610108},
  arxivid = {physics/0610108},
  pmid = {17280126},
  keywords = {book,econophysics,io,scale-free-networks},
  file = {/home/marnix/Zotero/storage/UEZ2SER5/De Masi, Iori, Caldarelli - 2006 - Fitness model for the Italian interbank money market.pdf}
}

@book{deMontaigne1910,
  title = {Essays of {{Montaigne}}},
  author = {{de Montaigne}, Michel},
  editor = {Hazlett, William Carew},
  translator = {Cotton, Charles},
  year = {1910},
  volume = {6},
  publisher = {Edwin C. Hill},
  address = {New York}
}

@inproceedings{Deng2006,
  title = {A Database of Vocal Tract Resonance Trajectories for Research in Speech Processing},
  booktitle = {2006 {{IEEE International Conference}} on {{Acoustics Speech}} and {{Signal Processing Proceedings}}},
  author = {Deng, Li and Cui, Xiaodong and Pruvenok, Robert and Huang, Jonathan and Momen, Safiyy and Chen, Yanyi and Alwan, Abeer},
  year = {2006},
  volume = {1},
  pages = {I-I},
  organization = {IEEE},
  file = {/home/marnix/Zotero/storage/SXT3VGY6/and et al. - 2006 - A Database of Vocal Tract Resonance Trajectories f.pdf}
}

@article{Deng2006a,
  title = {Tracking {{Vocal Tract Resonances Using}} a {{Quantized Nonlinear Function Embedded}} in a {{Temporal Constraint}}},
  author = {Deng, L. and Acero, A. and Bazzi, I.},
  year = {2006},
  month = mar,
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {14},
  number = {2},
  pages = {425--434},
  issn = {1558-7924},
  doi = {10.1109/TSA.2005.855841},
  abstract = {This paper presents a new technique for high-accuracy tracking of vocal-tract resonances (which coincide with formants for nonnasalized vowels) in natural speech. The technique is based on a discretized nonlinear prediction function, which is embedded in a temporal constraint on the quantized input values over adjacent time frames as the prior knowledge for their temporal behavior. The nonlinear prediction is constructed, based on its analytical form derived in detail in this paper, as a parameter-free, discrete mapping function that approximates the ``forward'' relationship from the resonance frequencies and bandwidths to the Linear Predictive Coding (LPC) cepstra of real speech. Discretization of the function permits the ``inversion'' of the function via a search operation. We further introduce the nonlinear-prediction residual, characterized by a multivariate Gaussian vector with trainable mean vectors and covariance matrices, to account for the errors due to the functional approximation. We develop and describe an expectation--maximization (EM)-based algorithm for training the parameters of the residual, and a dynamic programming-based algorithm for resonance tracking. Details of the algorithm implementation for computation speedup are provided. Experimental results are presented which demonstrate the effectiveness of our new paradigm for tracking vocal-tract resonances. In particular, we show the effectiveness of training the prediction-residual parameters in obtaining high-accuracy resonance estimates, especially during consonantal closure.},
  keywords = {Bandwidth,Cepstral analysis,Continuity constraint,Covariance matrix,dynamic programming,expectation--maximization (EM) optimization,formant,greedy search,Linear predictive coding,linear predictive coding (LPC) cepstrum,Natural languages,nonlinear prediction,prediction residual,quantization,Resonance,Resonant frequency,Speech analysis,Speech coding,Vectors,vocal-tract resonance (VTR)},
  file = {/home/marnix/Zotero/storage/BJPUY9KH/Deng et al. - 2006 - Tracking Vocal Tract Resonances Using a Quantized .pdf;/home/marnix/Zotero/storage/HHGMMKI5/1597248.html}
}

@article{Deng2007,
  title = {Adaptive {{Kalman Filtering}} and {{Smoothing}} for {{Tracking Vocal Tract Resonances Using}} a {{Continuous-Valued Hidden Dynamic Model}}},
  author = {Deng, L. and Lee, L. J. and Attias, H. and Acero, A.},
  year = {2007},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {15},
  number = {1},
  pages = {13--23},
  issn = {1558-7924},
  doi = {10.1109/TASL.2006.876724},
  abstract = {A novel Kalman filtering/smoothing algorithm is presented for efficient and accurate estimation of vocal tract resonances or formants, which are natural frequencies and bandwidths of the resonator from larynx to lips, in fluent speech. The algorithm uses a hidden dynamic model, with a state-space formulation, where the resonance frequency and bandwidth values are treated as continuous-valued hidden state variables. The observation equation of the model is constructed by an analytical predictive function from the resonance frequencies and bandwidths to LPC cepstra as the observation vectors. This nonlinear function is adaptively linearized, and a residual or bias term, which is adaptively trained, is added to the nonlinear function to represent the iteratively reduced piecewise linear approximation error. Details of the piecewise linearization design process are described. An iterative tracking algorithm is presented, which embeds both the adaptive residual training and piecewise linearization design in the Kalman filtering/smoothing framework. Experiments on estimating resonances in Switchboard speech data show accurate estimation results. In particular, the effectiveness of the adaptive residual training is demonstrated. Our approach provides a solution to the traditional "hidden formant problem," and produces meaningful results even during consonantal closures when the supra-laryngeal source may cause no spectral prominences in speech acoustics},
  keywords = {adaptive filters,Adaptive filters,adaptive Kalman filtering,Adaptive piecewise linearization,adaptive residual parameter learning,adaptive residual training,approximation theory,Bandwidth,continuous dynamics,continuous-valued hidden dynamic model,Filtering algorithms,formant analysis,Frequency estimation,hidden dynamic model,iterative methods,iterative tracking algorithm,Kalman filters,LPC cepstra,nonlinear prediction,piecewise linear approximation error,piecewise linearization design,Resonance,Resonant frequency,Resonator filters,smoothing methods,Smoothing methods,Speech,speech acoustics,speech processing,state-space formulation,state-space model,supra-laryngeal source,vocal tract resonance,vocal tract resonances smoothing,vocal tract resonances tracking},
  file = {/home/marnix/Zotero/storage/38JMF5EA/Deng et al. - 2007 - Adaptive Kalman Filtering and Smoothing for Tracki.pdf;/home/marnix/Zotero/storage/7ADPX862/4032768.html}
}

@misc{Dettmers2023,
  title = {{{QLoRA}}: {{Efficient Finetuning}} of {{Quantized LLMs}}},
  shorttitle = {{{QLoRA}}},
  author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  year = {2023},
  month = may,
  number = {arXiv:2305.14314},
  eprint = {2305.14314},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.14314},
  urldate = {2023-09-16},
  abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/marnix/Zotero/storage/84HVWUWY/Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf;/home/marnix/Zotero/storage/58PCC28D/2305.html}
}

@article{DeWitte2017,
  title = {A {{Forensic Speaker Identification Study}}. {{An Auditory-Acoustic Analysis}} of {{Phonetic Features}} and an {{Exploration}} of the" {{Telephone Effect}}"},
  author = {De Witte, Wolf},
  year = {2017},
  file = {/home/marnix/Zotero/storage/A7AW6SIV/De Witte - 2017 - A Forensic Speaker Identification Study. An Audito.pdf}
}

@article{deWolff2020,
  title = {Gaussian Process Imputation of Multiple Financial Series},
  author = {{de Wolff}, Taco and Cuevas, Alejandro and Tobar, Felipe},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.05789 [cs, eess, q-fin, stat]},
  eprint = {2002.05789},
  primaryclass = {cs, eess, q-fin, stat},
  urldate = {2021-09-23},
  abstract = {In Financial Signal Processing, multiple time series such as financial indicators, stock prices and exchange rates are strongly coupled due to their dependence on the latent state of the market and therefore they are required to be jointly analysed. We focus on learning the relationships among financial time series by modelling them through a multi-output Gaussian process (MOGP) with expressive covariance functions. Learning these market dependencies among financial series is crucial for the imputation and prediction of financial observations. The proposed model is validated experimentally on two real-world financial datasets for which their correlations across channels are analysed. We compare our model against other MOGPs and the independent Gaussian process on real financial data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Quantitative Finance - Statistical Finance,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/WZ4KX6Z4/de Wolff et al. - 2020 - Gaussian process imputation of multiple financial .pdf;/home/marnix/Zotero/storage/DBTJTPUJ/2002.html}
}

@article{Diaconis1999,
  title = {Iterated Random Functions},
  author = {Diaconis, Persi and Freedman, David},
  year = {1999},
  journal = {SIAM review},
  volume = {41},
  number = {1},
  pages = {45--76},
  publisher = {SIAM},
  file = {/home/marnix/Zotero/storage/NB5BBLC6/Diaconis and Freedman - ITERATED RANDOM FUNCTIONS.pdf}
}

@article{DiDonato1967,
  title = {The {{Efficient Calculation}} of the {{Incomplete Beta-Function Ratio}} for {{Half- Integer Values}} of the {{Parameters}} a, b},
  author = {DiDonato, A. R. and Jarnagin, M. P.},
  year = {1967},
  month = oct,
  journal = {Mathematics of Computation},
  volume = {21},
  number = {100},
  eprint = {2005009},
  eprinttype = {jstor},
  pages = {652},
  issn = {00255718},
  doi = {10.2307/2005009},
  urldate = {2025-06-24},
  file = {/home/marnix/Zotero/storage/2JJM9ZSS/DiDonato and Jarnagin - 1967 - The Efficient Calculation of the Incomplete Beta-Function Ratio for Half- Integer Values of the Para.pdf}
}

@article{Dixit2014,
  title = {The Finite {{Fourier}} Transform of Classical Polynomials},
  author = {Dixit, Atul and Jiu, Lin and Moll, Victor H. and Vignat, Christophe},
  year = {2014},
  month = feb,
  journal = {arXiv:1402.5544 [math]},
  eprint = {1402.5544},
  primaryclass = {math},
  urldate = {2021-11-24},
  abstract = {The finite Fourier transform of a family of orthogonal polynomials \$A\_\{n\}(x)\$, is the usual transform of the polynomial extended by \$0\$ outside their natural domain. Explicit expressions are given for the Legendre, Jacobi, Gegenbauer and Chebyshev families.},
  archiveprefix = {arXiv},
  keywords = {{42A16, 33C45},Mathematics - Classical Analysis and ODEs,Mathematics - Functional Analysis},
  file = {/home/marnix/Zotero/storage/CN7RZZEA/Dixit et al. - 2014 - The finite Fourier transform of classical polynomi.pdf;/home/marnix/Zotero/storage/AK83NHEE/1402.html}
}

@article{Donoho2000,
  title = {High-Dimensional Data Analysis: {{The}} Curses and Blessings of Dimensionality},
  author = {Donoho, David L and {Others}},
  year = {2000},
  publisher = {AMS Math Challenges Lecture 1-32},
  file = {/home/marnix/Zotero/storage/DNREJD7A/Donoho, others - 2000 - High-dimensional data analysis The curses and blessings of dimensionality.pdf}
}

@inproceedings{Doval2003,
  title = {The Voice Source as a Causal/Anticausal Linear Filter},
  booktitle = {{{VOQUAL}}'03},
  author = {Doval, Boris and D'Alessandro, Christophe and Henrich Bernardoni, Nathalie},
  year = {2003},
  pages = {1},
  address = {Gen{\`e}ve, Switzerland},
  urldate = {2022-04-08},
  abstract = {A new type of glottal flow model, namely a causal-anticausal linear filter model, is proposed. It is shown that the glottal flow signal can be considered as the impulse response of a linear filter. Then the source/filter speech model can be interpreted as an excitation/filter speech model, the " filter " comprising the glottal flow, vocal tract and radiation components. The spectral features of the voice source models are reviewed, both in the amplitude and phase domains. In the spectral amplitude domain the main features are a spectral maximum (the " glottal formant ") and spectral tilt. Evidence for a mixed causal/anticausal phase behavior of the source is given. Then, a causal-anticausal linear filter voice source model is designed. In conclusion, applications of this new approach are discussed for voice quality modification, voice source estimation, voice quality perception.},
  file = {/home/marnix/Zotero/storage/AANXN4QP/Doval et al. - 2003 - The voice source as a causalanticausal linear fil.pdf}
}

@article{Doval2006,
  title = {The Spectrum of Glottal Flow Models},
  author = {Doval, Boris and D'Alessandro, Christophe and Henrich, Nathalie},
  year = {2006},
  journal = {Acta Acustica united with Acustica},
  volume = {92},
  number = {6},
  pages = {1026--1046},
  issn = {16101928},
  abstract = {A unified description of the most-common glottal-flow models (KLGLOTT88, Rosenberg C, R++, LF) is proposed in the time domain, using a set of five generic glottal-flow parameters: fundamental period, maximum excitation, open quotient, asymmetry coefficient, and return-phase quotient. A unified set of time-domain equations is derived, and their analytical Laplace-transform computation leads to a set of frequency-domain equations. On the basis of this mathematical framework, the spectral properties of the glottal-flow models and their derivatives are studied. It is shown that any glottal-flow model can be described by a combination of low-pass filters, the cut-off frequencies and amplitudes of which can be expressed directly in terms of time-domain parameters. The spectral correlates of time-domain glottal-flow parameters are then explored. It is shown that the maximum excitation corresponds to a gain factor, and that it controls the mid-to-high-frequency spectral slope. A non-null return-phase quotient adds an additional spectral tilt in the high-frequency part of the glottal-flow spectrum. The open quotient and asymmetry coefficient are related to the low-frequency spectral peak, also called the glottal formant. The glottal-formant frequency is mainly controlled by the open quotient, and its amplitude (or bandwidth) by the asymmetry coefficient. As a direct application, it is shown that the amplitude difference between the first two harmonics, commonly assumed to be correlated to the open quotient, is also theoretically dependent on the asymmetry coefficient.},
  file = {/home/marnix/Zotero/storage/PXZEEGAY/Doval2006 The Spectrum of Glottal Flow Models.pdf}
}

@article{Drugman2011,
  title = {Causal--Anticausal Decomposition of Speech Using Complex Cepstrum for Glottal Source Estimation},
  author = {Drugman, Thomas and Bozkurt, Baris and Dutoit, Thierry},
  year = {2011},
  month = jul,
  journal = {Speech Communication},
  volume = {53},
  number = {6},
  pages = {855--866},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2011.02.004},
  urldate = {2023-04-17},
  abstract = {Complex cepstrum is known in the literature for linearly separating causal and anticausal components. Relying on advances achieved by the Zeros of the Z-Transform (ZZT) technique, we here investigate the possibility of using complex cepstrum for glottal flow estimation on a large-scale database. Via a systematic study of the windowing effects on the deconvolution quality, we show that the complex cepstrum causal--anticausal decomposition can be effectively used for glottal flow estimation when specific windowing criteria are met. It is also shown that this complex cepstral decomposition gives similar glottal estimates as obtained with the ZZT method. However, as complex cepstrum uses FFT operations instead of requiring the factoring of high-degree polynomials, the method benefits from a much higher speed. Finally in our tests on a large corpus of real expressive speech, we show that the proposed method has the potential to be used for voice quality analysis.},
  langid = {english},
  keywords = {Complex cepstrum,Glottal source estimation,Homomorphic analysis,Source-tract separation},
  file = {/home/marnix/Zotero/storage/CL52FXI5/Drugman et al. - 2011 - Causal–anticausal decomposition of speech using co.pdf;/home/marnix/Zotero/storage/XFKBE98W/S0167639311000227.html}
}

@article{Drugman2012,
  title = {Detection of {{Glottal Closure Instants From Speech Signals}}: {{A Quantitative Review}}},
  shorttitle = {Detection of {{Glottal Closure Instants From Speech Signals}}},
  author = {Drugman, T. and Thomas, M. and Gudnason, J. and Naylor, P. and Dutoit, T.},
  year = {2012},
  month = mar,
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {20},
  number = {3},
  pages = {994--1006},
  issn = {1558-7916},
  doi = {10.1109/TASL.2011.2170835},
  abstract = {The pseudo-periodicity of voiced speech can be exploited in several speech processing applications. This requires however that the precise locations of the glottal closure instants (GCIs) are available. The focus of this paper is the evaluation of automatic methods for the detection of GCIs directly from the speech waveform. Five state-of-the-art GCI detection algorithms are compared using six different databases with contemporaneous electroglottographic recordings as ground truth, and containing many hours of speech by multiple speakers. The five techniques compared are the Hilbert Envelope-based detection (HE), the Zero Frequency Resonator-based method (ZFR), the Dynamic Programming Phase Slope Algorithm (DYPSA), the Speech Event Detection using the Residual Excitation And a Mean-based Signal (SEDREAMS) and the Yet Another GCI Algorithm (YAGA). The efficacy of these methods is first evaluated on clean speech, both in terms of reliabililty and accuracy. Their robustness to additive noise and to reverberation is also assessed. A further contribution of the paper is the evaluation of their performance on a concrete application of speech processing: the causal-anticausal decomposition of speech. It is shown that for clean speech, SEDREAMS and YAGA are the best performing techniques, both in terms of identification rate and accuracy. ZFR and SEDREAMS also show a superior robustness to additive noise and reverberation.},
  keywords = {causal-anticausal speech decomposition,contemporaneous electroglottographic recording,Delay,dynamic programming phase slope algorithm,DYPSA,GCI detection algorithm,Glottal closure instant (GCI),glottal closure instant detection algorithm,HE-based detection,Heuristic algorithms,Hilbert Envelope-based detection,mean-based signal,pitch-synchronous,reliability,Reliability,residual excitation,Resonant frequency,SEDREAMS,Speech,speech analysis,speech event detection,speech processing,Speech processing,speech processing application,speech signal detection,speech waveform,Synchronization,voiced speech pseudoperiodicity,YAGA,yet another GCI algorithm,zero frequency resonator-based method,ZFR-based method},
  file = {/home/marnix/Zotero/storage/Z56D39WC/Drugman et al. - 2012 - Detection of Glottal Closure Instants From Speech .pdf;/home/marnix/Zotero/storage/YMIXYFUG/6080715.html}
}

@article{Drugman2019,
  title = {A {{Comparative Study}} of {{Glottal Source Estimation Techniques}}},
  author = {Drugman, Thomas and Bozkurt, Baris and Dutoit, Thierry},
  year = {2019},
  month = dec,
  journal = {arXiv:2001.00840 [cs, eess]},
  eprint = {2001.00840},
  primaryclass = {cs, eess},
  urldate = {2022-04-05},
  abstract = {Source-tract decomposition (or glottal flow estimation) is one of the basic problems of speech processing. For this, several techniques have been proposed in the literature. However studies comparing different approaches are almost nonexistent. Besides, experiments have been systematically performed either on synthetic speech or on sustained vowels. In this study we compare three of the main representative state-of-the-art methods of glottal flow estimation: closed-phase inverse filtering, iterative and adaptive inverse filtering, and mixed-phase decomposition. These techniques are first submitted to an objective assessment test on synthetic speech signals. Their sensitivity to various factors affecting the estimation quality, as well as their robustness to noise are studied. In a second experiment, their ability to label voice quality (tensed, modal, soft) is studied on a large corpus of real connected speech. It is shown that changes of voice quality are reflected by significant modifications in glottal feature distributions. Techniques based on the mixed-phase decomposition and on a closed-phase inverse filtering process turn out to give the best results on both clean synthetic and real speech signals. On the other hand, iterative and adaptive inverse filtering is recommended in noisy environments for its high robustness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/marnix/Zotero/storage/3DBFKA5T/Drugman et al. - 2019 - A Comparative Study of Glottal Source Estimation T.pdf;/home/marnix/Zotero/storage/C5IXKKH7/2001.html}
}

@misc{Drugman2019a,
  title = {Glottal {{Source Processing}}: From {{Analysis}} to {{Applications}}},
  shorttitle = {Glottal {{Source Processing}}},
  author = {Drugman, Thomas and Alku, Paavo and Alwan, Abeer and Yegnanarayana, Bayya},
  year = {2019},
  month = dec,
  number = {arXiv:1912.12604},
  eprint = {1912.12604},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  urldate = {2023-03-08},
  abstract = {The great majority of current voice technology applications relies on acoustic features characterizing the vocal tract response, such as the widely used MFCC of LPC parameters. Nonetheless, the airflow passing through the vocal folds, and called glottal flow, is expected to exhibit a relevant complementarity. Unfortunately, glottal analysis from speech recordings requires specific and more complex processing operations, which explains why it has been generally avoided. This review gives a general overview of techniques which have been designed for glottal source processing. Starting from fundamental analysis tools of pitch tracking, glottal closure instant detection, glottal flow estimation and modelling, this paper then highlights how these solutions can be properly integrated within various voice technology applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/marnix/Zotero/storage/UKYS2VPA/Drugman et al. - 2019 - Glottal Source Processing from Analysis to Applic.pdf;/home/marnix/Zotero/storage/WCBXZQDK/1912.html}
}

@article{Du2010,
  title = {A Note on the von {{Neumann}} Entropy of Random Graphs},
  author = {Du, Wenxue and Li, Xueliang and Li, Yiyang and Severini, Simone},
  year = {2010},
  journal = {Linear Algebra and its Applications},
  volume = {433},
  number = {11},
  pages = {1722--1725},
  issn = {0024-3795},
  doi = {http://dx.doi.org/10.1016/j.laa.2010.06.040},
  keywords = {Graph spectra,Laplacian,von Neumann entropy},
  file = {/home/marnix/Zotero/storage/HRN9PVV6/Du et al. - 2010 - A note on the von Neumann entropy of random graphs.pdf}
}

@book{Duch2012,
  title = {Artificial {{Neural Networks}} and {{Machine Learning}}--{{ICANN}} 2012},
  booktitle = {22nd International Conference on Artificial Neural Networks, Lausanne, Switzerland, September 11-14, 2012, Proceedings, Part I},
  author = {Duch, {\relax AEPVW} and Masulli, {\relax P{\'E}F} and Palm, G},
  year = {2012},
  volume = {7552},
  doi = {10.1007/978-3-642-33269-2_1},
  isbn = {978-3-642-33269-2},
  file = {/home/marnix/Zotero/storage/9PEZJ967/2012 Artificial Neural Networks and Machine Learning.pdf}
}

@article{Duff2001,
  title = {Trialogue on the Number of Fundamental Constants},
  author = {Duff, M. J. and Okun, L. B. and Veneziano, G.},
  year = {2001},
  eprintclass = {physics},
  issn = {1029-8479},
  doi = {10.1088/1126-6708/2002/03/023},
  abstract = {This paper consists of three separate articles on the number of fundamental dimensionful constants in physics. We started our debate in summer 1992 on the terrace of the famous CERN cafeteria. In the summer of 2001 we returned to the subject to find that our views still diverged and decided to explain our current positions. LBO develops the traditional approach with three constants, GV argues in favor of at most two (within superstring theory), while MJD advocates zero.},
  arxiv = {0110060},
  arxivid = {physics/0110060},
  isbn = {1126-6708},
  keywords = {models of quantum gravity,standard model},
  file = {/home/marnix/Zotero/storage/958TR6RA/Trialogue on the number of fundamental constants.pdf}
}

@incollection{Duifhuis1986,
  title = {Modelling the {{Cochlear Partition}} with {{Coupled}} van Der {{Pol Oscillators}}},
  booktitle = {Peripheral {{Auditory Mechanisms}}: {{Proceedings}} of a Conference Held at {{Boston University}}, {{Boston}}, {{MA}}, {{August}} 13--16, 1985},
  author = {Duifhuis, H and Hoogstraten, H W and {van Netten}, S M and Diependaal, R J and Bialek, W},
  editor = {Allen, J B and Hall, J L and Hubbard, A E and Neely, S T and Tubis, A},
  year = {1986},
  pages = {290--297},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-50038-1_36},
  abstract = {Within the context of interest in analyzing `active' and nonlinear processes in the cochlea we have been studying a model cochlea in which the local membrane impedance is described by a Van der Pol-oscillator. The behaviour of the undriven and sinusoidally driven discretized model is examined numerically. The undriven model describes the behaviour of a discrete number of coupled oscillators, which, if uncoupled, would have limit cycles gradually differing in frequency. In the coupled case the limit cycle behaviour is less predictable: it appears to exhibit quasi stochastic properties. In the driven model a sufficiently strong stimulus causes entrainment to the stimulus, and odd order harmonics appear. In the range where the driven response is small compared with the average limit cycle, the response is almost linear. The strict Van der Pol-damping function, which is parabolic in velocity, produces strong saturation. A generalized Van der Pol-damping term, which causes small-amplitude instability and large-amplitude stability, produces much the same general behaviour, but the intensity response can be modelled more realistically.},
  isbn = {978-3-642-50038-1}
}

@book{Duifhuis2012,
  title = {Cochlear {{Mechanics}}},
  author = {Duifhuis, H},
  year = {2012},
  isbn = {978-1-4419-6116-7}
}

@article{Dunn1961,
  title = {Methods of {{Measuring Vowel Formant Bandwidths}}},
  author = {Dunn, H. K.},
  year = {1961},
  month = dec,
  journal = {The Journal of the Acoustical Society of America},
  volume = {33},
  number = {12},
  pages = {1737--1746},
  publisher = {Acoustical Society of America},
  issn = {0001-4966},
  doi = {10.1121/1.1908558},
  urldate = {2022-10-29},
  file = {/home/marnix/Zotero/storage/664WBIBH/Dunn - 1961 - Methods of Measuring Vowel Formant Bandwidths.pdf}
}

@book{Dunne2013,
  title = {Speculative Everything: {{Design}}, Fiction, and Social Dreaming},
  author = {Dunne, Anthony and Raby, Fiona},
  year = {2013},
  pages = {240},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  isbn = {978-0-262-01984-2}
}

@article{Dupoux2018,
  title = {Cognitive Science in the Era of Artificial Intelligence: {{A}} Roadmap for Reverse-Engineering the Infant Language-Learner},
  shorttitle = {Cognitive Science in the Era of Artificial Intelligence},
  author = {Dupoux, Emmanuel},
  year = {2018},
  month = apr,
  journal = {Cognition},
  volume = {173},
  pages = {43--59},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2017.11.008},
  urldate = {2020-06-09},
  abstract = {Spectacular progress in the information processing sciences (machine learning, wearable sensors) promises to revolutionize the study of cognitive development. Here, we analyse the conditions under which 'reverse engineering' language development, i.e., building an effective system that mimics infant's achievements, can contribute to our scientific understanding of early language development. We argue that, on the computational side, it is important to move from toy problems to the full complexity of the learning situation, and take as input as faithful reconstructions of the sensory signals available to infants as possible. On the data side, accessible but privacy-preserving repositories of home data have to be setup. On the psycholinguistic side, specific tests have to be constructed to benchmark humans and machines at different linguistic levels. We discuss the feasibility of this approach and present an overview of current results.},
  langid = {english},
  keywords = {Artificial intelligence,Computational modeling,Corpus analysis,Early language acquisition,Infant development,Language bootstrapping,Machine learning,psycholinguistics,Speech},
  file = {/home/marnix/Zotero/storage/NJGL3FHE/Dupoux - 2018 - Cognitive science in the era of artificial intelli.pdf;/home/marnix/Zotero/storage/AGPL5LRU/S0010027717303013.html}
}

@article{Durrande2013,
  title = {{{ANOVA}} Kernels and {{RKHS}} of Zero Mean Functions for Model-Based Sensitivity Analysis},
  author = {Durrande, N. and Ginsbourger, D. and Roustant, O. and Carraro, L.},
  year = {2013},
  month = mar,
  journal = {Journal of Multivariate Analysis},
  volume = {115},
  pages = {57--67},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2012.08.016},
  urldate = {2021-08-10},
  abstract = {Given a reproducing kernel Hilbert space (H,{\textlangle}.,.{\textrangle}) of real-valued functions and a suitable measure {$\mu$} over the source space D{$\subset$}R, we decompose H as the sum of a subspace of centered functions for {$\mu$} and its orthogonal in H. This decomposition leads to a special case of ANOVA kernels, for which the functional ANOVA representation of the best predictor can be elegantly derived, either in an interpolation or regularization framework. The proposed kernels appear to be particularly convenient for analyzing the effect of each (group of) variable(s) and computing sensitivity indices without recursivity.},
  langid = {english},
  keywords = {Gaussian process regression,Global sensitivity analysis,Hoeffding--Sobol decomposition,SS-ANOVA},
  file = {/home/marnix/Zotero/storage/6A54IDYF/Durrande et al. - 2013 - ANOVA kernels and RKHS of zero mean functions for .pdf}
}

@article{Durrande2016,
  title = {Detecting Periodicities with {{Gaussian}} Processes},
  author = {Durrande, Nicolas and Hensman, James and Rattray, Magnus and Lawrence, Neil D.},
  year = {2016},
  month = apr,
  journal = {PeerJ Computer Science},
  volume = {2},
  pages = {e50},
  publisher = {PeerJ Inc.},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.50},
  urldate = {2025-04-21},
  abstract = {We consider the problem of detecting and quantifying the periodic component of a function given noise-corrupted observations of a limited number of input/output tuples. Our approach is based on Gaussian process regression, which provides a flexible non-parametric framework for modelling periodic data. We introduce a novel decomposition of the covariance function as the sum of periodic and aperiodic kernels. This decomposition allows for the creation of sub-models which capture the periodic nature of the signal and its complement. To quantify the periodicity of the signal, we derive a periodicity ratio which reflects the uncertainty in the fitted sub-models. Although the method can be applied to many kernels, we give a special emphasis to the Mat{\'e}rn family, from the expression of the reproducing kernel Hilbert space inner product to the implementation of the associated periodic kernels in a Gaussian process toolkit. The proposed method is illustrated by considering the detection of periodically expressed genes in the arabidopsis genome.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/PGC8DKKL/Durrande et al. - 2016 - Detecting periodicities with Gaussian processes.pdf}
}

@inproceedings{Dutordoir2020,
  title = {Sparse {{Gaussian Processes}} with {{Spherical Harmonic Features}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Dutordoir, Vincent and Durrande, Nicolas and Hensman, James},
  year = {2020},
  month = nov,
  pages = {2793--2802},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-04-24},
  abstract = {We introduce a new class of inter-domain variational Gaussian processes (GP) where data is mapped onto the unit hypersphere in order to use spherical harmonic representations. Our inference scheme is comparable to variational Fourier features, but it does not suffer from the curse of dimensionality, and leads to diagonal covariance matrices between inducing variables. This enables a speed-up in inference, because it bypasses the need to invert large covariance matrices. Our experiments show that our model is able to fit a regression model for a dataset with 6 million entries two orders of magnitude faster compared to standard sparse GPs, while retaining state of the art accuracy. We also demonstrate competitive performance on classification with non-conjugate likelihoods.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/D4JHU8DD/Dutordoir et al. - 2020 - Sparse Gaussian Processes with Spherical Harmonic Features.pdf;/home/marnix/Zotero/storage/MA47Z6WC/Dutordoir et al. - 2020 - Sparse Gaussian Processes with Spherical Harmonic Features.pdf}
}

@article{Dutordoir2021,
  title = {Deep {{Neural Networks}} as {{Point Estimates}} for {{Deep Gaussian Processes}}},
  author = {Dutordoir, Vincent and Hensman, James and {van der Wilk}, Mark and Ek, Carl Henrik and Ghahramani, Zoubin and Durrande, Nicolas},
  year = {2021},
  month = may,
  journal = {arXiv:2105.04504 [cs, stat]},
  eprint = {2105.04504},
  primaryclass = {cs, stat},
  urldate = {2021-10-04},
  abstract = {Deep Gaussian processes (DGPs) have struggled for relevance in applications due to the challenges and cost associated with Bayesian inference. In this paper we propose a sparse variational approximation for DGPs for which the approximate posterior mean has the same mathematical structure as a Deep Neural Network (DNN). We make the forward pass through a DGP equivalent to a ReLU DNN by finding an interdomain transformation that represents the GP posterior mean as a sum of ReLU basis functions. This unification enables the initialisation and training of the DGP as a neural network, leveraging the well established practice in the deep learning community, and so greatly aiding the inference task. The experiments demonstrate improved accuracy and faster training compared to current DGP methods, while retaining favourable predictive uncertainties.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/DU26BLGC/Dutordoir et al. - 2021 - Deep Neural Networks as Point Estimates for Deep G.pdf;/home/marnix/Zotero/storage/SBJ3C9PK/2105.html}
}

@article{Eisenberg2001,
  title = {Systemic {{Risk}} in {{Financial Systems}}},
  author = {Eisenberg, Larry K and Noe, Thomas H},
  year = {2001},
  journal = {Management Science},
  number = {February 2015},
  pages = {1--21},
  issn = {15565068},
  doi = {10.2139/ssrn.173249},
  abstract = {We consider default by firms that are part of a single clearing mechanism. The obligations of all firms within the system are determined simultaneously in a fashion consistent with the priority of debt claims and the limited liability of equity. We first show, via a fixed-point argument, that there always exists a "clearing payment vector" that clears the obligations of the members of the clearing system; under mild regularity conditions, this clearing vector is unique. Next, we develop an algorithm that both clears the financial network in a computationally efficient fashion and provides information on the systemic risk faced by individual system firms. Finally, we produce qualitative comparative statics for financial networks. These comparative statics imply that, in contrast to single-firm results, unsystematic, nondissipative shocks to the system will lower the total value of the network and may lower the value of the equity of some of the individual network firms.},
  keywords = {ali,ali jadbabaie,alp simsek,and 2013,contagion,counterparty risk,d85,david brown,financial network,for useful feedback and,g01,gary gorton,jean-charles rochet,jel classification,ozan candogan,participants at the 2012,shourideh and rakesh vohra,suggestions,systemic risk,we also thank seminar,we are grateful to},
  file = {/home/marnix/Zotero/storage/DEUMC3XK/Eisenberg, Noe - 2001 - Systemic Risk in Financial Systems.pdf}
}

@inproceedings{Ek2024,
  title = {Speculative Design through the Lens of {{AI}}},
  booktitle = {Proceedings of the 26th International Conference on Engineering and Product Design Education ({{EPDE2024}})},
  author = {Ek, Mathias {\O}stvold and Paulsen, Magnus and Trondsen, June Kyong},
  year = {2024},
  publisher = {The Design Society},
  address = {Birmingham, UK}
}

@book{Ekstrom2023,
  title = {{{PREQUEL}}: {{Supervised}} Phonetic Approaches to Analyses of Great Ape Quasi-Vowels},
  shorttitle = {{{PREQUEL}}},
  author = {Ekstr{\"o}m, Axel G. and Moran, Steven and Sundberg, Johan and Lameira, Adriano},
  year = {2023},
  month = apr,
  abstract = {There is renewed interest in potential vowel production by nonhuman primates, but no agreed-upon methodologies for its estimation from real-life vocalizations. Here, we present a set of supervised approaches for estimating primate vowel-like articulation, with reference to orangutan long call pulses (N=36). We summarize our approach as a cohesive framework, the Primate Quasi-Vowel (PREQUEL) protocol. We (1) estimated f 0 from correlograms, (2) and vocal tract resonances (formants) from spectrograms, (3) the results of which were then compared against synthesized vowels for those frequency values; and (4) presented to uninformed listeners (N=16), who largely agreed on the categorization of vowel-like qualities for vocalizations (Cronbach's alpha=.701). We also provide descriptions of methods that are seemingly inadequate for formant estimation in great ape calls. We argue that a combination of phonetic methods is required to develop a science of nonhuman primate articulation.},
  file = {/home/marnix/Zotero/storage/S8U5LXG2/Ekström et al. - 2023 - PREQUEL Supervised phonetic approaches to analyse.pdf}
}

@misc{Eldan2023,
  title = {{{TinyStories}}: {{How Small Can Language Models Be}} and {{Still Speak Coherent English}}?},
  shorttitle = {{{TinyStories}}},
  author = {Eldan, Ronen and Li, Yuanzhi},
  year = {2023},
  month = may,
  number = {arXiv:2305.07759},
  eprint = {2305.07759},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.07759},
  urldate = {2023-09-16},
  abstract = {Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention). In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities. We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency. We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/marnix/Zotero/storage/XBYYH2U3/Eldan and Li - 2023 - TinyStories How Small Can Language Models Be and .pdf;/home/marnix/Zotero/storage/WY3776GY/2305.html}
}

@misc{Eleftheriadis2023,
  title = {Sparse {{Gaussian Processes}} with {{Spherical Harmonic Features Revisited}}},
  author = {Eleftheriadis, Stefanos and Richards, Dominic and Hensman, James},
  year = {2023},
  month = mar,
  number = {arXiv:2303.15948},
  eprint = {2303.15948},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.15948},
  urldate = {2025-09-03},
  abstract = {We revisit the Gaussian process model with spherical harmonic features and study connections between the associated RKHS, its eigenstructure and deep models. Based on this, we introduce a new class of kernels which correspond to deep models of continuous depth. In our formulation, depth can be estimated as a kernel hyper-parameter by optimizing the evidence lower bound. Further, we introduce sparseness in the eigenbasis by variational learning of the spherical harmonic phases. This enables scaling to larger input dimensions than previously, while also allowing for learning of high frequency variations. We validate our approach on machine learning benchmark datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/YI6QH9VU/Eleftheriadis et al. - 2023 - Sparse Gaussian Processes with Spherical Harmonic Features Revisited.pdf;/home/marnix/Zotero/storage/89CFAYW7/2303.html}
}

@article{Elliott2009,
  title = {The Modulation Transfer Function for Speech Intelligibility},
  author = {Elliott, Taffeta M. and Theunissen, Fr{\'e}d{\'e}ric E.},
  year = {2009},
  journal = {PLoS Computational Biology},
  volume = {5},
  number = {3},
  issn = {1553734X},
  doi = {10.1371/journal.pcbi.1000302},
  abstract = {We systematically determined which spectrotemporal modulations in speech are necessary for comprehension by human listeners. Speech comprehension has been shown to be robust to spectral and temporal degradations, but the specific relevance of particular degradations is arguable due to the complexity of the joint spectral and temporal information in the speech signal. We applied a novel modulation filtering technique to recorded sentences to restrict acoustic information quantitatively and to obtain a joint spectrotemporal modulation transfer function for speech comprehension, the speech MTF. For American English, the speech MTF showed the criticality of low modulation frequencies in both time and frequency. Comprehension was significantly impaired when temporal modulations 12 Hz or spectral modulations 4 cycles/kHz were removed. More specifically, the MTF was bandpass in temporal modulations and low-pass in spectral modulations: temporal modulations from 1 to 7 Hz and spectral modulations 1 cycles/kHz were the most important. We evaluated the importance of spectrotemporal modulations for vocal gender identification and found a different region of interest: removing spectral modulations between 3 and 7 cycles/kHz significantly increases gender misidentifications of female speakers. The determination of the speech MTF furnishes an additional method for producing speech signals with reduced bandwidth but high intelligibility. Such compression could be used for audio applications such as file compression or noise removal and for clinical applications such as signal processing for cochlear implants.},
  isbn = {1553-7358 (Electronic)\r1553-734X (Linking)},
  pmid = {19266016},
  file = {/home/marnix/Zotero/storage/YLLBZR5X/Elliot2009 The Modulation Transfer Function for Speech Intelligibility.pdf}
}

@article{Elsinger2007,
  title = {A New Approach to Assessing the Risk of Interbank Loans},
  author = {Elsinger, H. and Lehar, a. and Summer, M.},
  year = {2007},
  journal = {Financial Stability Review},
  volume = {3},
  pages = {75--86},
  abstract = {The use of complex and powerful risk management methods has been one of the key innovations in the banking sector over the past two decades. One of the factors driving this development is certainly that since the early 1970s banks have had to cope with a significantly more volatile and dynamic environment compared to the years following World War II. In the immediate post-war period, currency crises were largely insignificant, market interest rates fluctu- ated only negligibly, competition was limited by cartels and interest rate regu- lation, and competition by financial intermediaries outside the banking sector was insignificant. Once the Bretton Woods system had collapsed, the situation changed drastically, however. Exchange rate risks started to play a role, interest rate fluctuations reached previously unknown dimensions, the lifting of capital controls resulted in a considerable internationalization of the financial system, and competition by nonbanks increased strongly. New technologies and means of communications rendered barriers to competition, such as distance and national borders, obsolete. In addition, financial innovations abounded. Against this background, regulators started to exert more pressure on banks. As capital adequacy provisions were continuously extended and refined, regulators relied heavily on individual risk management models. In the public these regulatory measures were invariably justified by pointing out the need to attenuate systemic risks and strengthen financial stability. The question remains, however, whether improving risk management models and implementing capital adequacy guide- lines at the level of individual banks automatically leads to more efficient risk control at the level of the banking system.},
  file = {/home/marnix/Zotero/storage/NUWHJ46B/Elsinger, Lehar, Summer - 2007 - A new approach to assessing the risk of interbank loans.pdf}
}

@book{Engel2001,
  title = {Statistical {{Mechanics}} of {{Learning}}},
  author = {Engel, Andreas and {den Broeck}, Christian P L Van},
  year = {2001},
  publisher = {Cambridge University Press},
  address = {New York, NY, USA},
  isbn = {0-521-77307-5}
}

@article{Erell1991,
  title = {{{JND}}'s in the {{LPC}} Poles of Speech and Their Application to Quantization of the {{LPC}} Filter},
  author = {Erell, A. and Orgad, Y. and Goldstein, J.L.},
  year = {1991},
  month = feb,
  journal = {IEEE Transactions on Signal Processing},
  volume = {39},
  number = {2},
  pages = {308--318},
  issn = {1053587X},
  doi = {10.1109/78.80813},
  urldate = {2019-04-29},
  file = {/home/marnix/Zotero/storage/YTRPZ64Z/Erell et al. - 1991 - JND's in the LPC poles of speech and their applica.pdf}
}

@article{Erp2017,
  title = {Deriving {{Proper Uniform Priors}} for {{Regression Coefficients}}, {{Parts I}}, {{II}}, and {{III}}},
  author = {van Erp, H. R. Noel and Linger, Ronald O. and van Gelder, Pieter H. A. J. M.},
  year = {2017},
  month = jun,
  journal = {Entropy},
  volume = {19},
  number = {6},
  pages = {250},
  publisher = {Multidisciplinary Digital Publishing Institute},
  doi = {10.3390/e19060250},
  urldate = {2021-03-24},
  abstract = {It is a relatively well-known fact that in problems of Bayesian model selection, improper priors should, in general, be avoided. In this paper we will derive and discuss a collection of four proper uniform priors which lie on an ascending scale of informativeness. It will turn out that these priors lead us to evidences that are closely associated with the implied evidence of the Bayesian Information Criterion (BIC) and the Akaike Information Criterion (AIC). All the discussed evidences are then used in two small Monte Carlo studies, wherein for different sample sizes and noise levels the evidences are used to select between competing C-spline regression models. Also, there is given, for illustrative purposes, an outline on how to construct simple trivariate C-spline regression models. In regards to the length of this paper, only one half of this paper consists of theory and derivations, the other half consists of graphs and outputs of the two Monte Carlo studies.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Akaike Information Criterion (AIC),Bayesian,Bayesian Information Criterion (BIC),model selection,non-linear,proper uniform priors,regression analysis,regression coefficients,splines},
  file = {/home/marnix/Zotero/storage/WFDI8GLE/Erp et al. - 2017 - Deriving Proper Uniform Priors for Regression Coef.pdf;/home/marnix/Zotero/storage/RQPPAGQI/htm.html}
}

@misc{Ethayarajh2024,
  title = {{{KTO}}: {{Model Alignment}} as {{Prospect Theoretic Optimization}}},
  shorttitle = {{{KTO}}},
  author = {Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  year = {2024},
  month = feb,
  number = {arXiv:2402.01306},
  eprint = {2402.01306},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.01306},
  urldate = {2024-04-14},
  abstract = {Kahneman \& Tversky's \${\textbackslash}textit\{prospect theory\}\$ tells us that humans perceive random variables in a biased but well-defined manner; for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them being \${\textbackslash}textit\{human-aware loss functions\}\$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach Kahneman-Tversky Optimization (KTO), and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B. Crucially, KTO does not need preferences -- only a binary signal of whether an output is desirable or undesirable for a given input. This makes it far easier to use in the real world, where preference data is scarce and expensive.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/marnix/Zotero/storage/53U6W7M4/Ethayarajh et al. - 2024 - KTO Model Alignment as Prospect Theoretic Optimiz.pdf;/home/marnix/Zotero/storage/V6UTLMD7/2402.html}
}

@article{Ezzat2007,
  title = {Spectro-Temporal Analysis of Speech Using 2-d {{Gabor}} Filters.},
  author = {Ezzat, Tony and Bouvrie, Jake Jv and Poggio, Tomaso},
  year = {2007},
  journal = {Interspeech},
  pages = {1--4},
  abstract = {We present a 2-D spectro-temporal Gabor filterbank based on the 2-D Fast Fourier Transform, and show how it may be used to analyze localized patches of a spectrogram. We argue that the 2-D Gabor filterbank has the capacity to decompose a patch into its underlying dominant spectro-temporal components, and we illustrate the response of our filterbank to different speech phe-nomena such as harmonicity, formants, vertical onsets/offsets, noise, and overlapping simultaneous speakers.},
  isbn = {9781605603162},
  file = {/home/marnix/Zotero/storage/3L668W6Y/ezzat Gabor Filter spectral analysis.pdf}
}

@article{Faccin2020,
  title = {State Aggregations in {{Markov}} Chains and Block Models of Networks},
  author = {Faccin, Mauro and Schaub, Michael T. and Delvenne, Jean-Charles},
  year = {2020},
  month = may,
  journal = {arXiv:2005.00337 [physics]},
  eprint = {2005.00337},
  primaryclass = {physics},
  urldate = {2020-12-22},
  abstract = {We consider state aggregation schemes for Markov chains from an information-theoretic perspective. Specifically, we consider aggregating the states of a Markov chain such that the mutual information of the aggregated states separated by T time steps is maximized. We show that for T = 1 this approach recovers the maximum-likelihood estimator of the degree-corrected stochastic block model as a particular case, thereby enabling us to explain certain features of the likelihood landscape of this popular generative network model from a dynamical lens. We further highlight how we can uncover coherent, long-range dynamical modules for which considering a time-scale T {$>>$} 1 is essential, using synthetic flows and real-world ocean currents, where we are able to recover the fundamental features of the surface currents of the Oceans.},
  archiveprefix = {arXiv},
  keywords = {{05C81, 60J10},{Physics - Data Analysis, Statistics and Probability},Physics - Applied Physics,Physics - Physics and Society},
  file = {/home/marnix/Zotero/storage/VECCGQXB/Faccin et al. - 2020 - State aggregations in Markov chains and block mode.pdf;/home/marnix/Zotero/storage/BD8U9477/2005.html}
}

@article{Fancher2016,
  title = {Use of {{Bayesian Inference}} in {{Crystallographic Structure Refinement}} via {{Full Diffraction Profile Analysis}}},
  author = {Fancher, Chris M. and Han, Zhen and Levin, Igor and Page, Katharine and Reich, Brian J. and Smith, Ralph C. and Wilson, Alyson G. and Jones, Jacob L.},
  year = {2016},
  month = aug,
  journal = {Scientific Reports},
  volume = {6},
  number = {1},
  pages = {31625},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/srep31625},
  urldate = {2021-03-19},
  abstract = {A Bayesian inference method for refining crystallographic structures is presented. The distribution of model parameters is stochastically sampled using Markov chain Monte Carlo. Posterior probability distributions are constructed for all model parameters to properly quantify uncertainty by appropriately modeling the heteroskedasticity and correlation of the error structure. The proposed method is demonstrated by analyzing a National Institute of Standards and Technology silicon standard reference material. The results obtained by Bayesian inference are compared with those determined by Rietveld refinement. Posterior probability distributions of model parameters provide both estimates and uncertainties. The new method better estimates the true uncertainties in the model as compared to the Rietveld method.},
  copyright = {2016 The Author(s)},
  langid = {english},
  file = {/home/marnix/Zotero/storage/FIK6BRGW/Fancher et al. - 2016 - Use of Bayesian Inference in Crystallographic Stru.pdf;/home/marnix/Zotero/storage/HYX4YI8P/srep31625.html}
}

@article{Fano1950,
  title = {The {{Information Theory Point}} of {{View}} in {{Speech Communication}}},
  author = {Fano, R. M.},
  year = {1950},
  month = nov,
  journal = {The Journal of the Acoustical Society of America},
  volume = {22},
  number = {6},
  pages = {691--696},
  issn = {0001-4966},
  doi = {10.1121/1.1906671},
  urldate = {2019-02-04},
  file = {/home/marnix/Zotero/storage/89UHRZIN/Fano - 1950 - The Information Theory Point of View in Speech Com.pdf;/home/marnix/Zotero/storage/X5XZEJ96/1.html}
}

@book{Fant1960,
  title = {Acoustic Theory of Speech Production},
  author = {Fant, Gunnar},
  year = {1960},
  publisher = {Mouton},
  address = {Den Haag}
}

@article{Fant1962,
  title = {Formant Bandwidth Data},
  author = {Fant, G},
  year = {1962},
  journal = {Speech Transmission Laboratory Quarterly Progress and Status Report 2},
  volume = {3},
  pages = {1--3},
  file = {/home/marnix/Zotero/storage/PBPY83XD/Fant - Formant bandwidth data.pdf}
}

@article{Fant1966,
  title = {A Note on Vocal Tract Size Factors and Non-Uniform {{F-pattern}} Scalings},
  author = {Fant, Gunnar},
  year = {1966},
  journal = {Speech Transmission Laboratory Quarterly Progress and Status Report},
  volume = {1},
  pages = {22--30},
  file = {/home/marnix/Zotero/storage/U8Y3QVDD/Fant - F-Pattern Scalings.pdf}
}

@article{Fant1972,
  title = {Vocal Tract Wall Effects, Losses, and Resonance Bandwidths},
  author = {Fant, Gunnar},
  year = {1972},
  journal = {Speech Transmission Laboratory Quarterly progress and status report},
  volume = {2},
  number = {3},
  pages = {28--52},
  publisher = {Royal Institute of Technology Stockholm},
  file = {/home/marnix/Zotero/storage/JCRCDRP9/Fant - 1972 - Vocal tract wall effects, losses, and resonance ba.pdf}
}

@article{Fant1979,
  title = {Glottal Source and Excitation Analysis},
  author = {Fant, Gunnar},
  year = {1979},
  journal = {STL-QPSR},
  volume = {1},
  number = {1979},
  pages = {85--107},
  publisher = {KTH},
  file = {/home/marnix/Zotero/storage/HZ47KFH6/Fant - Glottal source and excitation analysis.pdf}
}

@article{Fant1981,
  title = {The Source Filter Concept in Voice Production},
  author = {Fant, Gunnar},
  year = {1981},
  journal = {STL-QPSR},
  volume = {1},
  number = {1981},
  pages = {21--37},
  file = {/home/marnix/Zotero/storage/IGAW25TN/Fant - 1981 - The source filter concept in voice production.pdf}
}

@article{Fant1982,
  title = {The Voice Source-Acoustic Modeling},
  author = {Fant, G},
  year = {1982},
  journal = {STL-QPSR},
  volume = {4},
  number = {1982},
  pages = {28--48},
  file = {/home/marnix/Zotero/storage/INWPSENQ/Fant - The voice source - acoustic modeling.pdf}
}

@article{Fant1985,
  title = {A Four-Parameter Model of Glottal Flow},
  author = {Fant, Gunnar and Liljencrants, Johan and Lin, Qi-guang},
  year = {1985},
  journal = {STL-QPSR},
  volume = {4},
  number = {1985},
  pages = {1--13},
  publisher = {Citeseer},
  file = {/home/marnix/Zotero/storage/28W6P4R9/Prnwctirn et al. - A four-parameter model of glottal flow.pdf;/home/marnix/Zotero/storage/VRMCWD4G/summary.html}
}

@inproceedings{Fant1994,
  title = {Voice Source Parameters in Continuous Speech, Transformation of {{LF-parameters}}.},
  booktitle = {{{ICSLP}}},
  author = {Fant, Gunnar and Kruckenberg, Anita and Liljencrants, Johan and B{\aa}veg{\aa}rd, Mats},
  year = {1994},
  volume = {94},
  pages = {1451--1454},
  file = {/home/marnix/Zotero/storage/NEKA522U/Fant et al. - 1994 - Voice source parameters in continuous speech, tran.pdf}
}

@article{Fant1995,
  title = {The {{LF-model}} Revisited. {{Transformations}} and Frequency Domain Analysis},
  author = {Fant, Gunnar},
  year = {1995},
  journal = {Speech Trans. Lab. Q. Rep., Royal Inst. of Tech. Stockholm},
  volume = {2},
  number = {3},
  pages = {40},
  file = {/home/marnix/Zotero/storage/F769ZGYH/Fant - The LF-model revisited. Transformations and freque.pdf}
}

@article{Faranda2014,
  title = {Modelling and Analysis of Turbulent Datasets Using {{Auto Regressive Moving Average}} Processes},
  author = {Faranda, Davide and Pons, Flavio Maria Emanuele and Dubrulle, B{\'e}reng{\`e}re and Daviaud, Fran{\c c}ois and {Saint-Michel}, Brice and Herbert, {\'E}ric and Cortet, Pierre-Philippe},
  year = {2014},
  month = oct,
  journal = {Physics of Fluids},
  volume = {26},
  number = {10},
  pages = {105101},
  issn = {1070-6631},
  doi = {10.1063/1.4896637},
  urldate = {2019-04-23},
  file = {/home/marnix/Zotero/storage/TS9PRKQX/Faranda et al. - 2014 - Modelling and analysis of turbulent datasets using.pdf;/home/marnix/Zotero/storage/UU6ZTEHV/1.html}
}

@article{Farrier1984,
  title = {Jaynes' {{Principle}} and {{Maximum Entropy Spectral Estimation}}},
  author = {Farrier, D R},
  year = {1984},
  volume = {ASSP-32},
  number = {6},
  pages = {1176--1183},
  file = {/home/marnix/Zotero/storage/QLYSK39W/Farrier1984 Including prior information in MAXENT spectral information.pdf}
}

@article{Feroz2009,
  title = {{{MULTINEST}}: An Efficient and Robust {{Bayesian}} Inference Tool for Cosmology and Particle Physics},
  shorttitle = {{{MULTINEST}}},
  author = {Feroz, F. and Hobson, M. P. and Bridges, M.},
  year = {2009},
  month = oct,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {398},
  pages = {1601--1614},
  issn = {0035-8711},
  doi = {10.1111/j.1365-2966.2009.14548.x},
  urldate = {2021-04-15},
  abstract = {We present further development and the first public release of our  multimodal nested sampling algorithm, called MULTINEST. This Bayesian inference tool calculates the evidence, with an associated error estimate, and produces posterior samples from distributions that may contain multiple modes and pronounced (curving) degeneracies in high dimensions. The developments presented here lead to further substantial improvements in sampling efficiency and robustness, as compared to the original algorithm presented in Feroz \& Hobson, which itself significantly outperformed existing Markov chain Monte Carlo techniques in a wide range of astrophysical inference problems. The accuracy and economy of the MULTINEST algorithm are demonstrated by application to two toy problems and to a cosmological inference problem focusing on the extension of the vanilla {$\Lambda$} cold dark matter model to include spatial curvature and a varying equation of state for dark energy. The MULTINEST software, which is fully parallelized using MPI and includes an interface to COSMOMC, is available at http://www.mrao.cam.ac.uk/software/multinest/. It will also be released as part of the SUPERBAYES package, for the analysis of supersymmetric theories of particle physics, at http://www.superbayes.org.},
  keywords = {methods: data analysis,methods: statistical},
  file = {/home/marnix/Zotero/storage/TY39LFUM/Feroz et al. - 2009 - MULTINEST an efficient and robust Bayesian infere.pdf}
}

@article{Ferrer-i-Cancho2022,
  title = {Optimal Coding and the Origins of {{Zipfian}} Laws},
  author = {{Ferrer-i-Cancho}, Ramon and Bentz, Christian and Seguin, Caio},
  year = {2022},
  month = apr,
  journal = {Journal of Quantitative Linguistics},
  volume = {29},
  number = {2},
  eprint = {1906.01545},
  primaryclass = {physics},
  pages = {165--194},
  issn = {0929-6174, 1744-5035},
  doi = {10.1080/09296174.2020.1778387},
  urldate = {2022-06-16},
  abstract = {The problem of compression in standard information theory consists of assigning codes as short as possible to numbers. Here we consider the problem of optimal coding -- under an arbitrary coding scheme -- and show that it predicts Zipf's law of abbreviation, namely a tendency in natural languages for more frequent words to be shorter. We apply this result to investigate optimal coding also under so-called nonsingular coding, a scheme where unique segmentation is not warranted but codes stand for a distinct number. Optimal non-singular coding predicts that the length of a word should grow approximately as the logarithm of its frequency rank, which is again consistent with Zipf's law of abbreviation. Optimal non-singular coding in combination with the maximum entropy principle also predicts Zipf's rank-frequency distribution. Furthermore, our findings on optimal non-singular coding challenge common beliefs about random typing. It turns out that random typing is in fact an optimal coding process, in stark contrast with the common assumption that it is detached from cost cutting considerations. Finally, we discuss the implications of optimal coding for the construction of a compact theory of Zipfian laws more generally as well as other linguistic laws.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Theory,Physics - Physics and Society},
  file = {/home/marnix/Zotero/storage/E396UWS6/Ferrer-i-Cancho et al. - 2022 - Optimal coding and the origins of Zipfian laws.pdf}
}

@misc{Fessler2004,
  title = {Digital {{Signal Processing}} and {{Analysis Lecture Notes}}},
  author = {Fessler, J},
  year = {2004},
  langid = {english},
  file = {/home/marnix/Zotero/storage/APCSVJIH/Fessler - 2004 - EECS 451 Digital Signal Processing and Analysis.pdf}
}

@article{Feugere2017,
  title = {Cantor {{Digitalis}}: Chironomic Parametric Synthesis of Singing},
  shorttitle = {Cantor {{Digitalis}}},
  author = {Feug{\`e}re, Lionel and {d'Alessandro}, Christophe and Doval, Boris and Perrotin, Olivier},
  year = {2017},
  month = jan,
  journal = {EURASIP Journal on Audio, Speech, and Music Processing},
  volume = {2017},
  number = {1},
  pages = {2},
  issn = {1687-4722},
  doi = {10.1186/s13636-016-0098-5},
  urldate = {2022-07-07},
  abstract = {Cantor Digitalis is a performative singing synthesizer that is composed of two main parts: a chironomic control interface and a parametric voice synthesizer. The control interface is based on a pen/touch graphic tablet equipped with a template representing vocalic and melodic spaces. Hand and pen positions, pen pressure, and a graphical user interface are assigned to specific vocal controls. This interface allows for real-time accurate control over high-level singing synthesis parameters. The sound generation system is based on a parametric synthesizer that features a spectral voice source model, a vocal tract model consisting of parallel filters for vocalic formants and cascaded with anti-resonance for the spectral effect of hypo-pharynx cavities, and rules for parameter settings and source/filter dependencies between fundamental frequency, vocal effort, and formants. Because Cantor Digitalis is a parametric system, every aspect of voice quality can be controlled (e.g., vocal tract size, aperiodicities in the voice source, vowels, and so forth). It offers several presets for different voice types. Cantor Digitalis has been played on stage in several public concerts, and it has also been proven to be useful as a tool for voice pedagogy. The aim of this article is to provide a comprehensive technical overview of Cantor Digitalis.},
  keywords = {Digital musical instrument,Gestural control,Singing voice,Voice synthesis},
  file = {/home/marnix/Zotero/storage/MGSWIIQP/Feugère et al. - 2017 - Cantor Digitalis chironomic parametric synthesis .pdf;/home/marnix/Zotero/storage/ABY84D79/s13636-016-0098-5.html}
}

@article{Feynman1982,
  title = {Simulating Physics with Computers},
  author = {Feynman, Richard P.},
  year = {1982},
  journal = {International Journal of Theoretical Physics},
  volume = {21},
  number = {6-7},
  eprintclass = {quant-ph},
  pages = {467--488},
  issn = {00207748},
  doi = {10.1007/BF02650179},
  abstract = {A digital computer is generally believed to be an efficient universal computing device; that is, it is believed able to simulate any physical computing device with an increase in computation time of at most a polynomial factor. This may not be true when quantum mechanics is taken into consideration. This paper considers factoring integers and finding discrete logarithms, two problems which are generally thought to be hard on a classical computer and have been used as the basis of several proposed cryptosystems. Efficient randomized algorithms are given for these two problems on a hypothetical quantum computer. These algorithms take a number of steps polynomial in the input size, e.g., the number of digits of the integer to be factored. AMS subject classifications: 82P10, 11Y05, 68Q10. 1 Introduction One of the first results in the mathematics of computation, which underlies the subsequent development of much of theoretical computer science, was the distinction between computable and ...},
  arxiv = {9508027},
  arxivid = {quant-ph/9508027},
  isbn = {0020774815729575},
  file = {/home/marnix/Zotero/storage/KGW3HLER/Feynman 1981 Simulating physics with computers.pdf}
}

@article{Filip2019,
  title = {Smooth {{Random Functions}}, {{Random ODEs}}, and {{Gaussian Processes}}},
  author = {Filip, Silviu and Javeed, Aurya and Trefethen, Lloyd N.},
  year = {2019},
  month = jan,
  journal = {SIAM Review},
  volume = {61},
  number = {1},
  pages = {185--205},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/17M1161853},
  urldate = {2021-11-08},
  abstract = {The usual way in which mathematicians work with randomness is by a rigorous formulation of the idea of Brownian motion, which is the limit of a random walk as the step length goes to zero. A Brownian path is continuous but nowhere differentiable, and this nonsmoothness is associated with technical complications that can be daunting. However, there is another approach to random processes that is more elementary, involving smooth random functions defined by finite Fourier series with random coefficients or, equivalently, by trigonometric polynomial interpolation through random data values. We show here how smooth random functions can provide a very practical way to explore random effects. For example, one can solve smooth random ordinary differential equations using standard mathematical definitions and numerical algorithms, rather than having to develop new definitions and algorithms of stochastic differential equations. In the limit as the number of Fourier coefficients defining a smooth random function goes to \${\textbackslash}infty\$, one obtains the usual stochastic objects in what is known as their Stratonovich interpretation.},
  keywords = {42A16,60G15,62M40,band-limited white noise,Brownian motion,Chebfun,Dirichlet kernel,energy landscape,Fourier--Wiener series,Gaussian process,stochastic differential equation,white noise},
  file = {/home/marnix/Zotero/storage/IZCGG3E8/Filip et al. - 2019 - Smooth Random Functions, Random ODEs, and Gaussian.pdf}
}

@article{Finch2005,
  title = {Comparison of {{Distance Measures}} in {{Cluster Analysis}} with {{Dichotomous Data}}},
  author = {Finch, Holmes},
  year = {2005},
  journal = {Journal of Data Science},
  volume = {3},
  pages = {85--100},
  issn = {00223751},
  doi = {10.1111/j.1469-7793.1998.677bd.x},
  abstract = {The current study examines the performance of cluster analysis with dichotomous data using distance measures based on response pattern similarity. In many contexts, such as educational and psychological testing, cluster analysis is a useful means for exploring datasets and identifying un- derlying groups among individuals. However, standard approaches to cluster analysis assume that the variables used to group observations are continu- ous in nature. This paper focuses on four methods for calculating distance between individuals using dichotomous data, and the subsequent introduc- tion of these distances to a clustering algorithm such as Ward's. The four methods in question, are potentially useful for practitioners because they are relatively easy to carry out using standard statistical software such as SAS and SPSS, and have been shown to have potential for correctly grouping ob- servations based on dichotomous data. Results of both a simulation study and application to a set of binary survey responses show that three of the four measures behave similarly, and can yield correct cluster recovery rates of between 60\% and 90\%. Furthermore, these methods were found to work better, in nearly all cases, than using the raw data with Ward's clustering algorithm.},
  pmid = {9769413},
  keywords = {cluster analysis,dichotomous data,distance measures},
  file = {/home/marnix/Zotero/storage/2XS577YI/Finch - 2005 - Comparison of Distance Measures in Cluster Analysis with Dichotomous Data.pdf}
}

@article{Finger2013,
  title = {Network Analysis of the E-{{MID}} Overnight Money Market: The Informational Value of Different Aggregation Levels for Intrinsic Dynamic Processes},
  author = {Finger, Karl and Fricke, Daniel and Lux, Thomas},
  year = {2013},
  journal = {Computational Management Science},
  volume = {10},
  number = {2},
  pages = {187--211},
  issn = {1619-6988},
  doi = {10.1007/s10287-013-0171-9},
  abstract = {In this paper, we analyze the network properties of the Italian e-MID data based on overnight loans during the period 1999--2010. We show that the networks appear to be random at the daily level, but contain significant non-random structure for longer aggregation periods. In this sense, the daily networks cannot be considered as being representative for the underlying `latent' network. Rather, the development of various network statistics under time aggregation points toward strong non-random determinants of link formation. We also identify the global financial crisis as a significant structural break for many network measures.}
}

@article{Fischer2015,
  title = {Sampling {{Motif-Constrained Ensembles}} of {{Networks}}},
  author = {Fischer, Rico and Leit{\~a}o, Jorge C. and Peixoto, Tiago P. and Altmann, Eduardo G.},
  year = {2015},
  month = oct,
  journal = {Physical Review Letters},
  volume = {115},
  number = {18},
  pages = {188701},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.115.188701},
  urldate = {2022-05-25},
  abstract = {The statistical significance of network properties is conditioned on null models which satisfy specified properties but that are otherwise random. Exponential random graph models are a principled theoretical framework to generate such constrained ensembles, but which often fail in practice, either due to model inconsistency or due to the impossibility to sample networks from them. These problems affect the important case of networks with prescribed clustering coefficient or number of small connected subgraphs (motifs). In this Letter we use the Wang-Landau method to obtain a multicanonical sampling that overcomes both these problems. We sample, in polynomial time, networks with arbitrary degree sequences from ensembles with imposed motifs counts. Applying this method to social networks, we investigate the relation between transitivity and homophily, and we quantify the correlation between different types of motifs, finding that single motifs can explain up to 60\% of the variation of motif profiles.},
  file = {/home/marnix/Zotero/storage/XPIE586F/Fischer et al. - 2015 - Sampling Motif-Constrained Ensembles of Networks.pdf}
}

@article{Fitch1999,
  title = {Morphology and Development of the Human Vocal Tract: {{A}} Study Using Magnetic Resonance Imaging},
  shorttitle = {Morphology and Development of the Human Vocal Tract},
  author = {Fitch, W. Tecumseh and Giedd, Jay},
  year = {1999},
  month = sep,
  journal = {The Journal of the Acoustical Society of America},
  volume = {106},
  number = {3},
  pages = {1511--1522},
  issn = {0001-4966},
  doi = {10.1121/1.427148},
  urldate = {2019-12-13},
  langid = {english},
  file = {/home/marnix/Zotero/storage/PACEKVNA/Fitch and Giedd - 1999 - Morphology and development of the human vocal trac.pdf}
}

@incollection{Fitzgerald1993,
  title = {Speech {{Processing Using Bayesian Inference}}},
  booktitle = {Maximum {{Entropy}} and {{Bayesian Methods}}: {{Paris}}, {{France}}, 1992},
  author = {Fitzgerald, W. J. and Niranjan, M.},
  editor = {{Mohammad-Djafari}, Ali and Demoment, Guy},
  year = {1993},
  series = {Fundamental {{Theories}} of {{Physics}}},
  pages = {215--223},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-017-2217-9_27},
  urldate = {2019-07-24},
  abstract = {In this paper, we report on the application of Bayesian methods to the analysis of speech signals. Voiced speech can be modelled as a superposition of decaying sinusoids and estimates of the resonant frequencies, decay rates, phases, amplitudes as well as the number of model functions are calculated. The motivation for this model is that in speech analysis, the frequencies and decay rates correspond to formants and bandwidths which are perceptually significant parameters. Speech parameters are estimated by calculating the posterior probabilities for the model parameters, after various nuisance parameters have been marginalised, and it is shown how model order evidence can be calculated. Comparisons with methods such as Minimum Description Length and the Akaike Information Criteria will be made.},
  isbn = {978-94-017-2217-9},
  langid = {english},
  keywords = {Bayesian Inference,Model Order,Noise Variance,Speech Signal,Vocal Tract},
  file = {/home/marnix/Zotero/storage/4ZZ7PFMY/Fitzgerald and Niranjan - 1993 - Speech Processing Using Bayesian Inference.pdf}
}

@inproceedings{Fitzgerald1999,
  title = {Bayesian {{Methods}} in {{Signal}} and {{Image}}},
  author = {Fitzgerald, P. and Godsill, Simon J. and Kokaram, Anil C.},
  year = {1999},
  abstract = {SUMMARY In this paper, an overview of Bayesian methods and models in signal and image processing is given. The rst part of the paper reviews some traditional classes of model employed for signal processing time series analysis. Marginal inference based upon analytic integration of hyperparameters is described for these models and illustrations are given for the problem of estimating sinusoidal frequency components in white Gaussian noise and for the general changepoint problem applied to digital communications. In the second part of the paper, state of the art applications are described which employ MCMC methods for the enhancement of noise-degraded audio signals, non-linear system identiication and image sequence restoration. The complex modelling requirements and large datasets involved in these problems require sophisticated MCMC schemes employing eecient blocking schemes, model uncertainty strategies (both reversible jump and Gibbs variable selection) and non-linear/non-Gaussian models.},
  keywords = {Audio Media,Blocking (computing),Circuit restoration,Estimated,Feature selection,Image processing,Linear system,Marginal model,Nonlinear system,Normal Statistical Distribution,Requirement,Reversible-jump Markov chain Monte Carlo,Signal processing},
  file = {/home/marnix/Zotero/storage/DXF9XU84/Fitzgerald et al. - 1999 - Bayesian Methods in Signal and Image.pdf}
}

@book{Flanagan1965,
  title = {Speech {{Analysis Synthesis}} and {{Perception}}},
  author = {Flanagan, James L.},
  year = {1965},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-00849-2},
  urldate = {2021-01-07},
  isbn = {978-3-662-00851-5 978-3-662-00849-2},
  langid = {english},
  file = {/home/marnix/Zotero/storage/D5U74EHY/Flanagan - 1965 - Speech Analysis Synthesis and Perception.pdf}
}

@incollection{Flandrin1989,
  title = {Some Aspects of Non-Stationary Signal Processing with Emphasis on Time-Frequency and Time-Scale Methods},
  booktitle = {Wavelets: {{Time-frequency}} Methods and Phase Space},
  author = {Flandrin, Patrick},
  year = {1989},
  pages = {68--98},
  publisher = {Springer}
}

@article{Fokas2012,
  title = {The {{Fourier Transforms}} of the {{Chebyshev}} and {{Legendre Polynomials}}},
  author = {Fokas, A. S. and Smitheman, S. A.},
  year = {2012},
  month = nov,
  journal = {arXiv:1211.4943 [math]},
  eprint = {1211.4943},
  primaryclass = {math},
  urldate = {2021-11-04},
  abstract = {Analytic expressions for the Fourier transforms of the Chebyshev and Legendre polynomials are derived, and the latter is used to find a new representation for the half-order Bessel functions. The numerical implementation of the so-called unified method in the interior of a convex polygon provides an example of the applicability of these analytic expressions.},
  archiveprefix = {arXiv},
  keywords = {{33C45, 65N35},Mathematics - Analysis of PDEs,Mathematics - Numerical Analysis},
  file = {/home/marnix/Zotero/storage/6X8MKYHB/Fokas and Smitheman - 2012 - The Fourier Transforms of the Chebyshev and Legend.pdf;/home/marnix/Zotero/storage/C9KGB4AH/1211.html}
}

@article{Foreman-Mackey2017,
  title = {Fast and Scalable {{Gaussian}} Process Modeling with Applications to Astronomical Time Series},
  author = {{Foreman-Mackey}, Daniel and Agol, Eric and Ambikasaran, Sivaram and Angus, Ruth},
  year = {2017},
  month = dec,
  journal = {The Astronomical Journal},
  volume = {154},
  number = {6},
  eprint = {1703.09710},
  primaryclass = {astro-ph},
  pages = {220},
  issn = {0004-6256, 1538-3881},
  doi = {10.3847/1538-3881/aa9332},
  urldate = {2025-08-18},
  abstract = {The growing field of large-scale time domain astronomy requires methods for probabilistic data analysis that are computationally tractable, even with large datasets. Gaussian Processes are a popular class of models used for this purpose but, since the computational cost scales, in general, as the cube of the number of data points, their application has been limited to small datasets. In this paper, we present a novel method for Gaussian Process modeling in one-dimension where the computational requirements scale linearly with the size of the dataset. We demonstrate the method by applying it to simulated and real astronomical time series datasets. These demonstrations are examples of probabilistic inference of stellar rotation periods, asteroseismic oscillation spectra, and transiting planet parameters. The method exploits structure in the problem when the covariance function is expressed as a mixture of complex exponentials, without requiring evenly spaced observations or uniform noise. This form of covariance arises naturally when the process is a mixture of stochastically-driven damped harmonic oscillators -- providing a physical motivation for and interpretation of this choice -- but we also demonstrate that it can be a useful effective model in some other cases. We present a mathematical description of the method and compare it to existing scalable Gaussian Process methods. The method is fast and interpretable, with a range of potential applications within astronomical data analysis and beyond. We provide well-tested and documented open-source implementations of this method in C++, Python, and Julia.},
  archiveprefix = {arXiv},
  keywords = {{Physics - Data Analysis, Statistics and Probability},Astrophysics - Earth and Planetary Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics,Astrophysics - Solar and Stellar Astrophysics,Statistics - Applications},
  file = {/home/marnix/Zotero/storage/JQ6PA4RG/Foreman-Mackey et al. - 2017 - Fast and scalable Gaussian process modeling with applications to astronomical time series.pdf;/home/marnix/Zotero/storage/BYC6TSNE/1703.html}
}

@article{Forth2016,
  title = {Entraining {{IDyOT}}: {{Timing}} in the Information Dynamics of Thinking},
  author = {Forth, Jamie and Agres, Kat and Purver, Matthew and Wiggins, Geraint A.},
  year = {2016},
  journal = {Frontiers in Psychology},
  volume = {7},
  number = {OCT},
  pages = {1--19},
  issn = {16641078},
  doi = {10.3389/fpsyg.2016.01575},
  abstract = {We present a novel hypothetical account of entrainment in music and language, in context of the Information Dynamics of Thinking model, IDyOT. The extended model affords an alternative view of entrainment, and its companion term, pulse, from earlier accounts. The model is based on hiearchical, statistical prediction, modeling expectations of both what an event will be and when it will happen. As such,it constitutes a kind of predictive coding, with a particular novel hypothetical implementation. Here, we focus on the model's mechanism for predicting when a perceptual event will happen, given an existing sequence of past events, which maybe musical or linguistic. We propose a range of tests to validate or falsify the model, at various different levels of abstraction, and argue that computational modelling in particular, and this model in particular, can offer a means of providing limited but useful evidence for evolutionary hypotheses.},
  keywords = {Cognition,Cognitive modeling,Entrainment,Information dynamics,Rhythm},
  file = {/home/marnix/Zotero/storage/DDR9CRI9/Forth2016 Entraining IDyOT.pdf}
}

@article{Fowler2006,
  title = {Compensation for Coarticulation Reflects Gesture Perception, Not Spectral Contrast},
  author = {Fowler, Carol A.},
  year = {2006},
  journal = {Perception and Psychophysics},
  volume = {68},
  number = {2},
  pages = {161--177},
  issn = {00315117},
  doi = {10.3758/BF03193666},
  abstract = {This article reports three experiments designed to explore the basis for speech perceivers' apparent compensations for coarticulation. In the first experiment, the stimuli were members of three /da/-to-/ga/ continua hybridized from natural speech. The monosyllables had originally been produced in disyllables /ada/ and /aga/ to make Continuum 1, /alda/ and /alga/ (Continuum 2), and /arda/ and /arga/ (Continuum 3). Members of the second and third continua were influenced by carryover coarticulation from the preceding /l/ or /r/ context. Listeners showed compensation for this carryover coarticulation in the absence of the precursor /al/ or /ar/ syllables. This rules out an account in which compensation for coarticulation reflects a spectral contrast effect exerted by a precursor syllable, as previously has been proposed by Lotto, Holt, and colleagues (e.g., Lotto, Kluender, \& Holt, 1997; Lotto \& Kluender, 1998). The second experiment showed an enhancing effect of the endpoint monosyllables in Experiment 1 on identifications of preceding natural hybrids along an /al/-to-/ar/ continuum. That is, coarticulatory /l/ and /r/ information in /da/ and /ga/ syllables led to increased judgments of /l/ and /r/, respectively, in the precursor /al/-to-/ar/ continuum members. This was opposite to the effect, in Experiment 3, of /da/ and /ga/ syllables on preceding tones synthesized to range in frequency from approximately the ending F3 of /ar/ to the ending F3 of /al/. The enhancing, not contrastive, effect in Experiment 2, juxtaposed to the contrastive effect in Experiment 3, further disconfirms the spectral contrast account of compensation for coarticulation. A review of the literature buttresses that conclusion and provides strong support for an account that invokes listeners' attention to information in speech for the occurrence of gestural overlap.},
  isbn = {0031-5117},
  pmid = {16773890},
  file = {/home/marnix/Zotero/storage/F7RMW2SB/Fowler2006 Compensation for coarticulation reflects gesture perception, not spectral contrast.pdf}
}

@article{Fradi2024,
  title = {Reduced-Rank Spectral Mixtures {{Gaussian}} Processes for Probabilistic Time--Frequency Representations},
  author = {Fradi, Anis and Daoudi, Khalid},
  year = {2024},
  month = may,
  journal = {Signal Processing},
  volume = {218},
  pages = {109355},
  issn = {0165-1684},
  doi = {10.1016/j.sigpro.2023.109355},
  urldate = {2025-08-27},
  abstract = {Deterministic time--frequency representations are commonly used in signal processing, particularly in audio processing. Whilst presenting many potential advantages, their probabilistic counterparts are not widely used, essentially because of the computational load and the lack of clear interpretability of the different underlying models. However, using state space models, they have been shown recently to be equivalent to Spectral Mixtures Gaussian processes (SM-GP). This pioneer work unlocks this problem and opens the path for the development of tractable and interpretable probabilistic time--frequency analysis. In this paper, we propose a relatively simple yet a significant improvement of that work in terms of computational complexity, flexibility and practical application. To do so, we use a recent approach for covariance approximation to develop an algorithm for faster inference of SM-GP, while opting for a frequency-domain approach to hyperparameter learning. We illustrate the practical potential of our method using voiced speech data. We first show that key speech features can be accurately learned from the data. Second, we show that our method can yield better performances in denoising.},
  keywords = {Gaussian process,Probabilistic filter banks,Probabilistic time--frequency analysis,Reduced-rank covariances,Spectral mixtures Gaussian process},
  file = {/home/marnix/Zotero/storage/NJK9ED2M/Fradi and Daoudi - 2024 - Reduced-rank spectral mixtures Gaussian processes for probabilistic time–frequency representations.pdf;/home/marnix/Zotero/storage/EAFFAUE8/S0165168423004292.html}
}

@article{Frank2016,
  title = {Common Probability Patterns Arise from Simple Invariances},
  author = {Frank, Steven A.},
  year = {2016},
  journal = {Entropy},
  volume = {18},
  number = {5},
  eprint = {1602.03559},
  pages = {1--15},
  issn = {10994300},
  doi = {10.3390/e18050192},
  abstract = {Shift and stretch invariance lead to the exponential-Boltzmann probability distribution. Rotational invariance generates the Gaussian distribution. Particular scaling relations transform the canonical exponential and Gaussian patterns into the variety of commonly observed patterns. The scaling relations themselves arise from the fundamental invariances of shift, stretch and rotation, plus a few additional invariances. Prior work described the three fundamental invariances as a consequence of the equilibrium canonical ensemble of statistical mechanics or the Jaynesian maximization of information entropy. By contrast, I emphasize the primacy and sufficiency of invariance alone to explain the commonly observed patterns. Primary invariance naturally creates the array of commonly observed scaling relations and associated probability patterns, whereas the classical approaches derived from statistical mechanics or information theory require special assumptions to derive commonly observed scales. Keywords:},
  archiveprefix = {arXiv},
  arxivid = {1602.03559},
  keywords = {Extreme value distributions,Information theory,Maximum entropy,Measurement,Statistical mechanics},
  file = {/home/marnix/Zotero/storage/6H3MJ3IN/Frank2016 Common probability patterns arise from simple invariances.pdf}
}

@article{Frank2018,
  title = {Measurement Invariance Explains the Universal Law of Generalization for Psychological Perception},
  author = {Frank, Steven A.},
  year = {2018},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {26},
  pages = {201809787},
  issn = {0027-8424},
  doi = {10.1073/pnas.1809787115},
  abstract = {The universal law of generalization describes how animals discriminate between alternative sensory stimuli. On an appropriate perceptual scale, the probability that an organism perceives two stimuli as similar typically declines exponentially with the difference on the perceptual scale. Exceptions often follow a Gaussian probability pattern rather than an exponential pattern. Previous explanations have been based on underlying theoretical frameworks such as information theory, Kolmogorov complexity, or empirical multidimensional scaling. This article shows that the few inevitable invariances that must apply to any reasonable perceptual scale provide a sufficient explanation for the universal exponential law of generalization. In particular, reasonable measurement scales of perception must be invariant to shift by a constant value, which by itself leads to the exponential form. Similarly, reasonable measurement scales of perception must be invariant to multiplication, or stretch, by a constant value, which leads to the conservation of the slope of discrimination with perceptual difference. In some cases, an additional assumption about exchangeability or rotation of underlying perceptual dimensions leads to a Gaussian pattern of discrimination, which can be understood as a special case of the more general exponential form. The three measurement invariances of shift, stretch, and rotation provide a sufficient explanation for the universally observed patterns of perceptual generalization. All of the additional assumptions and language associated with information, complexity, and empirical scaling are superfluous with regard to the broad patterns of perception. scaling patterns {\textbar} categorization {\textbar} sensory information {\textbar} animal behavior {\textbar} probability theory T he probability that an organism perceives two stimuli as similar typically decays exponentially with separation between the stimuli. The exponential decay in perceptual similarity is often referred to as the universal law of generalization (1, 2). "Generalization" arises because perceived similarity may describe recognition of a general category. For example, two circles may have different sizes, colors, and shadings. Perceived similarity arises from the generalized perception of "circle" as a category. "Universal law" arises because many empirical observations fit the pattern for diverse sensory modalities across different species. Typical exceptions take on a Gaussian probability pattern for perceived separation (3). Both theory and empirical analysis depend on the definition of the perceptual scale. How does one translate the perceived differences between two circles with different properties into a quantitative measurement scale? There are many different suggestions in the literature for how to define a perceptual scale. Each of those suggestions develop very specific notions of measurement based, for example, on information theory, Kolmogorov complexity theory, or multidimen-sional scaling descriptions derived from observations (1, 2, 4). I focus on the minimal properties that any reasonable perceptual measurement scale must have rather than on detailed assumptions motivated by external theories of information, complexity , or empirical scaling. I express the minimal properties as simple invariances. I show that a few inevitable invariances of any reasonable perceptual scale determine the exponential form for the universal law of generalization in perception. All of the other details of information, complexity, and empirical scaling are superfluous with respect to understanding why the universal law of generalization has the exponential form. I also show that, when the separation between stimuli depends on various underlying perceptional dimensions, it sometimes makes sense to assume that the perceptual scale will also obey exchangeability or rotational invariance. When that additional invariance holds, the universal law takes on the Gaussian form, which I show to be a special case of the general exponential form. Basic Problem and Notation Chater and Vit{\'a}nyi (ref. 2, p. 346) state the law as "the probability of perceiving similarity or analogy between two items, a and b, is a negative exponential function of the distance d (a, b) between them in an internal psychological space." Let the notation P (R b {\textbar}Sa) describe the probability of a positive response, R b , to the event b, given an initial stimulus, Sa , by the event a. A positive response expresses the perceived similarity of b to a, which may also be thought of as expressing the generalization that b and a belong to the same category. The goal here is to understand how the perceived similarity of b to a, observed as R b {\textbar}Sa , translates into a continuous psychological measurement scale, T b{\textbar}a , so that P (R b {\textbar}Sa) {$\equiv$} f (T b{\textbar}a) [1] for a suitably defined mapping R b {\textbar}Sa {$\rightarrow$} T b{\textbar}a and probability distribution function, f. We seek the characteristics of the mapping and the associated function, f. Significance When an animal is presented with two stimuli, it may consider them similar or different. Similarity often expresses a generalized notion of a category, such as two circles with different sizes, shadings, and colors both being circles. In many studies, perception of similarity declines exponentially with the measure of separation, a pattern often called the universal law of generalization. This article shows that the universal exponential law can be explained by simple properties any reasonable perceptual scale must have. A shift of the scale by a constant amount, or a stretch by a constant amount, should not change the animal's ability to perceive generalities or differences. Those invariant measurement properties by themselves explain why perceived generalization follows an exponential pattern.},
  pmid = {30201714},
  keywords = {animal behavior,categorization,scaling patterns,sensory information},
  file = {/home/marnix/Zotero/storage/7KPBGS6R/Frank2018 Measurement invariance explains the universal law of generalization for psychological perception.pdf}
}

@article{Frazier2018,
  title = {A {{Tutorial}} on {{Bayesian Optimization}}},
  author = {Frazier, Peter I.},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.02811 [cs, math, stat]},
  eprint = {1807.02811},
  primaryclass = {cs, math, stat},
  urldate = {2022-05-04},
  abstract = {Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/2GW9ZYHD/Frazier - 2018 - A Tutorial on Bayesian Optimization.pdf;/home/marnix/Zotero/storage/NAV2Z6L2/1807.html}
}

@inproceedings{Freixes2018,
  title = {Influence of Tense, Modal and Lax Phonation on the Three-Dimensional Finite Element Synthesis of Vowel [{{A}}].},
  booktitle = {{{IberSPEECH}}},
  author = {Freixes, Marc and Arnela, Marc and Socor{\'o}, Joan Claudi and Pujol, Francesc Al{\'i}as and Guasch, Oriol},
  year = {2018},
  pages = {132--136},
  file = {/home/marnix/Zotero/storage/J8R3MEZC/Freixes et al. - 2018 - Influence of tense, modal and lax phonation on the.pdf}
}

@article{Freixes2023,
  title = {Evaluation of {{Glottal Inverse Filtering Techniques}} on {{OPENGLOT Synthetic Male}} and {{Female Vowels}}},
  author = {Freixes, Marc and {Joglar-Ongay}, Luis and Socor{\'o}, Joan Claudi and {Al{\'i}as-Pujol}, Francesc},
  year = {2023},
  month = jan,
  journal = {Applied Sciences},
  volume = {13},
  number = {15},
  pages = {8775},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app13158775},
  urldate = {2023-08-04},
  abstract = {Current articulatory-based three-dimensional source--filter models, which allow the production of vowels and diphtongs, still present very limited expressiveness. Glottal inverse filtering (GIF) techniques can become instrumental to identify specific characteristics of both the glottal source signal and the vocal tract transfer function to resemble expressive speech. Several GIF methods have been proposed in the literature; however, their comparison becomes difficult due to the lack of common and exhaustive experimental settings. In this work, first, a two-phase analysis methodology for the comparison of GIF techniques based on a reference dataset is introduced. Next, state-of-the-art GIF techniques based on iterative adaptive inverse filtering (IAIF) and quasi closed phase (QCP) approaches are thoroughly evaluated on OPENGLOT, an open database specifically designed to evaluate GIF, computing well-established GIF error measures after extending male vowels with their female counterparts. The results show that GIF methods obtain better results on male vowels. The QCP-based techniques significantly outperform IAIF-based methods for almost all error metrics and scenarios and are, at the same time, more stable across sex, phonation type, F0, and vowels. The IAIF variants improve the original technique for most error metrics on male vowels, while QCP with spectral tilt compensation achieves a lower spectral tilt error for male vowels than the original QCP.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {glottal inverse filtering,glottal source,OPENGLOT,performance evaluation,phonation types,speech analysis},
  file = {/home/marnix/Zotero/storage/UI8GDSQU/Freixes et al. - 2023 - Evaluation of Glottal Inverse Filtering Techniques.pdf}
}

@phdthesis{Frigola2015,
  title = {Bayesian Time Series Learning with {{Gaussian}} Processes},
  author = {Frigola, Roger},
  year = {2015},
  school = {University of Cambridge},
  file = {/home/marnix/Zotero/storage/EYJZHP5P/Frigola-Alcalde - Bayesian Time Series Learning with Gaussian Proces.pdf}
}

@article{Fu2006,
  title = {Robust {{Glottal Source Estimation Based}} on {{Joint Source-Filter Model Optimization}}},
  author = {Fu, Q. and Murphy, P.},
  year = {2006},
  month = mar,
  journal = {IEEE Transactions on Audio, Speech and Language Processing},
  volume = {14},
  number = {2},
  pages = {492--501},
  issn = {1558-7916},
  doi = {10.1109/TSA.2005.857807},
  urldate = {2023-04-05},
  abstract = {This paper describes a robust glottal source estimation method based on a joint source-filter separation technique. In this method, the Liljencrants--Fant (LF) model, which models the glottal flow derivative, is integrated into a time-varying ARX speech production model. These two models are estimated in a joint optimization procedure, in which a Kalman filtering process is embedded for adaptively identifying the vocal tract parameters. Since the formulated joint estimation problem is a multiparameter nonlinear optimization procedure, we separate the optimization procedure into two passes. The first pass initializes the glottal source and vocal tract models by solving a quasi-convex approximate optimization problem. Having robust initial values, the joint estimation procedure determines the accuracy of model estimation implemented with a trust-region descent optimization algorithm. Experiments with synthetic and real voice signals show that the proposed method is a robust glottal source parameter estimation method with a high degree of accuracy.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/9DHXN8MR/Fu and Murphy - 2006 - Robust Glottal Source Estimation Based on Joint So.pdf}
}

@inproceedings{Fujisaki1986,
  title = {Proposal and Evaluation of Models for the Glottal Source Waveform},
  booktitle = {{{ICASSP}} '86. {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Fujisaki, H. and Ljungqvist, M.},
  year = {1986},
  volume = {11},
  pages = {1605--1608},
  publisher = {{Institute of Electrical and Electronics Engineers}},
  address = {Tokyo, Japan},
  doi = {10.1109/ICASSP.1986.1169239},
  urldate = {2025-09-26},
  file = {/home/marnix/Zotero/storage/Q4SGW37U/Fujisaki and Ljungqvist - 1986 - Proposal and evaluation of models for the glottal source waveform.pdf}
}

@book{Fulop2011,
  title = {Speech Spectrum Analysis},
  author = {Fulop, Sean A},
  year = {2011},
  series = {Signals and Communication Technology},
  publisher = {Springer},
  address = {Berlin},
  isbn = {978-3-642-17477-3 978-3-642-17478-0},
  langid = {english},
  annotation = {OCLC: 746243279},
  file = {/home/marnix/Zotero/storage/7Z34R4GT/Fulop - 2011 - Speech spectrum analysis.pdf}
}

@inproceedings{Fulop2011a,
  title = {Examining the Voice Bar},
  booktitle = {Proceedings of {{Meetings}} on {{Acoustics 162ASA}}},
  author = {Fulop, Sean A and Disner, Sandra F},
  year = {2011},
  volume = {14},
  number = {1},
  pages = {060002},
  organization = {ASA},
  file = {/home/marnix/Zotero/storage/D333MEMP/Fulop and Disner - 2011 - Examining the voice bar.pdf}
}

@book{Fungacova2015,
  title = {High Liquidity Creation and Bank Failures},
  author = {Fungacova, Zuzana and Turk, Rima and Weill, Laurent},
  year = {2015},
  number = {15-103},
  publisher = {International Monetary Fund}
}

@article{Gabor1947,
  title = {Acoustical {{Quanta}} and the {{Theory}} of {{Hearing}}},
  author = {Gabor, D.},
  year = {1947},
  month = may,
  journal = {Nature},
  volume = {159},
  number = {4044},
  pages = {591--594},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/159591a0},
  urldate = {2019-04-28},
  langid = {english},
  file = {/home/marnix/Zotero/storage/LESRHEDS/Gabor - 1947 - Acoustical Quanta and the Theory of Hearing.pdf}
}

@inproceedings{Gai2010,
  title = {Contagion in Financial Networks},
  booktitle = {Proceedings of the {{Royal Society}} of {{London A}}: {{Mathematical}}, {{Physical}} and {{Engineering Sciences}}},
  author = {Gai, Prasanna and Kapadia, Sujit},
  year = {2010},
  pages = {rspa20090410},
  organization = {The Royal Society}
}

@article{Gallotti2015,
  title = {The Multilayer Temporal Network of Public Transport in {{Great Britain}}},
  author = {Gallotti, Riccardo and Barthelemy, Marc},
  year = {2015},
  month = jan,
  journal = {Scientific Data},
  volume = {2},
  eprint = {1501.02159},
  pages = {140056+},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/sdata.2014.56},
  abstract = {Despite the widespread availability of information concerning public transport coming from different sources, it is extremely hard to have a complete picture, in particular at a national scale. Here, we integrate timetable data obtained from the United Kingdom open-data program together with timetables of domestic flights, and obtain a comprehensive snapshot of the temporal characteristics of the whole \{UK\} public transport system for a week in October 2010. In order to focus on multi-modal aspects of the system, we use a coarse graining procedure and define explicitly the coupling between different transport modes such as connections at airports, ferry docks, rail, metro, coach and bus stations. The resulting weighted, directed, temporal and multilayer network is provided in simple, commonly used formats, ensuring easy access and the possibility of a straightforward use of old or specifically developed methods on this new and extensive dataset.},
  archiveprefix = {arXiv},
  arxivid = {1501.02159},
  pmid = {25977806},
  keywords = {data,dataset,multilayer},
  file = {/home/marnix/Zotero/storage/99CAX9LW/Gallotti, Barthelemy - 2015 - The multilayer temporal network of public transport in Great Britain.pdf}
}

@article{Galy-Fajou2021,
  title = {Flexible and {{Efficient Inference}} with {{Particles}} for the {{Variational Gaussian Approximation}}},
  author = {{Galy-Fajou}, Th{\'e}o and Perrone, Valerio and Opper, Manfred},
  year = {2021},
  month = aug,
  journal = {Entropy},
  volume = {23},
  number = {8},
  pages = {990},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e23080990},
  urldate = {2025-04-08},
  abstract = {Variational inference is a powerful framework, used to approximate intractable posteriors through variational distributions. The de facto standard is to rely on Gaussian variational families, which come with numerous advantages: they are easy to sample from, simple to parametrize, and many expectations are known in closed-form or readily computed by quadrature. In this paper, we view the Gaussian variational approximation problem through the lens of gradient flows. We introduce a flexible and efficient algorithm based on a linear flow leading to a particle-based approximation. We prove that, with a sufficient number of particles, our algorithm converges linearly to the exact solution for Gaussian targets, and a low-rank approximation otherwise. In addition to the theoretical analysis, we show, on a set of synthetic and real-world high-dimensional problems, that our algorithm outperforms existing methods with Gaussian targets while performing on a par with non-Gaussian targets.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Gaussian,particle flow,variable flow,variational inference},
  file = {/home/marnix/Zotero/storage/5K824DH8/Galy-Fajou et al. - 2021 - Flexible and Efficient Inference with Particles for the Variational Gaussian Approximation.pdf}
}

@misc{Gardner2021,
  title = {{{GPyTorch}}: {{Blackbox Matrix-Matrix Gaussian Process Inference}} with {{GPU Acceleration}}},
  shorttitle = {{{GPyTorch}}},
  author = {Gardner, Jacob R. and Pleiss, Geoff and Bindel, David and Weinberger, Kilian Q. and Wilson, Andrew Gordon},
  year = {2021},
  month = jun,
  number = {arXiv:1809.11165},
  eprint = {1809.11165},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-01-05},
  abstract = {Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from \$O(n{\textasciicircum}3)\$ to \$O(n{\textasciicircum}2)\$. Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/LUGBY7WM/Gardner et al. - 2021 - GPyTorch Blackbox Matrix-Matrix Gaussian Process .pdf;/home/marnix/Zotero/storage/DYPE46ST/1809.html}
}

@article{Garlaschelli2004,
  title = {Patterns of {{Link Reciprocity}} in {{Directed Networks}}},
  author = {Garlaschelli, Diego and Loffredo, Maria I},
  year = {2004},
  month = dec,
  journal = {Physical Review Letters},
  volume = {93},
  number = {26},
  publisher = {APS},
  issn = {0031-9007},
  doi = {10.1103/physrevlett.93.268701},
  abstract = {We address the problem of link reciprocity, the nonrandom presence of two mutual links between pairs of vertices. We propose a new measure of reciprocity that allows the ordering of networks according to their actual degree of correlation between mutual links. We find that real networks are always either correlated or anticorrelated, and that networks of the same type (economic, social, cellular, financial, ecological, etc.) display similar values of the reciprocity. The observed patterns are not reproduced by current models. This leads us to introduce a more general framework where mutual links occur with a conditional connection probability. In some of the studied networks we discuss the form of the conditional connection probability and the size dependence of the reciprocity.},
  keywords = {directed,network,reciprocity},
  file = {/home/marnix/Zotero/storage/CTQGUD6G/Garlaschelli, Loffredo - 2004 - Patterns of Link Reciprocity in Directed Networks.pdf}
}

@article{Garner2011,
  title = {Bayesian {{Approaches}} to {{Uncertainty}} in {{Speech Processing}}},
  author = {Garner, Philip Neil},
  year = {2011},
  journal = {Language}
}

@article{Gavalda-Miralles2014,
  title = {Impact of Heterogeneity and Socioeconomic Factors on Individual Behavior in Decentralized Sharing Ecosystems},
  author = {{Gavalda-Miralles}, A. and Choffnes, D. R. and Otto, J. S. and Sanchez, M. A. and Bustamante, F. E. and Amaral, L. A. N. and Duch, J. and Guimera, R.},
  year = {2014},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {111},
  number = {43},
  eprint = {1404.2263v1},
  pages = {15322--15327},
  issn = {0027-8424},
  doi = {10.1073/pnas.1309389111},
  abstract = {Tens of millions of individuals around the world use decentralized content distribution systems, a fact of growing social, economic, and technological importance. These sharing systems are poorly understood because, unlike in other technosocial systems, it is difficult to gather large-scale data about user behavior. Here, we investigate user activity patterns and the socioeconomic factors that could explain the behavior. Our analysis reveals that (i) the ecosystem is heterogeneous at several levels: content types are heterogeneous, users specialize in a few content types, and countries are heterogeneous in user profiles; and (ii) there is a strong correlation between socioeconomic indicators of a country and users behavior. Our findings open a research area on the dynamics of decentralized sharing ecosystems and the socioeconomic factors affecting them, and may have implications for the design of algorithms and for policymaking.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1404.2263v1},
  isbn = {0027-8424},
  pmid = {25288755},
  file = {/home/marnix/Zotero/storage/ZKLXTP7B/Gavalda-Miralles et al. - 2014 - Impact of heterogeneity and socioeconomic factors on individual behavior in decentralized sharing ecosy.pdf}
}

@book{Gelman2004,
  title = {Bayesian {{Data Analysis}}},
  author = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Rubin, Donald B},
  year = {2004},
  edition = {2},
  publisher = {Chapman \& Hall/CRC},
  isbn = {1-58488-388-X},
  file = {/home/marnix/Zotero/storage/89JQ8P9Q/Gelman et al. - 2004 - Bayesian data analysis chapman & hall.pdf}
}

@article{Geman1984,
  title = {Stochastic {{Relaxation}}, {{Gibbs Distributions}}, and the {{Bayesian Restoration}} of {{Images}}},
  author = {Geman, S. and Geman, D.},
  year = {1984},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {PAMI-6},
  number = {6},
  pages = {721--741},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.1984.4767596},
  abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
  keywords = {Additive noise,Annealing,Bayesian methods,Deformable models,Degradation,Energy states,Gibbs distribution,image restoration,Image restoration,line process,MAP estimate,Markov random field,Markov random fields,relaxation,scene modeling,spatial degradation,Stochastic processes,Temperature distribution},
  file = {/home/marnix/Zotero/storage/MRXQJABZ/Geman and Geman - 1984 - Stochastic Relaxation, Gibbs Distributions, and th.pdf;/home/marnix/Zotero/storage/MQGNB3SJ/4767596.html}
}

@book{Geroimenko2018,
  title = {Augmented Reality Art: {{From}} an Emerging Technology to a Novel Creative Medium},
  editor = {Geroimenko, Vladimir},
  year = {2018},
  series = {Springer Series on Cultural Computing},
  publisher = {Springer},
  address = {Cham, Switzerland},
  isbn = {9783030099214}
}

@article{Gershman2015,
  title = {Computational Rationality: {{A}} Converging Paradigm for Intelligence in Brains, Minds, and Machines},
  shorttitle = {Computational Rationality},
  author = {Gershman, S. J. and Horvitz, E. J. and Tenenbaum, J. B.},
  year = {2015},
  month = jul,
  journal = {Science},
  volume = {349},
  number = {6245},
  pages = {273--278},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac6076},
  urldate = {2020-04-16},
  langid = {english},
  file = {/home/marnix/Zotero/storage/6YZECYDX/Gershman et al. - 2015 - Computational rationality A converging paradigm f.pdf}
}

@article{Ghahramani2015,
  title = {Probabilistic Machine Learning and Artificial Intelligence},
  author = {Ghahramani, Zoubin},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {452--459},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14541},
  urldate = {2024-02-22},
  abstract = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.},
  copyright = {2015 Springer Nature Limited},
  langid = {english},
  keywords = {Computer science,Mathematics and computing,Neuroscience},
  file = {/home/marnix/Zotero/storage/MRE6WIWC/Ghahramani - 2015 - Probabilistic machine learning and artificial inte.pdf}
}

@article{Ghazi2010,
  title = {Why and How to Use Arbitrary Precision},
  author = {Ghazi, Kaveh R. and Lef{\`e}vre, Vincent and Th{\'e}veny, Philippe and Zimmermann, Paul},
  year = {2010},
  journal = {Computing in Science and Engineering},
  volume = {12},
  number = {3},
  pages = {62--65},
  issn = {15219615},
  doi = {10.1109/MCSE.2010.73},
  abstract = {Although double precision is usually enough, arbitrary precision increases accuracy and the reproducibility of floating-point computations.},
  isbn = {0000000000000},
  file = {/home/marnix/Zotero/storage/HG8E73AH/Why and how to use arbitrary precision.pdf}
}

@book{Ghosal2017,
  title = {Fundamentals of {{Nonparametric Bayesian Inference}}},
  author = {Ghosal, Subhashis and {van der Vaart}, Aad},
  year = {2017},
  month = jun,
  edition = {1},
  publisher = {Cambridge University Press},
  doi = {10.1017/9781139029834},
  urldate = {2023-03-30},
  abstract = {Explosive growth in computing power has made Bayesian methods for infinite-dimensional models - Bayesian nonparametrics - a nearly universal framework for inference, finding practical use in numerous subject areas. Written by leading researchers, this authoritative text draws on theoretical advances of the past twenty years to synthesize all aspects of Bayesian nonparametrics, from prior construction to computation and large sample behavior of posteriors. Because understanding the behavior of posteriors is critical to selecting priors that work, the large sample theory is developed systematically, illustrated by various examples of model and prior combinations. Precise sufficient conditions are given, with complete proofs, that ensure desirable posterior properties and behavior. Each chapter ends with historical notes and numerous exercises to deepen and consolidate the reader's understanding, making the book valuable for both graduate students and researchers in statistics and machine learning, as well as in application areas such as econometrics and biostatistics.},
  isbn = {978-0-521-87826-5 978-1-139-02983-4},
  langid = {english},
  file = {/home/marnix/Zotero/storage/MAQ34SM8/Ghosal and van der Vaart - 2017 - Fundamentals of Nonparametric Bayesian Inference.pdf}
}

@article{Giannakis1989,
  title = {Identification of Nonminimum Phase Systems Using Higher Order Statistics},
  author = {Giannakis, G. B. and Mendel, J. M.},
  year = {1989},
  month = mar,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {37},
  number = {3},
  pages = {360--377},
  issn = {0096-3518},
  doi = {10.1109/29.21704},
  abstract = {A method is presented for identification of linear, time-variant, nonminimum phase systems when only output data are available. The input sequence need not be independent, but it must be non-Gaussian, with some special properties described in the test. The authors model a finite-dimensional system as an ARMA (autoregressive moving-average) rational function of known orders, but the special cases of AR, MA, and all-pass models are also considered. To estimate the parameters of their model, the authors utilize both second- and higher-order statistics of the output, which may be contaminated by additive, zero-mean, Gaussian white noise of unknown variance. The parameter estimators obtained are proved, under mild conditions, to be consistent. Simulations verify the performance of the proposed method in the case of relatively low signal-to-noise ratios, and when there is a model-order mismatch.{$<<$}ETX{$>>$}},
  keywords = {Additive white noise,ARMA,autoregressive moving-average,Ear,filtering and prediction theory,Gaussian white noise,higher order statistics,Higher order statistics,identification,linear,Noise level,nonminimum phase systems,Parameter estimation,Phase estimation,Phase noise,Poles and zeros,spectral analysis,Strontium,time-variant,White noise},
  file = {/home/marnix/Zotero/storage/YSWP4G8M/Giannakis and Mendel - 1989 - Identification of nonminimum phase systems using h.pdf;/home/marnix/Zotero/storage/ZR7ZXS5Z/21704.html}
}

@unpublished{Gibbs1997,
  title = {Efficient Implementation of {{Gaussian}} Processes},
  author = {Gibbs, M. N. and MacKay, D. J. C.},
  year = {1997},
  address = {Department of Physics, Cavendish Laboratory, Cambridge University},
  abstract = {Neural networks and Bayesian inference provide a useful framework within which to solve regression problems. However their parameterisation means that the Bayesian analysis of neural networks can be difficult. In this paper, we investigate a method for regression using Gaussian Process Priors which allows exact Bayesian analysis using matrix manipulation for fixed values of hyperparameters. We discuss in detail the workings of the method and we detail a range of mathematical and numerical techniques that are useful in applying Gaussian Processes to general problems including efficient approximate matrix inversion methods developed by Skilling.},
  file = {/home/marnix/Zotero/storage/HYSCAX65/gpros.ps.pdf;/home/marnix/Zotero/storage/YSLZZT2J/Gibbs and MacKay - 1997 - Efficient implementation of Gaussian processes.gz}
}

@phdthesis{Gibbs1997a,
  title = {Bayesian {{Gaussian Processes}} for {{Regression}} and {{Classification}}},
  author = {Gibbs, M. N.},
  year = {1997},
  abstract = {Bayesian inference offers us a powerful tool with which to tackle the problem of data modelling. However the performance of Bayesian methods is crucially dependent on being able to find good models for our data. The principal focus of this thesis is the development of models based on Gaussian process priors. Such models, which can be thought of as the infinite extension of several existing finite models have the flexibility to model complex phenomena while being mathematically simple. In thesis, I present a review of the theory of Gaussian processes and their covariance functions and demonstrate how they fit into the Bayesian framework. The efficient implementation of a Gaussian process is discussed with particular reference to approximate methods for matrix inversion based on the work of Skilling (1993). Several regression problems are examined. Non-stationary covariance functions are developed for the regression of neuron spike data and the use of Gaussian processes to model the potential energy surfaces of weakly bound molecules is discussed. Classification methods based on Gaussian processes are implemented using variational methods. Existing bounds (Jaakkola and Jordan 1996) for the sigmoid function are used to tackle binary problems and multi-dimensional bounds on the softmax function are presented for the multiple class case. The performance of the variational classifier is compared with that of other methods using the CRABS and PIMA datasets (Ripley 1996) and the problem of predicting the cracking of welds based on their chemical composition is also investigated. The theoretical calculation of the density of states of crystal structures is discussed in detail. Three possible approaches to the problem are described based on free energy minimization, Gaussian processes and the theory of random matrices. Results from these approaches are compared with the state-of-the-art techniques (Pickard 1997).},
  school = {University of Cambridge},
  file = {/home/marnix/Zotero/storage/TZK8AETE/Gibbs - 1997 - Bayesian Gaussian Processes for Regression and Cla.ps}
}

@article{Gibson2018,
  title = {Entropy {{Power}}, {{Autoregressive Models}}, and {{Mutual Information}}},
  author = {Gibson, Jerry},
  year = {2018},
  month = oct,
  journal = {Entropy},
  volume = {20},
  number = {10},
  pages = {750},
  doi = {10.3390/e20100750},
  urldate = {2019-04-25},
  abstract = {Autoregressive processes play a major role in speech processing (linear prediction), seismic signal processing, biological signal processing, and many other applications. We consider the quantity defined by Shannon in 1948, the entropy rate power, and show that the log ratio of entropy powers equals the difference in the differential entropy of the two processes. Furthermore, we use the log ratio of entropy powers to analyze the change in mutual information as the model order is increased for autoregressive processes. We examine when we can substitute the minimum mean squared prediction error for the entropy power in the log ratio of entropy powers, thus greatly simplifying the calculations to obtain the differential entropy and the change in mutual information and therefore increasing the utility of the approach. Applications to speech processing and coding are given and potential applications to seismic signal processing, EEG classification, and ECG classification are described.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {autoregressive models,entropy rate power,mutual information},
  file = {/home/marnix/Zotero/storage/T3PS3VAH/Gibson - 2018 - Entropy Power, Autoregressive Models, and Mutual I.pdf;/home/marnix/Zotero/storage/9MRRG3SY/750.html}
}

@article{Giffin2009,
  title = {Maximum {{Entropy}}: {{The Universal Method}} for {{Inference}}},
  author = {Giffin, A},
  year = {2009},
  month = jan,
  journal = {ArXiv e-prints},
  eprint = {0901.2987},
  eprintclass = {physics.data-an},
  archiveprefix = {arXiv},
  arxivid = {physics.data-an/0901.2987},
  keywords = {Data Analysis,Physics - Data Analysis,Statistics and Probability},
  file = {/home/marnix/Zotero/storage/4XZN6IG5/Giffin - 2008 - Maximum Entropy The Universal Method for Inference.pdf}
}

@article{Giordano2018,
  title = {Covariances, {{Robustness}}, and {{Variational Bayes}}},
  author = {Giordano, Ryan and Broderick, Tamara and Jordan, Michael I},
  year = {2018},
  journal = {Journal of machine learning research},
  volume = {19},
  number = {51},
  file = {/home/marnix/Zotero/storage/QB5WYJCN/Giordano et al. - Covariances, Robustness, and Variational Bayes.pdf}
}

@misc{Girdhar2023,
  title = {{{ImageBind}}: {{One Embedding Space To Bind Them All}}},
  shorttitle = {{{ImageBind}}},
  author = {Girdhar, Rohit and {El-Nouby}, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
  year = {2023},
  month = may,
  number = {arXiv:2305.05665},
  eprint = {2305.05665},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.05665},
  urldate = {2023-10-05},
  abstract = {We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia},
  file = {/home/marnix/Zotero/storage/C2VFWKKI/Girdhar et al. - 2023 - ImageBind One Embedding Space To Bind Them All.pdf;/home/marnix/Zotero/storage/JZE3VA8F/2305.html}
}

@article{Glasserman2016,
  title = {Contagion in {{Financial Networks}}},
  author = {Glasserman, Paul and Young, H. Peyton},
  year = {2016},
  journal = {Journal of Economic Literature},
  volume = {54},
  number = {3},
  eprint = {1208.3789v2},
  pages = {779--831},
  issn = {0022-0515},
  doi = {10.1257/jel.20151228},
  abstract = {This paper develops an analytical model of contagion in financial networks with arbitrary structure. We explore how the probability and potential impact of contagion is influenced by aggregate and idiosyncratic shocks, changes in network structure, and asset market liquidity. Our findings suggest that financial systems exhibit a robust-yet-fragile tendency: while the probability of contagion may be low, the effects can be extremely widespread when problems occur. And we suggest why the resilience of the system in withstanding fairly large shocks prior to 2007 should not have been taken as a reliable guide to its future robustness.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1208.3789v2},
  isbn = {1364502114712946},
  pmid = {25246403},
  keywords = {contagion,financial crises,liquidity risk,network models,systemic risk},
  file = {/home/marnix/Zotero/storage/WRYXVRQJ/Glasserman, Young - 2016 - Contagion in Financial Networks.pdf}
}

@inproceedings{Gobl2017,
  title = {Reshaping the {{Transformed LF Model}}: {{Generating}} the {{Glottal Source}} from the {{Waveshape Parameter Rd}}},
  shorttitle = {Reshaping the {{Transformed LF Model}}},
  booktitle = {Interspeech 2017},
  author = {Gobl, Christer},
  year = {2017},
  month = aug,
  pages = {3008--3012},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2017-1140},
  urldate = {2022-04-11},
  abstract = {Precise specification of the voice source would facilitate better modelling of expressive nuances in human spoken interaction. This paper focuses on the transformed version of the widely used LF voice source model, and proposes an algorithm which makes it possible to use the waveshape parameter Rd to directly control the LF pulse, for more effective analysis and synthesis of voice modulations. The Rd parameter, capturing much of the natural covariation between glottal parameters, is central to the transformed LF model. It is used to predict the standard R-parameters, which in turn are used to synthesise the LF waveform. However, the LF pulse that results from these predictions may have an Rd value noticeably different from the specified Rd, yielding undesirable artefacts, particularly when the model is used for detailed analysis and synthesis of non-modal voice. A further limitation is that only a subset of possible Rd values can be used, to avoid conflicting LF parameter settings. To eliminate these problems, a new iterative algorithm was developed based on the NewtonRaphson method for two variables, but modified to include constraints. This ensures that the correct Rd is always obtained and that the algorithm converges for effectively all permissible Rd values.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/NYY969KX/Gobl - 2017 - Reshaping the Transformed LF Model Generating the.pdf}
}

@article{Goggans2013,
  title = {Using Nested Sampling with {{Galilean Monte Carlo}} for Model Comparison Problems in Acoustics},
  author = {Goggans, Paul and Henderson, R. W. and Xiang, Ning},
  year = {2013},
  month = may,
  journal = {Proceedings of Meetings on Acoustics},
  volume = {19},
  number = {1},
  pages = {055089},
  doi = {10.1121/1.4800876},
  urldate = {2020-03-06},
  file = {/home/marnix/Zotero/storage/KIN5G3R2/Goggans et al. - 2013 - Using nested sampling with Galilean Monte Carlo fo.pdf;/home/marnix/Zotero/storage/2L3XTQQ8/1.html}
}

@misc{Goldblum2023,
  title = {The {{No Free Lunch Theorem}}, {{Kolmogorov Complexity}}, and the {{Role}} of {{Inductive Biases}} in {{Machine Learning}}},
  author = {Goldblum, Micah and Finzi, Marc and Rowan, Keefer and Wilson, Andrew Gordon},
  year = {2023},
  month = apr,
  number = {arXiv:2304.05366},
  eprint = {2304.05366},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-04-12},
  abstract = {No free lunch theorems for supervised learning state that no learner can solve all problems or that all learners achieve exactly the same accuracy on average over a uniform distribution on learning problems. Accordingly, these theorems are often referenced in support of the notion that individual problems require specially tailored inductive biases. While virtually all uniformly sampled datasets have high complexity, real-world problems disproportionately generate low-complexity data, and we argue that neural network models share this same preference, formalized using Kolmogorov complexity. Notably, we show that architectures designed for a particular domain, such as computer vision, can compress datasets on a variety of seemingly unrelated domains. Our experiments show that pre-trained and even randomly initialized language models prefer to generate low-complexity sequences. Whereas no free lunch theorems seemingly indicate that individual problems require specialized learners, we explain how tasks that often require human intervention such as picking an appropriately sized model when labeled data is scarce or plentiful can be automated into a single learning algorithm. These observations justify the trend in deep learning of unifying seemingly disparate problems with an increasingly small set of machine learning models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/T4HGER4C/Goldblum et al. - 2023 - The No Free Lunch Theorem, Kolmogorov Complexity, .pdf;/home/marnix/Zotero/storage/MRP3UP3Z/2304.html}
}

@phdthesis{Goldstein1980,
  title = {An Articulatory Model for the Vocal Tracts of Growing Children},
  author = {Goldstein, Ursula Gisela},
  year = {1980},
  school = {Massachusetts Institute of Technology}
}

@article{Golub2003,
  title = {Separable Nonlinear Least Squares: The Variable Projection Method and Its Applications},
  shorttitle = {Separable Nonlinear Least Squares},
  author = {Golub, Gene and Pereyra, Victor},
  year = {2003},
  month = feb,
  journal = {Inverse Problems},
  volume = {19},
  number = {2},
  pages = {R1--R26},
  publisher = {IOP Publishing},
  issn = {0266-5611},
  doi = {10.1088/0266-5611/19/2/201},
  urldate = {2020-03-30},
  abstract = {In this paper we review 30 years of developments and applications of the variable projection method for solving separable nonlinear least-squares problems. These are problems for which the model function is a linear combination of nonlinear functions. Taking advantage of this special structure, the method of variable projections eliminates the linear variables obtaining a somewhat more complicated function that involves only the nonlinear parameters. This procedure not only reduces the dimension of the parameter space but also results in a better-conditioned problem. The same optimization method applied to the original and reduced problems will always converge faster for the latter. We present first a historical account of the basic theoretical work and its various computer implementations, and then report on a variety of applications from electrical engineering, medical and biological imaging, chemistry, robotics, vision, and environmental sciences. An extensive bibliography is included. The method is particularly well suited for solving real and complex exponential model fitting problems, which are pervasive in their applications and are notoriously hard to solve.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/28CL5SMN/Golub and Pereyra - 2003 - Separable nonlinear least squares the variable pr.pdf}
}

@article{Gong2011,
  title = {Exploring the Roles of Complex Networks in Linguistic Categorization},
  author = {Gong, Tao and Baronchelli, Andrea and Puglisi, Andrea and Loreto, Vittorio},
  year = {2011},
  journal = {Artificial life},
  volume = {18},
  number = {1},
  pages = {107--121},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info {\dots}},
  file = {/home/marnix/Zotero/storage/YRDSUH7Q/Gong et al. - 2011 - Exploring the Roles of Complex Networks in Linguis (1).pdf}
}

@article{Good2010a,
  title = {Performance of Modularity Maximization in Practical Contexts},
  author = {Good, Benjamin H. and De Montjoye, Yves Alexandre and Clauset, Aaron},
  year = {2010},
  journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
  volume = {81},
  number = {4},
  eprint = {0910.0165},
  pages = {1--20},
  issn = {15393755},
  doi = {10.1103/PhysRevE.81.046106},
  abstract = {Although widely used in practice, the behavior and accuracy of the popular module identification technique called modularity maximization is not well understood in practical contexts. Here, we present a broad characterization of its performance in such situations. First, we revisit and clarify the resolution limit phenomenon for modularity maximization. Second, we show that the modularity function Q exhibits extreme degeneracies: it typically admits an exponential number of distinct high-scoring solutions and typically lacks a clear global maximum. Third, we derive the limiting behavior of the maximum modularity Q\_max for one model of infinitely modular networks, showing that it depends strongly both on the size of the network and on the number of modules it contains. Finally, using three real-world metabolic networks as examples, we show that the degenerate solutions can fundamentally disagree on many, but not all, partition properties such as the composition of the largest modules and the distribution of module sizes. These results imply that the output of any modularity maximization procedure should be interpreted cautiously in scientific contexts. They also explain why many heuristics are often successful at finding high-scoring partitions in practice and why different heuristics can disagree on the modular structure of the same network. We conclude by discussing avenues for mitigating some of these behaviors, such as combining information from many degenerate solutions or using generative models.},
  archiveprefix = {arXiv},
  arxivid = {0910.0165},
  isbn = {1550-2376 (Electronic)\n1539-3755 (Linking)},
  pmid = {20481785},
  file = {/home/marnix/Zotero/storage/MZUVB65X/Good2010 The performance of modularity maximization in practical contexts.pdf}
}

@article{Gopalan2013,
  title = {Efficient Discovery of Overlapping Communities in Massive Networks},
  author = {Gopalan, Prem K. and Blei, David M.},
  year = {2013},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {110},
  number = {36},
  pages = {14534--14539},
  publisher = {National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1221839110},
  urldate = {2021-01-20},
  abstract = {Detecting overlapping communities is essential to analyzing and exploring natural networks such as social networks, biological networks, and citation networks. However, most existing approaches do not scale to the size of networks that we regularly observe in the real world. In this paper, we develop a scalable approach to community detection that discovers overlapping communities in massive real-world networks. Our approach is based on a Bayesian model of networks that allows nodes to participate in multiple communities, and a corresponding algorithm that naturally interleaves subsampling from the network and updating an estimate of its communities. We demonstrate how we can discover the hidden community structure of several real-world networks, including 3.7 million US patents, 575,000 physics articles from the arXiv preprint server, and 875,000 connected Web pages from the Internet. Furthermore, we demonstrate on large simulated networks that our algorithm accurately discovers the true community structure. This paper opens the door to using sophisticated statistical models to analyze massive networks.},
  chapter = {Physical Sciences},
  copyright = {{\copyright}  . Freely available online through the PNAS open access option.},
  langid = {english},
  pmid = {23950224},
  keywords = {Bayesian statistics,massive data,network analysis},
  file = {/home/marnix/Zotero/storage/DLLWXKS3/Gopalan and Blei - 2013 - Efficient discovery of overlapping communities in .pdf;/home/marnix/Zotero/storage/FWP97WSC/14534.html}
}

@book{Gould2010,
  title = {Statistical and Thermal Physics: With Computer Applications},
  author = {Gould, Harvey and Tobochnik, Jan},
  year = {2010},
  publisher = {Princeton University Press}
}

@article{Gowda2020,
  title = {Time-{{Varying Quasi-Closed-Phase Analysis}} for {{Accurate Formant Tracking}} in {{Speech Signals}}},
  author = {Gowda, Dhananjaya and Kadiri, Sudarsana Reddy and Story, Brad and Alku, Paavo},
  year = {2020},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {28},
  pages = {1901--1914},
  issn = {2329-9304},
  doi = {10.1109/TASLP.2020.3000037},
  abstract = {In this paper, we propose a new method for the accurate estimation and tracking of formants in speech signals using time-varying quasi-closed-phase (TVQCP) analysis. Conventional formant tracking methods typically adopt a two-stage estimateand-track strategy wherein an initial set of formant candidates are estimated using short-time analysis (e.g., 10-50 ms), followed by a tracking stage based on dynamic programming or a linear state-space model. One of the main disadvantages of these approaches is that the tracking stage, however good it may be, cannot improve upon the formant estimation accuracy of the first stage. The proposed TVQCP method provides a single-stage formant tracking that combines the estimation and tracking stages into one. TVQCP analysis combines three approaches to improve formant estimation and tracking: (1) it uses temporally weighted quasi-closed-phase analysis to derive closed-phase estimates of the vocal tract with reduced interference from the excitation source, (2) it increases the residual sparsity by using the L1 optimization and (3) it uses time-varying linear prediction analysis overlong time windows (e.g., 100-200 ms) to impose a continuity constraint on the vocal tract model and hence on the formant trajectories. Formant tracking experiments with a wide variety of synthetic and natural speech signals show that the proposed TVQCP method performs better than conventional and popular formant tracking tools, such as Wavesurfer and Praat (based on dynamic programming), the KARMA algorithm (based on Kalman filtering), and DeepFormants (based on deep neural networks trained in a supervised manner). Matlab scripts for the proposed method can be found at:},
  keywords = {Analytical models,Cost function,Estimation,formant tracking,Predictive models,quasi-closed-phase analysis,Speech processing,Time-varying linear prediction,Video recording,weighted linear prediction},
  file = {/home/marnix/Zotero/storage/9C4SVCQG/Gowda et al. - 2020 - Time-Varying Quasi-Closed-Phase Analysis for Accur.pdf;/home/marnix/Zotero/storage/EUMU3ZGN/stamp.html}
}

@article{Graf2015,
  title = {Features for Voice Activity Detection: A Comparative Analysis},
  author = {Graf, Simon and Herbig, Tobias and Buck, Markus and Schmidt, Gerhard},
  year = {2015},
  journal = {EURASIP Journal on Advances in Signal Processing},
  volume = {2015},
  number = {1},
  pages = {91},
  publisher = {EURASIP Journal on Advances in Signal Processing},
  issn = {1687-6180},
  doi = {10.1186/s13634-015-0277-z},
  keywords = {feature selection,speech detection,speech properties}
}

@book{Graham1994,
  title = {Concrete Mathematics: A Foundation for Computer Science},
  shorttitle = {Concrete Mathematics},
  author = {Graham, Ronald L. and Knuth, Donald Ervin and Patashnik, Oren},
  year = {1994},
  edition = {2nd ed},
  publisher = {Addison-Wesley},
  address = {Reading, Mass},
  isbn = {978-0-201-55802-9},
  langid = {english},
  lccn = {QA39.2 .G733 1994},
  keywords = {Computer science,Mathematics},
  file = {/home/marnix/Zotero/storage/S23DLZ2B/Graham et al. - 1994 - Concrete mathematics a foundation for computer sc.pdf}
}

@article{Graham2011,
  title = {Efficient Coding of Natural Images},
  author = {Graham, D},
  year = {2011},
  isbn = {4122685060},
  pmid = {1000198885},
  file = {/home/marnix/Zotero/storage/MIS82WD5/Lewicki Efficient Coding of Natural Sounds.pdf}
}

@article{Granziol2019,
  title = {{{MEMe}}: {{An Accurate Maximum Entropy Method}} for {{Efficient Approximations}} in {{Large-Scale Machine Learning}}},
  shorttitle = {{{MEMe}}},
  author = {Granziol, Diego and Ru, Binxin and Zohren, Stefan and Dong, Xiaowen and Osborne, Michael and Roberts, Stephen},
  year = {2019},
  month = jun,
  journal = {Entropy},
  volume = {21},
  number = {6},
  pages = {551},
  publisher = {Multidisciplinary Digital Publishing Institute},
  doi = {10.3390/e21060551},
  urldate = {2020-09-01},
  abstract = {Efficient approximation lies at the heart of large-scale machine learning problems. In this paper, we propose a novel, robust maximum entropy algorithm, which is capable of dealing with hundreds of moments and allows for computationally efficient approximations. We showcase the usefulness of the proposed method, its equivalence to constrained Bayesian variational inference and demonstrate its superiority over existing approaches in two applications, namely, fast log determinant estimation and information-theoretic Bayesian optimisation.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Bayesian optimisation,log determinant estimation,maximum entropy},
  file = {/home/marnix/Zotero/storage/BZZ9FUNQ/Granziol et al. - 2019 - MEMe An Accurate Maximum Entropy Method for Effic.pdf;/home/marnix/Zotero/storage/K57LC9CJ/htm.html}
}

@article{Green1995,
  title = {Reversible {{Jump Markov Chain Monte Carlo Computation}} and {{Bayesian Model Determination}}},
  author = {Green, Peter J.},
  year = {1995},
  journal = {Biometrika},
  volume = {82},
  number = {4},
  eprint = {2337340},
  eprinttype = {jstor},
  pages = {711--732},
  issn = {00063444},
  doi = {10.2307/2337340},
  abstract = {Markov chain Monte Carlo methods for Bayesian computation have until recently been restricted to problems where the joint distribution of all variables has a density with respect to some fixed standard underlying measure. They have therefore not been available for application to Bayesian model determination, where the dimensionality of the parameter vector is typically not fixed. This paper proposes a new framework for the construction of reversible Markov chain samplers that jump between parameter subspaces of differing dimensionality, which is flexible and entirely constructive. It should therefore have wide applicability in model determination problems. The methodology is illustrated with applications to multiple change-point analysis in one and two dimensions, and to a Bayesian comparison of binomial experiments.},
  isbn = {00063444},
  keywords = {1995,4,711-32,82,bayesian model determination,metrika,monte carlo computation and,pp,printed in great britain,reversible jump markov chain},
  file = {/home/marnix/Zotero/storage/2NEPCG4Y/Green1995 Reversible jump Monte Carlo and Bayesian Model Determination.pdf}
}

@article{Green2018,
  title = {Introduction to Finite Mixtures},
  author = {Green, Peter J.},
  year = {2018},
  month = may,
  journal = {arXiv:1705.01505 [stat]},
  eprint = {1705.01505},
  primaryclass = {stat},
  urldate = {2021-04-06},
  abstract = {Mixture models have been around for over 150 years, as an intuitively simple and practical tool for enriching the collection of probability distributions available for modelling data. In this chapter we describe the basic ideas of the subject, present several alternative representations and perspectives on these models, and discuss some of the elements of inference about the unknowns in the models. Our focus is on the simplest set-up, of finite mixture models, but we discuss also how various simplifying assumptions can be relaxed to generate the rich landscape of modelling and inference ideas traversed in the rest of this book.},
  archiveprefix = {arXiv},
  keywords = {{62G07, 62F15},Statistics - Methodology},
  file = {/home/marnix/Zotero/storage/KXU5IG6D/Green - 2018 - Introduction to finite mixtures.pdf;/home/marnix/Zotero/storage/FMZMVVED/1705.html}
}

@article{Greengard2021,
  title = {Efficient Reduced-Rank Methods for {{Gaussian}} Processes with Eigenfunction Expansions},
  author = {Greengard, Philip and O'Neil, Michael},
  year = {2021},
  month = aug,
  urldate = {2021-11-08},
  abstract = {In this work we introduce a reduced-rank algorithm for Gaussian process regression. Our numerical scheme converts a Gaussian process on a user-specified interval to its Karhunen-Lo{\textbackslash}`eve expansion, the \$L{\textasciicircum}2\$-optimal reduced-rank representation. Numerical evaluation of the Karhunen-Lo{\textbackslash}`eve expansion is performed once during precomputation and involves computing a numerical eigendecomposition of an integral operator whose kernel is the covariance function of the Gaussian process. The Karhunen-Lo{\textbackslash}`eve expansion is independent of observed data and depends only on the covariance kernel and the size of the interval on which the Gaussian process is defined. The scheme of this paper does not require translation invariance of the covariance kernel. We also introduce a class of fast algorithms for Bayesian fitting of hyperparameters, and demonstrate the performance of our algorithms with numerical experiments in one and two dimensions. Extensions to higher dimensions are mathematically straightforward but suffer from the standard curses of high dimensions.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/2SXWJVEQ/Greengard and O'Neil - 2021 - Efficient reduced-rank methods for Gaussian proces.pdf;/home/marnix/Zotero/storage/TLSASUI2/2108.html}
}

@article{Greenwood1990,
  title = {A Cochlear Frequency-Position Function for Several Species - 29 Years Later},
  author = {Greenwood, Donald D.},
  year = {1990},
  month = jun,
  journal = {The Journal of the Acoustical Society of America},
  volume = {87},
  number = {6},
  pages = {2592--2605},
  issn = {0001-4966},
  doi = {10.1121/1.399052},
  file = {/home/marnix/Zotero/storage/S89ASD5B/Greenwood - 1990 - A cochlear frequency‐position function for several species—29 years later.pdf}
}

@article{Greenwood1991,
  title = {Critical Bandwidth and Consonance in Relation to Cochlear Frequency-Position Coordinates.},
  author = {Greenwood, Donald D.},
  year = {1991},
  month = aug,
  journal = {Hearing research},
  volume = {54},
  number = {2},
  eprint = {1938625},
  eprinttype = {pubmed},
  pages = {164--208},
  issn = {0378-5955},
  abstract = {A recent paper (Greenwood, 1990) has reviewed some of the data in the literature on the frequency-position coordinates of the cochlear partition in a number of species and the degree to which they are fitted by empirical functions developed in 1961 (Greenwood, 1961b, 1974b). Continued confirmation by physiological data makes this frequency-position function more independent of non-physiological data and provides a more secure means of testing possible relations of psychoacoustic data to cochlear coordinates. The present paper reviews various sets of critical band, or similar, data in humans and other species and finds that a considerable body of bandwidth estimates correspond to equal distances along the cochlear partition (on this assumption), confirming also to an exponential function of distance. As shown in 1961, such a function would imply that the same set of bandwidths is also a linear function of frequency. Some of the early critical bandwidth, and also 'consonant interval', estimates in man correspond to equal distances on the cochlear partition to a degree not generally recognized. Thus above about 300 to 500 Hz most of the critical band data (of Zwicker and G{\"a}ssler collated by Zwicker et al., 1957), correspond quite well to equal distances on the B{\'e}k{\'e}sy-Skarstein cochlear map fitted by the frequency-position function, as opposed to the values published in the critical band table or curve (which do not do so above 3 kHz). Consonant interval data tend to correspond closely to equal distances, from below 100 Hz to about 3 kHz. Certain post-1961 'critical band' (ERB) estimates collated by Moore and Glasberg (1983) and extended by Moore et al. (1990) and Shailer et al. (1990) also correspond quite closely to constant distances calculated by the 1961 function. So too do some, but not all, of the frequency intervals shown by Plomp (1964) and Plomp and Mimpen (1968) to be required to resolve the components of a harmonic complex. Some critical bandwidth data from animal studies may also correspond approximately to equal distances. This survey of old and new results, plotted on a rational distance scale, may assist in explaining what potential mix of factors operates to determine the estimated bandwidths when the values differ across experiments or in different frequency ranges. The correspondence, in the preponderance of cases, of critical bandwidth to a constant distance may facilitate an understanding of the operational definitions of critical bandwidth in different experiments and of the common underlying mechanisms.(ABSTRACT TRUNCATED AT 400 WORDS)},
  pmid = {1938625}
}

@article{Greenwood1997,
  title = {The {{Mel Scale}}'s Disqualifying Bias and a Consistency of Pitch-Difference Equisections in 1956 with Equal Cochlear Distances and Equal Frequency Ratios},
  author = {Greenwood, Donald D.},
  year = {1997},
  month = jan,
  journal = {Hearing Research},
  volume = {103},
  number = {1-2},
  eprint = {9007585},
  eprinttype = {pubmed},
  pages = {199--224},
  publisher = {Elsevier Science B.V.},
  issn = {03785955},
  doi = {10.1016/S0378-5955(96)00175-X},
  abstract = {In 1956, Stevens 'commissioned' an experiment to equisect a pitch difference between two tones. Results appear to reveal a methodological flaw that would invalidate the Mel Scale (Stevens and Volkmann, 1940). Stevens sought to distinguish sensory continua, e.g., loudness and pitch, on various criteria. He expected that the pitch continuum would not exhibit 'hysteresis'; i.e., that subjects dividing a pitch difference ({$\Delta$}f) into equal-appearing parts would not set dividing frequencies higher when listening to notes in ascending order than in descending order. Seven subjects equisected a pitch difference, between tones of 400 and 7000 Hz, into equal-seeming parts by adjusting the frequencies of three intermediate tones. All seven exhibited hysteresis, contrary to expectation. This outcome bears on other issues. Years prior, Stevens suggested that equal pitch differences might correspond to equal cochlear distances, but not to equal frequency ratios nor to equal musical intervals (Stevens and Davis, 1938; Stevens and Volkmann, 1940). In 1960 (reported now), both the 1940 Mel Scale and the equal pitch differences of 1956 were compared to equal cochlear distances, using a frequency-position function that fitted Bekesy's cochlear map (Greenwood, 1961, 1990). When ascending and descending settings were combined to contra-pose biases, equal pitch differences did coincide with equal distances - which the Mel Scale did not. Further, the biased ascending-order data coincided with the Mel Scale, suggesting the Mel Scale was similarly biased. Thus, the combined-order equal pitch differences of 1956 - but not the Mel Scale - are consistent with equal cochlear distances. However, since the map between 400 and 7000 Hz is nearly logarithmic, equal frequency ratios also approximate equal distances. Ironically, above 400 Hz, Bekesy's map and Stevens' equal-distance hypothesis jointly imply that musical intervals will nearly agree with equal pitch differences, which Stevens thought he had disconfirmed. However, given Bekesy's map, only near the cochlear apex will equal distances not approximate equal frequency ratios; and Pratt's (Pratt, 1928) bisections of {$\Delta$}fs greater than an octave indicated that equal pitch differences, on average, did agree with equal distances. However, they did so for only two of four subjects and coincided instead with qual frequency ratios for one musical subject. Historical distinctions suggest that between the parts of equisected {$\Delta$}fs subjective equivalence may be of two kinds - one linked to musical intervals, leading to equal frequency ratios; a second linked to 'tone-height' and 'distance', leading to deviations from equal frequency ratios near the apex, though not appreciably if equisected {$\Delta$}fs are less than an octave (or if perhaps subjects are musicians). Data of other kinds suggest that, if pure-tone pitch height were a function of place, the place could be the apical excitation-pattern edge, in any case not a maximum, which in neural data shifts and disappears with tone level.},
  pmid = {9007585},
  keywords = {cochlear map,critical bandwidth,Mel Scale,musical interval,pitch},
  file = {/home/marnix/Zotero/storage/ZAXR3ZCG/Greenwood1996 The Mel Scale's disqualifying bias.pdf}
}

@article{Gregory1992,
  title = {A {{New Method}} for the {{Detection}} of a {{Periodic Signal}}},
  author = {{Gregory}},
  year = {1992},
  journal = {New York},
  number = {January},
  file = {/home/marnix/Zotero/storage/XMD35J7P/Gregory1992 A new method for the detection of a periodic signal of unknown shape and period.pdf}
}

@article{Gregory2001,
  title = {A {{Bayesian}} Revolution in Spectral Analysis},
  author = {Gregory, P. C.},
  year = {2001},
  journal = {AIP Conference Proceedings},
  volume = {568},
  pages = {557--568},
  issn = {0094243X},
  doi = {10.1063/1.1381917},
  abstract = {The discrete Fourier transforms (DFT) is ubiquitous in spectral analysis as a result of the introduction of the Fast Fourier transform by Cooley and Tukey in 1965. In 1987, E. T. Jaynes derived the DFT using Bayesian Probability Theory and provided surprising new insights into its role in spectral analysis. From this new perspective the spectral resolution achievable is directly dependent on the signal to noise ratio and can be orders of magnitude better than that of a conventional Fourier power spectrum or periodogram. This was the starting point for an ongoing Bayesian revolution in spectral analysis which is reviewed in this paper, with examples taken from physics and astronomy. The revolution is based on a viewpoint of Bayesian Inference as extended logic.},
  isbn = {0735400040},
  file = {/home/marnix/Zotero/storage/ERJ4MDBD/Gregory2001 A Bayesian revolution in spectral analysis.pdf}
}

@book{Gregory2005,
  title = {Bayesian {{Logical Data Analysis}} for the {{Physical Sciences}}: {{A Comparative Approach}} with {{Mathematica}}{\textregistered} {{Support}}},
  shorttitle = {Bayesian {{Logical Data Analysis}} for the {{Physical Sciences}}},
  author = {Gregory, Phil},
  year = {2005},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9780511791277},
  urldate = {2021-04-06},
  abstract = {Bayesian inference provides a simple and unified approach to data analysis, allowing experimenters to assign probabilities to competing hypotheses of interest, on the basis of the current state of knowledge. By incorporating relevant prior information, it can sometimes improve model parameter estimates by many orders of magnitude. This book provides a clear exposition of the underlying concepts with many worked examples and problem sets. It also discusses implementation, including an introduction to Markov chain Monte-Carlo integration and linear and nonlinear model fitting. Particularly extensive coverage of spectral analysis (detecting and measuring periodic signals) includes a self-contained introduction to Fourier and discrete Fourier methods. There is a chapter devoted to Bayesian inference with Poisson sampling, and three chapters on frequentist methods help to bridge the gap between the frequentist and Bayesian approaches. Supporting Mathematica{\textregistered} notebooks with solutions to selected problems, additional worked examples, and a Mathematica tutorial are available at www.cambridge.org/9780521150125.},
  isbn = {978-0-521-84150-4},
  file = {/home/marnix/Zotero/storage/TQ5RSD2S/Gregory - 2005 - Bayesian Logical Data Analysis for the Physical Sc.pdf;/home/marnix/Zotero/storage/5MR9GR4Z/09E9A95DAE275F5B005676C71B542598.html}
}

@misc{Gribel2020,
  title = {Assortative-{{Constrained Stochastic Block Models}}},
  author = {Gribel, Daniel and Vidal, Thibaut and Gendreau, Michel},
  year = {2020},
  month = apr,
  number = {arXiv:2004.11890},
  eprint = {2004.11890},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.11890},
  urldate = {2025-07-22},
  abstract = {Stochastic block models (SBMs) are often used to find assortative community structures in networks, such that the probability of connections within communities is higher than in between communities. However, classic SBMs are not limited to assortative structures. In this study, we discuss the implications of this model-inherent indifference towards assortativity or disassortativity, and show that this characteristic can lead to undesirable outcomes for networks which are presupposedy assortative but which contain a reduced amount of information. To circumvent this issue, we introduce a constrained SBM that imposes strong assortativity constraints, along with efficient algorithmic approaches to solve it. These constraints significantly boost community recovery capabilities in regimes that are close to the information-theoretic threshold. They also permit to identify structurally-different communities in networks representing cerebral-cortex activity regions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/HG9UG5AM/Gribel et al. - 2020 - Assortative-Constrained Stochastic Block Models.pdf;/home/marnix/Zotero/storage/WBF8IZ8F/2004.html}
}

@article{Grossberg1997,
  title = {Neural Dynamics of Variable-Rate Speech Categorization.},
  author = {Grossberg, Stephen and Boardman, Ian and Cohen, Michael},
  year = {1997},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {23},
  number = {2},
  pages = {481--503},
  issn = {0096-1523},
  doi = {10.1037/0096-1523.23.2.481},
  abstract = {What is the neural representation of a speech code as it evolves in time? A neural model simulates data concerning segregation and integration of phonetic percepts. Hearing two phonetically related stops in a VC-CV pair (V = vowel; C = consonant) requires 150 ms more closure time than hearing two phonetically different stops in a VC1-C2V pair. Closure time also varies with long-term stimulus rate. The model simulates rate-dependent category boundaries that emerge from feedback interactions between a working memory for short-term storage of phonetic items and a list categorization network for grouping sequences of items. The conscious speech code is a resonant wave. It emerges after bottom-up signals from the working memory select list chunks which read out top-down expectations that amplify and focus attention on consistent working memory items. In VC1-C2V pairs, resonance is reset by mismatch of C2 with the C1 expectation. In VC-CV pairs, resonance prolongs a repeated C.},
  isbn = {0096-1523\n1939-1277},
  pmid = {9104006},
  file = {/home/marnix/Zotero/storage/878NYCRM/Grossberg Speech Categorization.pdf}
}

@article{Grossberg2000,
  title = {The Resonant Dynamics of Speech Perception: Interword Integration and Duration-Dependent Backward Effects.},
  author = {Grossberg, S and Myers, C W},
  year = {2000},
  journal = {Psychological review},
  volume = {107},
  number = {4},
  eprint = {1011.1669v3},
  pages = {735--767},
  issn = {0033-295X},
  doi = {10.1037/0033-295X.107.4.735},
  abstract = {How do listeners integrate temporally distributed phonemic information into coherent representations of syllables and words? For example, increasing the silence interval between the words "gray chip" may result in the percept "great chip," whereas increasing the duration of fricative noise in "chip" may alter the percept to "great ship" (B. H. Repp, A. M. Liberman, T. Eccardt, \& D. Pesetsky, 1978). The ARTWORD neural model quantitatively simulates such context-sensitive speech data. In ARTWORD, sequentially stored phonemic items in working memory provide bottom-up input to unitized list chunks that group together sequences of items of variable length. The list chunks compete with each other. The winning groupings feed back to establish a resonance which temporarily boosts the activation levels of selected items and chunks, thereby creating an emergent conscious percept whose properties match such data.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1011.1669v3},
  isbn = {0033-295X\n1939-1471},
  pmid = {11089405},
  file = {/home/marnix/Zotero/storage/C4MFGFKY/Grossberg Speech Perception.pdf}
}

@article{Grossberg2004,
  title = {{{ARTSTREAM}}: A Neural Network Model of Auditory Scene Analysis and Source Segregation.},
  author = {Grossberg, Stephen and Govindarajan, Krishna K and Wyse, Lonce L and a Cohen, Michael},
  year = {2004},
  journal = {Neural networks : the official journal of the International Neural Network Society},
  volume = {17},
  number = {4},
  eprint = {15109681},
  eprinttype = {pubmed},
  pages = {511--36},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2003.10.002},
  abstract = {Multiple sound sources often contain harmonics that overlap and may be degraded by environmental noise. The auditory system is capable of teasing apart these sources into distinct mental objects, or streams. Such an 'auditory scene analysis' enables the brain to solve the cocktail party problem. A neural network model of auditory scene analysis, called the ARTSTREAM model, is presented to propose how the brain accomplishes this feat. The model clarifies how the frequency components that correspond to a given acoustic source may be coherently grouped together into a distinct stream based on pitch and spatial location cues. The model also clarifies how multiple streams may be distinguished and separated by the brain. Streams are formed as spectral-pitch resonances that emerge through feedback interactions between frequency-specific spectral representations of a sound source and its pitch. First, the model transforms a sound into a spatial pattern of frequency-specific activation across a spectral stream layer. The sound has multiple parallel representations at this layer. A sound's spectral representation activates a bottom-up filter that is sensitive to the harmonics of the sound's pitch. This filter activates a pitch category which, in turn, activates a top-down expectation that is also sensitive to the harmonics of the pitch. Resonance develops when the spectral and pitch representations mutually reinforce one another. Resonance provides the coherence that allows one voice or instrument to be tracked through a noisy multiple source environment. Spectral components are suppressed if they do not match harmonics of the top-down expectation that is read-out by the selected pitch, thereby allowing another stream to capture these components, as in the 'old-plus-new heuristic' of Bregman. Multiple simultaneously occurring spectral-pitch resonances can hereby emerge. These resonance and matching mechanisms are specialized versions of Adaptive Resonance Theory, or ART, which clarifies how pitch representations can self-organize during learning of harmonic bottom-up filters and top-down expectations. The model also clarifies how spatial location cues can help to disambiguate two sources with similar spectral cues. Data are simulated from psychophysical grouping experiments, such as how a tone sweeping upwards in frequency creates a bounce percept by grouping with a downward sweeping tone due to proximity in frequency, even if noise replaces the tones at their intersection point. Illusory auditory percepts are also simulated, such as the auditory continuity illusion of a tone continuing through a noise burst even if the tone is not present during the noise, and the scale illusion of Deutsch whereby downward and upward scales presented alternately to the two ears are regrouped based on frequency proximity, leading to a bounce percept. Since related sorts of resonances have been used to quantitatively simulate psychophysical data about speech perception, the model strengthens the hypothesis that ART-like mechanisms are used at multiple levels of the auditory system. Proposals for developing the model to explain more complex streaming data are also provided.},
  pmid = {15109681},
  keywords = {Acoustic Stimulation,Algorithms,Auditory Pathways,Auditory Pathways: physiology,Auditory Perception,Auditory Perception: physiology,Cues,Functional Laterality,Humans,Neural Networks (Computer),Noise,Pitch Discrimination,Pitch Discrimination: physiology,Sound Spectrography,Sound Spectrography: methods,Speech Acoustics},
  file = {/home/marnix/Zotero/storage/9ZBD5JLN/Grossberg ARTSTREAM speech neural network.pdf}
}

@article{Grozier2020,
  title = {Should Physical Laws Be Unit-Invariant?},
  author = {Grozier, Jim},
  year = {2020},
  month = apr,
  journal = {Studies in History and Philosophy of Science},
  volume = {80},
  pages = {9--18},
  issn = {0039-3681},
  doi = {10.1016/j.shpsa.2018.12.009},
  langid = {english},
  pmid = {32383677},
  file = {/home/marnix/Zotero/storage/BQGNUJK3/Grozier - 2020 - Should physical laws be unit-invariant.pdf}
}

@article{Grund2012,
  title = {Ethnic Heterogeneity in the Activity and Structure of a {{Black}} Street Gang},
  author = {Grund, Thomas U and Densley, James A},
  year = {2012},
  journal = {European Journal of Criminology},
  volume = {9},
  number = {4},
  pages = {388--406},
  publisher = {SAGE Publications Sage UK: London, England}
}

@inproceedings{Guarnizo2018,
  title = {Impulse {{Response Estimation}} of {{Linear Time-Invariant Systems Using Convolved Gaussian Processes}} and {{Laguerre Functions}}},
  booktitle = {Progress in {{Pattern Recognition}}, {{Image Analysis}}, {{Computer Vision}}, and {{Applications}}},
  author = {Guarnizo, Cristian and {\'A}lvarez, Mauricio A.},
  editor = {Mendoza, Marcelo and Velast{\'i}n, Sergio},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {281--288},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-75193-1_34},
  abstract = {This paper presents a novel method to estimate the impulse response function of Linear Time-Invariant systems from input-output data by means of Laguerre functions and Convolved Gaussian Processes. We define a new non-stationary covariance function that encodes the convolution between the Laguerre functions and the input. The input (excitation) is modelled by a Gaussian Process prior. Thus, we are able to estimate the system's impulse response by performing maximum likelihood estimation over the model hyperparameters. Besides, the proposed model performs well in missing and noisy data scenarios.},
  isbn = {978-3-319-75193-1},
  langid = {english},
  keywords = {Convolved Gaussian Process,Impulse response function,Laguerre function},
  file = {/home/marnix/Zotero/storage/CG3BDEMA/Impulse Response Estimation of Linear Time-Invariant Systems Using Convolved Gaussian Processes and Laguerre Functions.pdf}
}

@article{Guleva2015,
  title = {Using Multiplex Networks for Banking Systems Dynamics Modelling},
  author = {Guleva, Valentina Y and Skvorcova, Maria V and Boukhanovsky, Alexander V},
  year = {2015},
  journal = {Procedia Computer Science},
  volume = {66},
  pages = {257--266},
  publisher = {Elsevier},
  file = {/home/marnix/Zotero/storage/8W557MXN/Guleva, Skvorcova, Boukhanovsky - 2015 - Using multiplex networks for banking systems dynamics modelling.pdf}
}

@article{Gull1978,
  title = {Image Reconstruction from Incomplete and Noisy Data},
  author = {Gull, S. F. and Daniell, Geoffrey J.},
  year = {1978},
  journal = {Nature},
  volume = {272},
  pages = {686--690},
  file = {/home/marnix/Zotero/storage/6UYWXKAE/Gull Daniell1978 Image reconstruction from incomplete and noisy data.pdf}
}

@incollection{Gull1988,
  title = {Bayesian Inductive Inference and Maximum Entropy},
  booktitle = {Maximum-Entropy and {{Bayesian}} Methods in Science and Engineering},
  author = {Gull, Stephen F},
  year = {1988},
  pages = {53--74},
  publisher = {Springer},
  file = {/home/marnix/Zotero/storage/49SF3UQD/Gull - 1988 - Bayesian inductive inference and maximum entropy.pdf}
}

@incollection{Gull1989,
  title = {Bayesian Data Analysis: {{Straight-line}} Fitting},
  booktitle = {Maximum Entropy and {{Bayesian}} Methods},
  author = {Gull, Stephen F},
  year = {1989},
  pages = {511--518},
  publisher = {Springer},
  file = {/home/marnix/Zotero/storage/AK4P5H5K/Gull - 1989 - Bayesian data analysis Straight-line fitting.pdf}
}

@misc{Gundersen2021,
  title = {Active Multi-Fidelity {{Bayesian}} Online Changepoint Detection},
  author = {Gundersen, Gregory W. and Cai, Diana and Zhou, Chuteng and Engelhardt, Barbara E. and Adams, Ryan P.},
  year = {2021},
  month = jul,
  number = {arXiv:2103.14224},
  eprint = {2103.14224},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.14224},
  urldate = {2023-11-02},
  abstract = {Online algorithms for detecting changepoints, or abrupt shifts in the behavior of a time series, are often deployed with limited resources, e.g., to edge computing settings such as mobile phones or industrial sensors. In these scenarios it may be beneficial to trade the cost of collecting an environmental measurement against the quality or "fidelity" of this measurement and how the measurement affects changepoint estimation. For instance, one might decide between inertial measurements or GPS to determine changepoints for motion. A Bayesian approach to changepoint detection is particularly appealing because we can represent our posterior uncertainty about changepoints and make active, cost-sensitive decisions about data fidelity to reduce this posterior uncertainty. Moreover, the total cost could be dramatically lowered through active fidelity switching, while remaining robust to changes in data distribution. We propose a multi-fidelity approach that makes cost-sensitive decisions about which data fidelity to collect based on maximizing information gain with respect to changepoints. We evaluate this framework on synthetic, video, and audio data and show that this information-based approach results in accurate predictions while reducing total cost.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/R7H57XJM/Gundersen et al. - 2021 - Active multi-fidelity Bayesian online changepoint .pdf;/home/marnix/Zotero/storage/R2EYRC8I/2103.html}
}

@article{Guttorp1988,
  title = {Finding the {{Location}} of a {{Signal}}: {{A Bayesian Analysis}}},
  shorttitle = {Finding the {{Location}} of a {{Signal}}},
  author = {Guttorp, Peter and Lockhart, Richard A.},
  year = {1988},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {83},
  number = {402},
  pages = {322--330},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1988.10478601},
  urldate = {2023-11-08},
  langid = {english},
  file = {/home/marnix/Zotero/storage/4PBF7PPU/Guttorp and Lockhart - 1988 - Finding the Location of a Signal A Bayesian Analy.pdf}
}

@article{Hackett2016,
  title = {Bond {{Percolation}} on {{Multiplex Networks}}},
  author = {Hackett, A and Cellai, D and G{\'o}mez, S and Arenas, A and Gleeson, J{\~.}P.},
  year = {2016},
  month = apr,
  journal = {Physical Review X},
  volume = {6},
  number = {2},
  eprint = {1509.09132},
  eprintclass = {physics.soc-ph},
  pages = {21002},
  doi = {10.1103/PhysRevX.6.021002},
  archiveprefix = {arXiv},
  arxivid = {physics.soc-ph/1509.09132},
  file = {/home/marnix/Zotero/storage/MZC8GZPE/Hackett et al. - 2016 - Bond Percolation on Multiplex Networks.pdf}
}

@article{Halaj2013,
  title = {Assessing Interbank Contagion Using Simulated Networks},
  author = {Ha{\l}aj, Grzegorz and Kok, Christoffer},
  year = {2013},
  journal = {Computational Management Science},
  volume = {10},
  number = {2-3},
  pages = {157--186},
  issn = {1619697X},
  doi = {10.1007/s10287-013-0168-4},
  abstract = {This paper presents a new approach to randomly generate interbank networks{\textbackslash}nwhile overcoming shortcomings in the availability of bank-by-bank bilateral{\textbackslash}nexposures. Our model can be used to simulate and assess interbank contagion effects{\textbackslash}non banking sector soundness and resilience. We find a strongly non-linear pattern{\textbackslash}nacross the distribution of simulated networks, whereby only for a small percentage of{\textbackslash}nnetworks the impact of interbank contagion will substantially reduce average solvency{\textbackslash}nof the system. In the vast majority of the simulated networks the system-wide contagion{\textbackslash}neffects are largely negligible. The approach furthermore enables to form a view{\textbackslash}nabout themost systemic banks in the system in terms of the banks whose failure would{\textbackslash}nhave the most detrimental contagion effects on the system as a whole. Finally, as the{\textbackslash}nsimulation of the network structures is computationally very costly, we also propose{\textbackslash}na simplified measure---a so-called Systemic Probability Index---that also captures the{\textbackslash}nlikelihood of contagion from the failure of a given bank to honour its interbank payment{\textbackslash}nobligations but at the same time is less costly to compute. We find that the SPI{\textbackslash}nis broadly consistent with the results from the simulated network structures.},
  isbn = {1506},
  keywords = {Banking,Interbank contagion,Network theory,Stress-testing,Systemic risk},
  file = {/home/marnix/Zotero/storage/JQHJQBZ4/ecbwp1506.pdf}
}

@incollection{Haldane2013,
  title = {Rethinking the Financial Network},
  booktitle = {Fragile Stabilit{\"a}t--Stabile Fragilit{\"a}t},
  author = {Haldane, Andrew G},
  year = {2013},
  pages = {243--278},
  publisher = {Springer}
}

@article{Hale2014,
  title = {An {{Algorithm}} for the {{Convolution}} of {{Legendre Series}}},
  author = {Hale, Nicholas and Townsend, Alex},
  year = {2014},
  month = jan,
  journal = {SIAM Journal on Scientific Computing},
  volume = {36},
  number = {3},
  pages = {A1207-A1220},
  issn = {1064-8275, 1095-7197},
  doi = {10.1137/140955835},
  urldate = {2021-11-24},
  abstract = {An O(N 2) algorithm for the convolution of compactly supported Legendre series is described. The algorithm is derived from the convolution theorem for Legendre polynomials and the recurrence relation satisfied by spherical Bessel functions. Combining with previous work yields an O(N 2) algorithm for the convolution of Chebyshev series. Numerical results are presented to demonstrate the improved efficiency over the existing algorithm.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/Y8THDCRY/Hale and Townsend - 2014 - An Algorithm for the Convolution of Legendre Serie.pdf}
}

@article{Hamming1980,
  title = {The {{Unreasonable Effectiveness}} of {{Mathematics}}},
  author = {Hamming, R. W.},
  year = {1980},
  journal = {The American Mathematical Monthly},
  volume = {87},
  number = {2},
  eprint = {2321982},
  eprinttype = {jstor},
  pages = {81--90},
  publisher = {Mathematical Association of America},
  issn = {0002-9890},
  doi = {10.2307/2321982},
  urldate = {2020-08-21},
  file = {/home/marnix/Zotero/storage/2P6MZPXX/Hamming - 1980 - The Unreasonable Effectiveness of Mathematics.pdf}
}

@article{Han2012,
  title = {Graph Characterizations from von {{Neumann}} Entropy},
  author = {Han, Lin and Escolano, Francisco and Hancock, Edwin R and Wilson, Richard C},
  year = {2012},
  journal = {Pattern Recognition Letters},
  volume = {33},
  number = {15},
  pages = {1958--1967},
  publisher = {North-Holland}
}

@article{Han2017,
  title = {The {{CAPIO}} 2017 {{Conversational Speech Recognition System}}},
  author = {Han, Kyu J. and Chandrashekaran, Akshay and Kim, Jungsuk and Lane, Ian},
  year = {2017},
  eprint = {1801.00059},
  pages = {1--8},
  abstract = {In this paper we show how we have achieved the state-of-the-art performance on the industry-standard NIST 2000 Hub5 English evaluation set. We explore densely connected LSTMs, inspired by the densely connected convolutional networks recently introduced for image classification tasks. We also propose an acoustic model adaptation scheme that simply averages the parameters of a seed neural network acoustic model and its adapted version. This method was applied with the CallHome training corpus and improved individual system performances by on average 6.1\% (relative) against the CallHome portion of the evaluation set with no performance loss on the Switchboard portion. With RNN-LM rescoring and lattice combination on the 5 systems trained across three different phone sets, our 2017 speech recognition system has obtained 5.0\% and 9.1\% on Switchboard and CallHome, respectively, both of which are the best word error rates reported thus far. According to IBM in their latest work to compare human and machine transcriptions, our reported Switchboard word error rate can be considered to surpass the human parity (5.1\%) of transcribing conversational telephone speech.},
  archiveprefix = {arXiv},
  arxivid = {1801.00059},
  file = {/home/marnix/Zotero/storage/KEXM27VV/Han2017 he CAPIO 2017 Conversational Speech Recognition System.pdf}
}

@article{Handley2015,
  title = {{{POLYCHORD}}: Nested Sampling for Cosmology},
  shorttitle = {{{POLYCHORD}}},
  author = {Handley, W. J. and Hobson, M. P. and Lasenby, A. N.},
  year = {2015},
  month = jun,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {450},
  pages = {L61-L65},
  issn = {0035-8711},
  doi = {10.1093/mnrasl/slv047},
  urldate = {2021-04-15},
  abstract = {POLYCHORD is a novel nested sampling algorithm tailored for high-dimensional parameter spaces. In addition, it can fully exploit a hierarchy of parameter speeds such as is found in COSMOMC and CAMB. It utilizes slice sampling at each iteration to sample within the hard likelihood constraint of nested sampling. It can identify and evolve separate modes of a posterior semi-independently and is parallelized using OPENMPI. POLYCHORD is available for download at http://ccpforge.cse.rl.ac.uk/gf/project/polychord/.},
  keywords = {methods: data analysis,methods: statistical},
  file = {/home/marnix/Zotero/storage/3L2LBRAT/Handley et al. - 2015 - POLYCHORD nested sampling for cosmology.pdf}
}

@article{Handley2015a,
  title = {{{POLYCHORD}}: Next-Generation Nested Sampling},
  shorttitle = {{{POLYCHORD}}},
  author = {Handley, W. J. and Hobson, M. P. and Lasenby, A. N.},
  year = {2015},
  month = nov,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {453},
  pages = {4384--4398},
  issn = {0035-8711},
  doi = {10.1093/mnras/stv1911},
  urldate = {2021-04-15},
  abstract = {POLYCHORD is a novel nested sampling algorithm tailored for high-dimensional parameter spaces. This paper coincides with the release of POLYCHORD v1.6, and provides an extensive account of the algorithm. POLYCHORD utilizes slice sampling at each iteration to sample within the hard likelihood constraint of nested sampling. It can identify and evolve separate modes of a posterior semi-independently, and is parallelized using OPENMPI. It is capable of exploiting a hierarchy of parameter speeds such as those present in COSMOMC and CAMB, and is now in use in the COSMOCHORD and MODECHORD codes. POLYCHORD is available for download from http://ccpforge.cse.rl.ac.uk/gf/project/polychord/.},
  keywords = {methods: data analysis,methods: statistical},
  file = {/home/marnix/Zotero/storage/7BT83WCX/Handley et al. - 2015 - POLYCHORD next-generation nested sampling.pdf}
}

@article{Hanna2016,
  title = {Frequencies, Bandwidths and Magnitudes of Vocal Tract and Surrounding Tissue Resonances, Measured through the Lips during Phonation},
  author = {Hanna, Noel and Smith, John and Wolfe, Joe},
  year = {2016},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {139},
  number = {5},
  pages = {2924--2936},
  publisher = {Acoustical Society of America},
  issn = {0001-4966},
  doi = {10.1121/1.4948754},
  urldate = {2021-02-25},
  file = {/home/marnix/Zotero/storage/L4AFFBAE/Hanna et al. - 2016 - Frequencies, bandwidths and magnitudes of vocal tr.pdf;/home/marnix/Zotero/storage/JUT7TC9M/1.html}
}

@article{Hanzon2001,
  title = {A {{State-Space Calculus}} for {{Rational Probability Density Functions}} and {{Applications}} to {{Non-Gaussian Filtering}}},
  author = {Hanzon, Bernard and Ober, Raimund J.},
  year = {2001},
  month = jan,
  journal = {SIAM Journal on Control and Optimization},
  volume = {40},
  number = {3},
  pages = {724--740},
  issn = {0363-0129, 1095-7138},
  doi = {10.1137/S036301299731610X},
  urldate = {2021-01-12},
  abstract = {We propose what we believe to be a novel approach to performing calculations for rational density functions using state-space representations of the densities. By standard results from realization theory, a rational probability density function is considered to be the transfer function of a linear system with generally complex entries. The stable part of this system is positive-real, which we call the density summand. The existence of moments is investigated using the Markov parameters of the density summand. Moreover, explicit formulae are given for the existing moments in terms of these Markov parameters. Some of the main contributions of the paper are explicit state-space descriptions for products and convolutions of rational densities.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/XB99N4VQ/Hanzon and Ober - 2001 - A State-Space Calculus for Rational Probability De.pdf}
}

@inproceedings{Hao2013,
  title = {Parameter-{{Free Audio Motif Discovery}} in {{Large Data Archives}}},
  booktitle = {2013 {{IEEE}} 13th {{International Conference}} on {{Data Mining}}},
  author = {Hao, Yuan and {Shokoohi-Yekta}, Mohammad and Papageorgiou, George and Keogh, Eamonn},
  year = {2013},
  month = dec,
  pages = {261--270},
  publisher = {IEEE},
  address = {Dallas, TX, USA},
  doi = {10.1109/ICDM.2013.30},
  urldate = {2021-04-11},
  abstract = {The discovery of repeated structure, i.e. motifs/nearduplicates, is often the first step in exploratory data mining. As such, the last decade has seen extensive research efforts in motif discovery algorithms for text, DNA, time series, protein sequences, graphs, images, and video. Surprisingly, there has been less attention devoted to finding repeated patterns in audio sequences, in spite of their ubiquity in science and entertainment. While there is significant work for the special case of motifs in music, virtually all this work makes many assumptions about data (often to the point of being genre specific) and thus these algorithms do not generalize to audio sequences containing animal vocalizations, industrial processes, or a host of other domains that we may wish to explore.},
  isbn = {978-0-7695-5108-1},
  langid = {english},
  file = {/home/marnix/Zotero/storage/XZ2JDM9Q/Hao et al. - 2013 - Parameter-Free Audio Motif Discovery in Large Data.pdf}
}

@book{Harman2018,
  title = {Object-Oriented Ontology: A New Theory of Everything},
  author = {Harman, Graham},
  year = {2018},
  publisher = {Penguin Random House}
}

@phdthesis{Harrison2013,
  title = {Making Accurate Formant Measurements: {{An}} Empirical Investigation of the Influence of the Measurement Tool, Analysis Settings and Speaker on Formant Measurements},
  author = {Harrison, Philip},
  year = {2013},
  school = {University of York},
  file = {/home/marnix/Zotero/storage/DKE6N6JT/Harrison - Making Accurate Formant Measurements An Empirical.pdf}
}

@article{Harvey2000,
  title = {Inference for Belief Networks Using Coupling from the Past},
  author = {Harvey, M and Neal, {\relax RM}},
  year = {2000},
  journal = {Proceedings of the Sixteenth conference on {\dots}},
  pages = {256--263},
  isbn = {1558607099},
  file = {/home/marnix/Zotero/storage/SMR8HVK6/Harvey and Neal 2000 Inference for belief networks using exact sampling.pdf}
}

@article{Harwath2017,
  title = {Learning {{Word-Like Units}} from {{Joint Audio-Visual Analysis}}},
  author = {Harwath, David and Glass, James R.},
  year = {2017},
  month = may,
  journal = {arXiv:1701.07481 [cs]},
  eprint = {1701.07481},
  primaryclass = {cs},
  urldate = {2020-04-16},
  abstract = {Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the word 'lighthouse' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/marnix/Zotero/storage/HJEARN2N/Harwath and Glass - 2017 - Learning Word-Like Units from Joint Audio-Visual A.pdf;/home/marnix/Zotero/storage/9FUJDQXE/1701.html}
}

@article{Harwath2020,
  title = {Learning {{Hierarchical Discrete Linguistic Units}} from {{Visually-Grounded Speech}}},
  author = {Harwath, David and Hsu, Wei-Ning and Glass, James},
  year = {2020},
  month = feb,
  journal = {arXiv:1911.09602 [cs, eess]},
  eprint = {1911.09602},
  primaryclass = {cs, eess},
  urldate = {2020-06-12},
  abstract = {In this paper, we present a method for learning discrete linguistic units by incorporating vector quantization layers into neural models of visually grounded speech. We show that our method is capable of capturing both word-level and sub-word units, depending on how it is configured. What differentiates this paper from prior work on speech unit learning is the choice of training objective. Rather than using a reconstruction-based loss, we use a discriminative, multimodal grounding objective which forces the learned units to be useful for semantic image retrieval. We evaluate the sub-word units on the ZeroSpeech 2019 challenge, achieving a 27.3\% reduction in ABX error rate over the top-performing submission, while keeping the bitrate approximately the same. We also present experiments demonstrating the noise robustness of these units. Finally, we show that a model with multiple quantizers can simultaneously learn phone-like detectors at a lower layer and word-like detectors at a higher layer. We show that these detectors are highly accurate, discovering 279 words with an F1 score of greater than 0.5.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/marnix/Zotero/storage/KXMGMUCE/Harwath et al. - 2020 - Learning Hierarchical Discrete Linguistic Units fr.pdf}
}

@article{Hasani2022,
  title = {Closed-Form Continuous-Time Neural Networks},
  author = {Hasani, Ramin and Lechner, Mathias and Amini, Alexander and Liebenwein, Lucas and Ray, Aaron and Tschaikowski, Max and Teschl, Gerald and Rus, Daniela},
  year = {2022},
  month = nov,
  journal = {Nature Machine Intelligence},
  volume = {4},
  number = {11},
  pages = {992--1003},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-022-00556-7},
  urldate = {2024-01-17},
  abstract = {Continuous-time neural networks are a class of machine learning systems that can tackle representation learning on spatiotemporal decision-making tasks. These models are typically represented by continuous differential equations. However, their expressive power when they are deployed on computers is bottlenecked by numerical differential equation solvers. This limitation has notably slowed down the scaling and understanding of numerous natural physical phenomena such as the dynamics of nervous systems. Ideally, we would circumvent this bottleneck by solving the given dynamical system in closed form. This is known to be intractable in general. Here, we show that it is possible to closely approximate the interaction between neurons and synapses---the building blocks of natural and artificial neural networks---constructed by liquid time-constant networks efficiently in closed form. To this end, we compute a tightly bounded approximation of the solution of an integral appearing in liquid time-constant dynamics that has had no known closed-form solution so far. This closed-form solution impacts the design of continuous-time and continuous-depth neural models. For instance, since time appears explicitly in closed form, the formulation relaxes the need for complex numerical solvers. Consequently, we obtain models that are between one and five orders of magnitude faster in training and inference compared with differential equation-based counterparts. More importantly, in contrast to ordinary differential equation-based continuous networks, closed-form networks can scale remarkably well compared with other deep learning instances. Lastly, as these models are derived from liquid networks, they show good performance in time-series modelling compared with advanced recurrent neural network models.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Computer science,Software,Statistics},
  file = {/home/marnix/Zotero/storage/3P4Y3UBX/Hasani et al. - 2022 - Closed-form continuous-time neural networks.pdf}
}

@article{Havlin2010,
  title = {Catastrophic {{Cascade}} of {{Failures}} in {{Interdependent Networks}}},
  author = {Havlin, S. and Araujo, N. A. M. and Buldyrev, S. V. and Dias, C. S. and Parshani, R. and Paul, G. and Stanley, H. E.},
  year = {2010},
  eprint = {1012.0206},
  pages = {1--15},
  issn = {0028-0836},
  doi = {10.1038/nature08932},
  abstract = {Modern network-like systems are usually coupled in such a way that failures in one network can affect the entire system. In infrastructures, biology, sociology, and economy, systems are interconnected and events taking place in one system can propagate to any other coupled system. Recent studies on such coupled systems show that the coupling increases their vulnerability to random failure. Properties for interdependent networks differ significantly from those of single-network systems. In this article, these results are reviewed and the main properties discussed.},
  archiveprefix = {arXiv},
  arxivid = {1012.0206},
  isbn = {0028-0836},
  pmid = {20393559},
  file = {/home/marnix/Zotero/storage/EB8GZKQE/Havlin et al. - 2010 - Catastrophic Cascade of Failures in Interdependent Networks.pdf}
}

@article{Hawks1995,
  title = {A Formant Bandwidth Estimation Procedure for Vowel Synthesis},
  author = {Hawks, John W. and Miller, James D.},
  year = {1995},
  month = feb,
  journal = {The Journal of the Acoustical Society of America},
  volume = {97},
  number = {2},
  pages = {1343--1344},
  issn = {0001-4966},
  doi = {10.1121/1.412986},
  urldate = {2022-04-21},
  langid = {english},
  file = {/home/marnix/Zotero/storage/2SHZIDNX/Hawks and Miller - 1995 - A formant bandwidth estimation procedure for vowel.pdf}
}

@article{He2018,
  title = {Acoustic {{Landmarks Contain More Information About}} the {{Phone String}} than {{Other Frames}} for {{Automatic Speech Recognition}} with {{Deep Neural Network Acoustic Model}}},
  author = {He, Di and Lim, Boon Pang and Yang, Xuesong and {Hasegawa-johnson}, Mark and Chen, Deming and Clara, Santa},
  year = {2018},
  eprint = {1710.09985v2},
  pages = {1--31},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1710.09985v2},
  file = {/home/marnix/Zotero/storage/R5RHTAR5/He2018 Acoustic Landmarks Contain More Information About the Phone String than Other Frames for Automatic Speech Recognition with Deep Neural Network Acoustic Model.pdf}
}

@article{He2018a,
  title = {Streaming {{End-to-end Speech Recognition For Mobile Devices}}},
  author = {He, Yanzhang and Sainath, Tara N. and Prabhavalkar, Rohit and McGraw, Ian and Alvarez, Raziel and Zhao, Ding and Rybach, David and Kannan, Anjuli and Wu, Yonghui and Pang, Ruoming and Liang, Qiao and Bhatia, Deepti and Shangguan, Yuan and Li, Bo and Pundak, Golan and Sim, Khe Chai and Bagby, Tom and Chang, Shuo-yiin and Rao, Kanishka and Gruenstein, Alexander},
  year = {2018},
  month = nov,
  journal = {arXiv:1811.06621 [cs]},
  eprint = {1811.06621},
  primaryclass = {cs},
  urldate = {2019-04-16},
  abstract = {End-to-end (E2E) models, which directly predict output character sequences given input speech, are good candidates for on-device speech recognition. E2E models, however, present numerous challenges: In order to be truly useful, such models must decode speech utterances in a streaming fashion, in real time; they must be robust to the long tail of use cases; they must be able to leverage user-specific context (e.g., contact lists); and above all, they must be extremely accurate. In this work, we describe our efforts at building an E2E speech recognizer using a recurrent neural network transducer. In experimental evaluations, we find that the proposed approach can outperform a conventional CTC-based model in terms of both latency and accuracy in a number of evaluation categories.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/marnix/Zotero/storage/PNVPK5G7/He et al. - 2018 - Streaming End-to-end Speech Recognition For Mobile.pdf;/home/marnix/Zotero/storage/U9NWQ3ND/1811.html}
}

@misc{He2024,
  title = {{{MA-LMM}}: {{Memory-Augmented Large Multimodal Model}} for {{Long-Term Video Understanding}}},
  shorttitle = {{{MA-LMM}}},
  author = {He, Bo and Li, Hengduo and Jang, Young Kyun and Jia, Menglin and Cao, Xuefei and Shah, Ashish and Shrivastava, Abhinav and Lim, Ser-Nam},
  year = {2024},
  month = apr,
  number = {arXiv:2404.05726},
  eprint = {2404.05726},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {With the success of large language models (LLMs), integrating the vision model into LLMs to build vision-language foundation models has gained much more interest recently. However, existing LLM-based large multimodal models (e.g., Video-LLaMA, VideoChat) can only take in a limited number of frames for short video understanding. In this study, we mainly focus on designing an efficient and effective model for long-term video understanding. Instead of trying to process more frames simultaneously like most existing work, we propose to process videos in an online manner and store past video information in a memory bank. This allows our model to reference historical video content for long-term analysis without exceeding LLMs' context length constraints or GPU memory limits. Our memory bank can be seamlessly integrated into current multimodal LLMs in an off-the-shelf manner. We conduct extensive experiments on various video understanding tasks, such as long-video understanding, video question answering, and video captioning, and our model can achieve state-of-the-art performances across multiple datasets. Code available at https://boheumd.github.io/MA-LMM/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/marnix/Zotero/storage/RSXP9CTN/He et al. - 2024 - MA-LMM Memory-Augmented Large Multimodal Model fo.pdf;/home/marnix/Zotero/storage/DQ9SNDYG/2404.html}
}

@misc{Heek2020,
  title = {Flax: {{A}} Neural Network Library and Ecosystem for {{JAX}}},
  author = {Heek, Jonathan and Levskaya, Anselm and Oliver, Avital and Ritter, Marvin and Rondepierre, Bertrand and Steiner, Andreas and {van Zee}, Marc},
  year = {2020}
}

@article{Heider2009,
  title = {Liquidity Hoarding and Interbank Market Spreads: {{The}} Role of Counterparty Risk},
  author = {Heider, Florian and Hoerova, Marie and Holthausen, Cornelia},
  year = {2009},
  publisher = {ECB working paper}
}

@inproceedings{Heilbron2015,
  title = {{{ActivityNet}}: {{A}} Large-Scale Video Benchmark for Human Activity Understanding},
  shorttitle = {{{ActivityNet}}},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Heilbron, Fabian Caba and Escorcia, Victor and Ghanem, Bernard and Niebles, Juan Carlos},
  year = {2015},
  month = jun,
  pages = {961--970},
  publisher = {IEEE},
  address = {Boston, MA, USA},
  doi = {10.1109/CVPR.2015.7298698},
  urldate = {2023-11-28},
  abstract = {In spite of many dataset efforts for human action recognition, current computer vision algorithms are still severely limited in terms of the variability and complexity of the actions that they can recognize. This is in part due to the simplicity of current benchmarks, which mostly focus on simple actions and movements occurring on manually trimmed videos. In this paper we introduce ActivityNet, a new largescale video benchmark for human activity understanding. Our benchmark aims at covering a wide range of complex human activities that are of interest to people in their daily living. In its current version, ActivityNet provides samples from 203 activity classes with an average of 137 untrimmed videos per class and 1.41 activity instances per video, for a total of 849 video hours. We illustrate three scenarios in which ActivityNet can be used to compare algorithms for human activity understanding: untrimmed video classification, trimmed activity classification and activity detection.},
  isbn = {978-1-4673-6964-0},
  langid = {english},
  file = {/home/marnix/Zotero/storage/BXHKJNRR/Heilbron et al. - 2015 - ActivityNet A large-scale video benchmark for hum.pdf}
}

@inproceedings{Heller2005,
  title = {Bayesian Hierarchical Clustering},
  booktitle = {Proceedings of the 22nd International Conference on {{Machine}} Learning  - {{ICML}} '05},
  author = {Heller, Katherine A. and Ghahramani, Zoubin},
  year = {2005},
  pages = {297--304},
  publisher = {ACM Press},
  address = {Bonn, Germany},
  doi = {10.1145/1102351.1102389},
  urldate = {2022-06-15},
  abstract = {We present a novel algorithm for agglomerative hierarchical clustering based on evaluating marginal likelihoods of a probabilistic model. This algorithm has several advantages over traditional distance-based agglomerative clustering algorithms. (1) It defines a probabilistic model of the data which can be used to compute the predictive distribution of a test point and the probability of it belonging to any of the existing clusters in the tree. (2) It uses a model-based criterion to decide on merging clusters rather than an ad-hoc distance metric. (3) Bayesian hypothesis testing is used to decide which merges are advantageous and to output the recommended depth of the tree. (4) The algorithm can be interpreted as a novel fast bottom-up approximate inference method for a Dirichlet process (i.e. countably infinite) mixture model (DPM). It provides a new lower bound on the marginal likelihood of a DPM by summing over exponentially many clusterings of the data in polynomial time. We describe procedures for learning the model hyperparameters, computing the predictive distribution, and extensions to the algorithm. Experimental results on synthetic and real-world data sets demonstrate useful properties of the algorithm.},
  isbn = {978-1-59593-180-1},
  langid = {english},
  file = {/home/marnix/Zotero/storage/58IRT52E/Heller and Ghahramani - 2005 - Bayesian hierarchical clustering.pdf}
}

@book{Helmholtz1895,
  title = {On the Sensations of Tone as a Physiological Basis for the Theory of Music},
  author = {von Helmholtz, Hermann and Ellis, Alexander John},
  year = {1895},
  publisher = {{London, New York : Longmans, Green, and Co.}},
  urldate = {2019-04-28},
  collaborator = {{University of California Libraries}},
  langid = {english},
  lccn = {nrlf\_ucb:GLAD-100699083},
  keywords = {Sound},
  file = {/home/marnix/Zotero/storage/4RIAYN8K/Helmholtz and Ellis - 1895 - On the sensations of tone as a physiological basis.pdf}
}

@article{Henderson2013,
  title = {Bayesian Inference Approach to Room-Acoustic Modal Analysis},
  author = {Henderson, Wesley and Goggans, Paul and Xiang, Ning and Botts, Jonathan},
  year = {2013},
  month = aug,
  journal = {AIP Conference Proceedings},
  volume = {1553},
  number = {1},
  pages = {38--45},
  issn = {0094-243X},
  doi = {10.1063/1.4819981},
  urldate = {2019-04-23},
  file = {/home/marnix/Zotero/storage/92KSYA44/Henderson et al. - 2013 - Bayesian inference approach to room-acoustic modal.pdf;/home/marnix/Zotero/storage/72F7EDZV/1.html}
}

@article{Hengel2014,
  title = {A {{Comparison}} of {{Spectro-Temporal Representations}} of {{Audio Signals}}},
  author = {Hengel, P W J Van and Krijnders, J D},
  year = {2014},
  volume = {22},
  number = {2},
  pages = {303--313},
  file = {/home/marnix/Zotero/storage/BCT7Y3UF/van Hengel2014 A Comparison of Spectro-Temporal Representations of Audio Signals.pdf}
}

@book{Hennig2022,
  title = {Probabilistic Numerics: {{Computation}} as Machine Learning},
  author = {Hennig, Philipp and Osborne, Michael A. and Kersting, Hans P.},
  year = {2022},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/9781316681411},
  file = {/home/marnix/Zotero/storage/VZWQPHFH/Hennig et al. - 2022 - Probabilistic numerics Computation as machine lea.pdf}
}

@article{Henrich2004,
  title = {On the Use of the Derivative of Electroglottographic Signals for Characterization of Nonpathological Phonation},
  author = {Henrich, Nathalie and {d'Alessandro}, Christophe and Doval, Boris and Castellengo, Mich{\`e}le},
  year = {2004},
  month = mar,
  journal = {The Journal of the Acoustical Society of America},
  volume = {115},
  number = {3},
  pages = {1321--1332},
  issn = {0001-4966},
  doi = {10.1121/1.1646401},
  urldate = {2022-04-05},
  langid = {english},
  file = {/home/marnix/Zotero/storage/BE9DQLZP/Henrich et al. - 2004 - On the use of the derivative of electroglottograph.pdf}
}

@article{Henrich2005,
  title = {Glottal Open Quotient in Singing: {{Measurements}} and Correlation with Laryngeal Mechanisms, Vocal Intensity, and Fundamental Frequency},
  shorttitle = {Glottal Open Quotient in Singing},
  author = {Henrich, Nathalie and {d'Alessandro}, Christophe and Doval, Boris and Castellengo, Mich{\`e}le},
  year = {2005},
  month = mar,
  journal = {The Journal of the Acoustical Society of America},
  volume = {117},
  number = {3},
  pages = {1417--1430},
  publisher = {Acoustical Society of America},
  issn = {0001-4966},
  doi = {10.1121/1.1850031},
  urldate = {2022-04-06},
  file = {/home/marnix/Zotero/storage/7Z7E42JU/Henrich et al. - 2005 - Glottal open quotient in singing Measurements and.pdf}
}

@misc{Hensman2013,
  title = {Gaussian {{Processes}} for {{Big Data}}},
  author = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D.},
  year = {2013},
  month = sep,
  number = {arXiv:1309.6835},
  eprint = {1309.6835},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1309.6835},
  urldate = {2025-04-21},
  abstract = {We introduce stochastic variational inference for Gaussian process models. This enables the application of Gaussian process (GP) models to data sets containing millions of data points. We show how GPs can be vari- ationally decomposed to depend on a set of globally relevant inducing variables which factorize the model in the necessary manner to perform variational inference. Our ap- proach is readily extended to models with non-Gaussian likelihoods and latent variable models based around Gaussian processes. We demonstrate the approach on a simple toy problem and two real world data sets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/HMKS7MT4/Hensman et al. - 2013 - Gaussian Processes for Big Data.pdf;/home/marnix/Zotero/storage/DMQLJYCV/1309.html}
}

@article{Hensman2017,
  title = {Variational Fourier Features for Gaussian Processes},
  author = {Hensman, James and Durrande, Nicolas and Solin, Arno},
  year = {2017},
  journal = {The Journal of Machine Learning Research},
  volume = {18},
  number = {1},
  pages = {5537--5588},
  publisher = {JMLR. org},
  file = {/home/marnix/Zotero/storage/NEFLHAPS/Hensman et al. - Variational Fourier Features for Gaussian Processe.pdf}
}

@inproceedings{Henter2012,
  title = {Gaussian Process Dynamical Models for Nonparametric Speech Representation and Synthesis},
  booktitle = {2012 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Henter, Gustav Eje and Frean, Marcus R. and Kleijn, W. Bastiaan},
  year = {2012},
  month = mar,
  pages = {4505--4508},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2012.6288919},
  abstract = {We propose Gaussian process dynamical models (GPDMs) as a new, nonparametric paradigm in acoustic models of speech. These use multidimensional, continuous state-spaces to overcome familiar issues with discrete-state, HMM-based speech models. The added dimensions allow the state to represent and describe more than just temporal structure as systematic differences in mean, rather than as mere correlations in a residual (which dynamic features or AR-HMMs do). Being based on Gaussian processes, the models avoid restrictive parametric or linearity assumptions on signal structure. We outline GPDM theory, and describe model setup and initialization schemes relevant to speech applications. Experiments demonstrate subjectively better quality of synthesized speech than from comparable HMMs. In addition, there is evidence for unsupervised discovery of salient speech structure.},
  keywords = {acoustic models,Acoustics,AR-HMM,Computational modeling,continuous state-spaces,Gaussian process dynamical models,Gaussian processes,hidden Markov models,Hidden Markov models,HMM-based speech models,Noise,nonparametric speech representation,nonparametric speech synthesis,outline GPDM theory,sampling,signal structure,Speech,speech acoustic models,speech synthesis,stochastic models},
  file = {/home/marnix/Zotero/storage/32JEIFAK/Henter et al. - 2012 - Gaussian process dynamical models for nonparametri.pdf;/home/marnix/Zotero/storage/Q689MBX4/6288919.html}
}

@article{Hermann1889,
  title = {Phonophotographische {{Untersuchungen}}},
  author = {Hermann, Ludimar},
  year = {1889},
  journal = {Pfl{\"u}gers Archiv European Journal of Physiology},
  volume = {45},
  number = {1},
  pages = {582--592},
  publisher = {Springer},
  file = {/home/marnix/Zotero/storage/CZ32LEUN/Chen_description_of_Hermann_1889.pdf;/home/marnix/Zotero/storage/NXCZ9ABK/Hermann - 1889 - Phonophotographische Untersuchungen.pdf}
}

@article{Hernandez-Stumpfhauser2017,
  title = {The {{General Projected Normal Distribution}} of {{Arbitrary Dimension}}: {{Modeling}} and {{Bayesian Inference}}},
  shorttitle = {The {{General Projected Normal Distribution}} of {{Arbitrary Dimension}}},
  author = {{Hernandez-Stumpfhauser}, Daniel and Breidt, F. Jay and Van Der Woerd, Mark J.},
  year = {2017},
  month = mar,
  journal = {Bayesian Analysis},
  volume = {12},
  number = {1},
  issn = {1936-0975},
  doi = {10.1214/15-BA989},
  urldate = {2025-08-21},
  abstract = {The general projected normal distribution is a simple and intuitive model for directional data in any dimension: a multivariate normal random vector divided by its length is the projection of that vector onto the surface of the unit hypersphere. Observed data consist of the projections, but not the lengths. Inference for this model has been restricted to the two-dimensional (circular) case, using Bayesian methods with data augmentation to generate the latent lengths and a Metropolis-within-Gibbs algorithm to sample from the posterior. We describe a new parameterization of the general projected normal distribution that makes inference in any dimension tractable, including the important three-dimensional (spherical) case, which has not previously been considered. Under this new parameterization, the full conditionals of the unknown parameters have closed forms, and we propose a new slice sampler to draw the latent lengths without the need for rejection. Gibbs sampling with this new scheme is fast and easy, leading to improved Bayesian inference; for example, it is now feasible to conduct model selection among complex mixture and regression models for large data sets. Our parameterization also allows straightforward incorporation of covariates into the covariance matrix of the multivariate normal, increasing the ability of the model to explain directional data as a function of independent regressors. Circular and spherical cases are considered in detail and illustrated with scientific applications. For the circular case, seasonal variation in time-of-day departures of anglers from recreational fishing sites is modeled using covariates in both the mean vector and covariance matrix. For the spherical case, we consider paired angles that describe the relative positions of carbon atoms along the backbone chain of a protein. We fit mixtures of general projected normals to these data, with the best-fitting mixture accurately describing biologically meaningful structures including helices, {$\beta$}-sheets, and coils and turns. Finally, we show via simulation that our methodology has satisfactory performance in some 10-dimensional and 50-dimensional problems.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/KET748NS/Hernandez-Stumpfhauser et al. - 2017 - The General Projected Normal Distribution of Arbitrary Dimension Modeling and Bayesian Inference.pdf}
}

@book{Herrmann2009,
  title = {Introduction to {{Computational Physics}}},
  author = {Herrmann, H J},
  year = {2009},
  publisher = {Swiss Federal Institute of Technology ETH},
  address = {Z{\"u}rich, Switzerland}
}

@article{Hetherington2007,
  title = {{{PocketSUMMIT}} : {{Small-Footprint Continuous Speech Recognition}}},
  author = {Hetherington, I Lee},
  year = {2007},
  journal = {Computer},
  pages = {1465--1468},
  isbn = {9781605603162},
  keywords = {[Electronic Manuscript]},
  file = {/home/marnix/Zotero/storage/LHAZ7883/Hetherington2007 PocketSUMMIT -- Small-Footprint Continuous Speech Recognition.pdf}
}

@book{Higham2015,
  title = {The {{Princeton}} Companion to Applied Mathematics},
  editor = {Higham, Nicholas J. and Dennis, Mark R.},
  year = {2015},
  publisher = {Princeton University Press},
  address = {Princeton},
  isbn = {978-0-691-15039-0},
  langid = {english},
  lccn = {QA155 .P75 2015},
  keywords = {Algebra,Mathematical models,Mathematics},
  file = {/home/marnix/Zotero/storage/APWN9QQV/Higham and Dennis - 2015 - The Princeton companion to applied mathematics.pdf}
}

@article{Hillenbrand1995,
  title = {Acoustic Characteristics of {{American English}} Vowels},
  author = {Hillenbrand, James and Getty, Laura A and Clark, Michael J and Wheeler, Kimberlee},
  year = {1995},
  journal = {The Journal of the Acoustical society of America},
  volume = {97},
  number = {5},
  pages = {3099--3111},
  publisher = {ASA},
  file = {/home/marnix/Zotero/storage/RE24AZG7/Acoustic Characteristics of American English Vowel.pdf}
}

@article{Hinich2000,
  title = {Statistical Theory of Signal Coherence},
  author = {Hinich, Melvin J.},
  year = {2000},
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {25},
  number = {2},
  pages = {256--261},
  issn = {03649059},
  doi = {10.1109/48.838988},
  abstract = {A periodic signal can be perfectly predicted far into the future since it perfectly repeats every period. There is always some variation in the waveform over time for signals which are labeled as periodic but which are not truly deterministic. A formal definition is presented in this paper for such a varying periodic signal and the properties of such a class of signals are exploited. A measure called a signal coherence function of the amount of random variation in each Fourier component of the signal is defined and its statistical properties are developed. This signal coherence function is very different from the coherence function between two stationary signals. The method is applied to a digitized record of an acoustic signal generated by a boat in a bag in the Baltic Sea south of Stockholm, Sweden},
  file = {/home/marnix/Zotero/storage/6S2UJFBM/Hinich2000 A Statistical Theory of Signal Coherence.pdf}
}

@article{Hinton2006,
  title = {A {{Fast Learning Algorithm}} for {{Deep Belief Nets}}},
  author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
  year = {2006},
  month = jul,
  journal = {Neural Computation},
  volume = {18},
  number = {7},
  pages = {1527--1554},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2006.18.7.1527},
  urldate = {2025-10-03},
  abstract = {We show how to use ``complementary priors'' to eliminate the explaining away effects that make inference difficult in densely-connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modelled by long ravines in the free-energy landscape of the top-level associative memory and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/JZNC79M7/Hinton et al. - 2006 - A Fast Learning Algorithm for Deep Belief Nets.pdf}
}

@article{Hinton2012,
  title = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}},
  author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
  year = {2012},
  pages = {27},
  abstract = {Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feedforward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks with many hidden layers, that are trained using new methods have been shown to outperform Gaussian mixture models on a variety of speech recognition benchmarks, sometimes by a large margin. This paper provides an overview of this progress and represents the shared views of four research groups who have had recent successes in using deep neural networks for acoustic modeling in speech recognition.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/ETYR4R6R/Hinton et al. - 2012 - Deep Neural Networks for Acoustic Modeling in Spee.pdf}
}

@incollection{Hinton2012a,
  title = {A {{Practical Guide}} to {{Training Restricted Boltzmann Machines}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  author = {Hinton, Geoffrey E.},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  volume = {7700},
  pages = {599--619},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-35289-8_32},
  urldate = {2023-09-18},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  langid = {english},
  file = {/home/marnix/Zotero/storage/M43HQL8V/Hinton - 2012 - A Practical Guide to Training Restricted Boltzmann.pdf}
}

@misc{Hinton2022,
  title = {The {{Forward-Forward Algorithm}}: {{Some Preliminary Investigations}}},
  shorttitle = {The {{Forward-Forward Algorithm}}},
  author = {Hinton, Geoffrey},
  year = {2022},
  month = dec,
  number = {arXiv:2212.13345},
  eprint = {2212.13345},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.13345},
  urldate = {2025-03-07},
  abstract = {The aim of this paper is to introduce a new learning procedure for neural networks and to demonstrate that it works well enough on a few small problems to be worth further investigation. The Forward-Forward algorithm replaces the forward and backward passes of backpropagation by two forward passes, one with positive (i.e. real) data and the other with negative data which could be generated by the network itself. Each layer has its own objective function which is simply to have high goodness for positive data and low goodness for negative data. The sum of the squared activities in a layer can be used as the goodness but there are many other possibilities, including minus the sum of the squared activities. If the positive and negative passes could be separated in time, the negative passes could be done offline, which would make the learning much simpler in the positive pass and allow video to be pipelined through the network without ever storing activities or stopping to propagate derivatives.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/marnix/Zotero/storage/ECQIEJ5J/Hinton - 2022 - The Forward-Forward Algorithm Some Preliminary Investigations.pdf;/home/marnix/Zotero/storage/H2CQDI84/2212.html}
}

@article{Hiyama2003,
  title = {Gaussian Expansion Method for Few-Body Systems},
  author = {Hiyama, E and Kino, Y and Kamimura, M},
  year = {2003},
  month = jan,
  journal = {Progress in Particle and Nuclear Physics},
  volume = {51},
  number = {1},
  pages = {223--307},
  issn = {0146-6410},
  doi = {10.1016/S0146-6410(03)90015-9},
  urldate = {2019-09-04},
  abstract = {We review our method of calculation, Gaussian Expansion Method (GEM), for bound and scattering states of few-body systems. The method was proposed in 1988 and has been applied to a variety of few-body systems. The understanding on the structure and the mechanisms of reactions of those systems obtained from such applications is discussed together with some useful techniques for the calculations. A well-chosen set of Gaussian basis functions forms an approximate complete set in a finite coordinate space so that it can describe accurately short-range corielations and long-range asymptotic behavior as well as highly oscillatory character of wave functions in the bound and the scattering states of the systems. Examples of applications of GEM include i) the latest determination of antiproton mass by the analysis of laser spectroscopic data for antiprotonic helium atoms, ii) predictions and experimental verifications on the structure of hypernuclei and hyperon-nucleon interactions, iii) Coulomb three-body calculations of bound and resonant states of muonic molecules as well as muon transfer reactions in muon catalyzed fusion cycles, iv) a new treatment of CDCC (continuum-discretized coupled channels) method for three- and four-body breakup processes, and v) benchmark test calculations for three- and four-nucleon bound states using realistic interactions.},
  file = {/home/marnix/Zotero/storage/M7WGMU6I/S0146641003900159.html}
}

@article{Hofert2013,
  title = {On {{Sampling}} from the {{Multivariate}} t {{Distribution}}},
  author = {Hofert, Marius},
  year = {2013},
  journal = {The R Journal},
  volume = {5},
  number = {2},
  pages = {129},
  issn = {2073-4859},
  doi = {10.32614/RJ-2013-033},
  urldate = {2022-11-22},
  abstract = {The multivariate normal and the multivariate t distributions belong to the most widely used multivariate distributions in statistics, quantitative risk management, and insurance. In contrast to the multivariate normal distribution, the parameterization of the multivariate t distribution does not correspond to its moments. This, paired with a non-standard implementation in the R package mvtnorm, provides traps for working with the multivariate t distribution. In this paper, common traps are clarified and corresponding recent changes to mvtnorm are presented.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/JTPUIRR3/Hofert - 2013 - On Sampling from the Multivariate t Distribution.pdf}
}

@article{Hoffman2010,
  title = {Bayesian {{Nonparametric Matrix Factorization}} for {{Recorded Music}}},
  author = {Hoffman, Matthew D and Blei, David M and Cook, Perry R},
  year = {2010},
  abstract = {Recent research in machine learning has focused on breaking audio spectrograms into separate sources of sound using latent variable decompositions. These methods require that the number of sources be specified in advance, which is not always possible. To address this problem, we develop Gamma Process Nonnegative Matrix Factorization (GaP-NMF), a Bayesian nonparametric approach to decomposing spectrograms. The assumptions behind GaP-NMF are based on research in signal processing regarding the expected distributions of spectrogram data, and GaP-NMF automatically discovers the number of latent sources. We derive a mean-field variational inference algorithm and evaluate GaP-NMF on both synthetic data and recorded music.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/JLR32L2G/Hoffman et al. - Bayesian Nonparametric Matrix Factorization for Recorded Music.pdf}
}

@article{Hoffman2013,
  title = {Stochastic Variational Inference},
  author = {Hoffman, Matthew D and Blei, David M and Wang, Chong and Paisley, John},
  year = {2013},
  journal = {The Journal of Machine Learning Research},
  volume = {14},
  number = {1},
  pages = {1303--1347},
  publisher = {JMLR. org},
  file = {/home/marnix/Zotero/storage/HQXVMU3U/Hoffman - Stochastic Variational Inference.pdf}
}

@inproceedings{Hoffman2016,
  title = {Elbo Surgery: Yet Another Way to Carve up the Variational Evidence Lower Bound},
  booktitle = {Workshop in Advances in Approximate Bayesian Inference, {{NIPS}}},
  author = {Hoffman, Matthew D and Johnson, Matthew J},
  year = {2016},
  volume = {1},
  pages = {2},
  file = {/home/marnix/Zotero/storage/G6ZB5CKA/Hoffman and Johnson - ELBO surgery yet another way to carve up the vari.pdf}
}

@article{Hogden2001,
  title = {A {{Stochastic Articulatory-To-Acoustic Mapping}} as a {{Basis}} for {{Speech Recognition}}},
  author = {Hogden, John and Valdez, Patrick},
  year = {2001},
  file = {/home/marnix/Zotero/storage/NRVRZZ7Z/Hogden2001 A stochastic articulatory-to-acoustic mapping as a basis for speech recognition.pdf}
}

@article{Hogden2009,
  title = {A Blind Algorithm for Recovering Articulator Positions from Acoustics},
  author = {Hogden, John},
  year = {2009},
  number = {December 2009},
  doi = {10.1109/ACSSC.2009.5469740},
  file = {/home/marnix/Zotero/storage/PFRC4L4A/Hogden2009 MIMICRI.pdf}
}

@article{Holmberg1988,
  title = {Glottal Airflow and Transglottal Air Pressure Measurements for Male and Female Speakers in Soft, Normal, and Loud Voice},
  author = {Holmberg, Eva B. and Hillman, Robert E. and Perkell, Joseph S.},
  year = {1988},
  month = aug,
  journal = {The Journal of the Acoustical Society of America},
  volume = {84},
  number = {2},
  pages = {511--529},
  issn = {0001-4966},
  doi = {10.1121/1.396829},
  urldate = {2022-04-19},
  langid = {english},
  file = {/home/marnix/Zotero/storage/S2TVKUDX/Holmberg et al. - 1988 - Glottal airflow and transglottal air pressure meas.pdf}
}

@article{Holmberg1989,
  title = {Glottal Airflow and Transglottal Air Pressure Measurements for Male and Female Speakers in Low, Normal, and High Pitch},
  author = {Holmberg, Eva B. and Hillman, Robert E. and Perkell, Joseph S.},
  year = {1989},
  month = dec,
  journal = {Journal of Voice},
  volume = {3},
  number = {4},
  pages = {294--305},
  issn = {0892-1997},
  doi = {10.1016/S0892-1997(89)80051-7},
  urldate = {2019-03-11},
  abstract = {Summary Measurements on the inverse filtered airflow waveform and of estimated average transglottal pressure and glottal airflow were made from syllable sequences in low, normal, and high pitch for 25 male and 20 female speakers. Correlation analyses indicated that several of the airflow measurements were more directly related to voice intensity than to fundamental frequency (F0). Results suggested that pressure may have different influences in low and high pitch in this speech task. It is suggested that unexpected results of increased pressure in low pitch were related to maintaining voice quality, that is, avoiding vocal fry. In high pitch, the increased pressure may serve to maintain vocal fold vibration. The findings suggested different underlying laryngeal mechanisms and vocal adjustments for increasing and decreasing F0 from normal pitch.},
  keywords = {Glottal airflow,Inverse filter,Male and female speakers,Pitch,Transglottal air pressure},
  file = {/home/marnix/Zotero/storage/2WD83ME2/Holmberg et al. - 1989 - Glottal airflow and transglottal air pressure meas.pdf;/home/marnix/Zotero/storage/GRKX56WN/S0892199789800517.html}
}

@article{Holmberg1989a,
  title = {Erratum: ``{{Glottal}} Airflow and Transglottal Air Pressure Measurements for Male and Female Speakers in Soft, Normal, and Loud Voice'' [{{J}}. {{Acoust}}. {{Soc}}. {{Am}}. 84, 511--529 (1988)]},
  shorttitle = {Erratum},
  author = {Holmberg, Eva B. and Hillman, Robert E. and Perkell, Joseph S.},
  year = {1989},
  month = apr,
  journal = {The Journal of the Acoustical Society of America},
  volume = {85},
  number = {4},
  pages = {1787--1787},
  publisher = {Acoustical Society of America},
  issn = {0001-4966},
  doi = {10.1121/1.398008},
  urldate = {2022-04-08}
}

@article{Holme2011,
  title = {Temporal {{Networks}}},
  author = {Holme, Petter and Saram{\"a}ki, Jari},
  year = {2011},
  eprint = {1108.1780},
  pages = {1--28},
  issn = {03701573},
  doi = {10.1016/j.physrep.2012.03.001},
  abstract = {A great variety of systems in nature, society and technology -- from the web of sexual contacts to the Internet, from the nervous system to power grids -- can be modeled as graphs of vertices coupled by edges. The network structure, describing how the graph is wired, helps us understand, predict and optimize the behavior of dynamical systems. In many cases, however, the edges are not continuously active. As an example, in networks of communication via email, text messages, or phone calls, edges represent sequences of instantaneous or practically instantaneous contacts. In some cases, edges are active for non-negligible periods of time: e.g., the proximity patterns of inpatients at hospitals can be represented by a graph where an edge between two individuals is on throughout the time they are at the same ward. Like network topology, the temporal structure of edge activations can affect dynamics of systems interacting through the network, from disease contagion on the network of patients to information diffusion over an e-mail network. In this review, we present the emergent field of temporal networks, and discuss methods for analyzing topological and temporal structure and models for elucidating their relation to the behavior of dynamical systems. In the light of traditional network theory, one can see this framework as moving the information of when things happen from the dynamical system on the network, to the network itself. Since fundamental properties, such as the transitivity of edges, do not necessarily hold in temporal networks, many of these methods need to be quite different from those for static networks.},
  archiveprefix = {arXiv},
  arxivid = {1108.1780},
  isbn = {0370-1573},
  pmid = {10418295},
  file = {/home/marnix/Zotero/storage/SWWB3DDK/Holme, Saramäki - 2011 - Temporal Networks.pdf}
}

@article{Holme2019,
  title = {Rare and Everywhere: {{Perspectives}} on Scale-Free Networks},
  shorttitle = {Rare and Everywhere},
  author = {Holme, Petter},
  year = {2019},
  month = mar,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {1016},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-09038-8},
  urldate = {2021-06-19},
  abstract = {Are scale-free networks rare or universal? Important or not? We present the recent research about degree distributions of networks. This is a controversial topic, but, we argue, with some adjustments of the terminology, it does not have to be.},
  copyright = {2019 The Author(s)},
  langid = {english},
  annotation = {Bandiera\_abtest: a\\
Cc\_license\_type: cc\_by\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Comments \& Opinion\\
Subject\_term: Complex networks;Power law\\
Subject\_term\_id: complex-networks;power-law},
  file = {/home/marnix/Zotero/storage/TA4C8HUU/Holme - 2019 - Rare and everywhere Perspectives on scale-free ne.pdf;/home/marnix/Zotero/storage/XCRFIBNE/s41467-019-09038-8.html}
}

@inproceedings{Holmes1976,
  title = {Formant Excitation before and after Glottal Closure},
  booktitle = {{{ICASSP}} '76. {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Holmes, J.},
  year = {1976},
  volume = {1},
  pages = {39--42},
  publisher = {{Institute of Electrical and Electronics Engineers}},
  address = {Philadelphia, PA, USA},
  doi = {10.1109/ICASSP.1976.1170095},
  urldate = {2020-04-06},
  file = {/home/marnix/Zotero/storage/VZ7TW4GG/Holmes - 1976 - Formant excitation before and after glottal closur.pdf}
}

@inproceedings{Hotz2002,
  title = {Isometric Embedding by Surface Reconstruction from Distances},
  booktitle = {{{IEEE Visualization}}, 2002. {{VIS}} 2002.},
  author = {Hotz, I.},
  year = {2002},
  pages = {251--257},
  publisher = {IEEE},
  address = {Boston, MA, USA},
  doi = {10.1109/VISUAL.2002.1183782},
  urldate = {2019-09-05},
  abstract = {To display the intuitive meaning of an abstract metric it is helpful to look on an embedded surface with the same inner geometry as the given metric. The resulting partial differential equations have no standard solution. Only for some special cases satisfactory methods are known. I present a new algorithmic approach which is not based on differential equations. In contrast to other methods this technique also works if the embedding exists only locally. The fundamental idea is to estimate Euclidean distances, from which the surface is built up. In this paper I focus on the reconstruction of a surface from these estimated distances. Particular the influence of a perturbation of the distances on the shape of the resulting surface is investigated.},
  isbn = {978-0-7803-7498-0},
  langid = {english},
  file = {/home/marnix/Zotero/storage/TT9F5W5S/Hotz - 2002 - Isometric embedding by surface reconstruction from.pdf}
}

@article{House1958,
  title = {Estimation of {{Formant Band Widths}} from {{Measurements}} of {{Transient Response}} of the {{Vocal Tract}}},
  author = {House, Arthur S. and Stevens, Kenneth N.},
  year = {1958},
  month = dec,
  journal = {Journal of Speech and Hearing Research},
  volume = {1},
  number = {4},
  pages = {309--315},
  issn = {0022-4685},
  doi = {10.1044/jshr.0104.309},
  urldate = {2019-08-13},
  langid = {english},
  file = {/home/marnix/Zotero/storage/DK5VUWHJ/House and Stevens - 1958 - Estimation of Formant Band Widths from Measurement.pdf}
}

@article{Housen2017,
  title = {What Is the Cost of Parallelism and Distributed Computing ?},
  author = {Housen, Kobe},
  year = {2017},
  pages = {12--14},
  file = {/home/marnix/Zotero/storage/KCLXB4C8/Housen - 2017 - What is the cost of parallelism and distributed computing.pdf}
}

@inproceedings{Howard2015,
  title = {White Noise: {{A}} Time Domain Basis},
  shorttitle = {White Noise},
  booktitle = {2015 {{International Conference}} on {{Noise}} and {{Fluctuations}} ({{ICNF}})},
  author = {Howard, R. M.},
  year = {2015},
  month = jun,
  pages = {1--4},
  doi = {10.1109/ICNF.2015.7288581},
  abstract = {Time domain definitions for finite bandwidth white noise, and filtered white noise, are detailed and are of pedagogical value. The associated power spectral density and autocorrelation functions are given. The potential Gaussianity of white noise is noted.},
  keywords = {autocorrelation,autocorrelation functions,Bandwidth,Correlation,filtered white noise,finite bandwidth white noise,Gaussian white noise,Maximum likelihood detection,Nonlinear filters,power spectral density,Random processes,time domain,white noise,White noise,white noise Gaussianity},
  file = {/home/marnix/Zotero/storage/LWRZJ37Y/Howard - 2015 - White noise A time domain basis.pdf;/home/marnix/Zotero/storage/VB3NCSHA/7288581.html}
}

@inproceedings{Howe2003,
  title = {Theory of {{Vortex Sound}}},
  author = {Howe, Michael S.},
  year = {2003},
  abstract = {Theory of Vortex Sound is an introduction to the theory of sound generated by hydrodynamic flows. Startingwith a reviewof elementary theoretical acoustics, the book proceeds to a unified treatment of low Mach number vortex-surface interaction noise in terms of the compact Green's function. Problems are provided at the end of each chapter, many ofwhich can be used for extended student projects, and a whole chapter is devoted to worked examples. It is designed for a one-semester introductory course at the advanced undergraduate or graduate levels. Great care is taken to explain underlying fluid mechanical and acoustic concepts, and to describe as fully as possible the steps in a complicated derivation.},
  keywords = {Acoustic cryptanalysis,Flow,Hydrodynamics,Vortex},
  file = {/home/marnix/Zotero/storage/FR7MXW6X/Howe - 2003 - Theory of Vortex Sound.pdf}
}

@misc{Huang2025,
  title = {Situational {{Agency}}: {{The Framework}} for {{Designing Behavior}} in {{Agent-based}} Art},
  shorttitle = {Situational {{Agency}}},
  author = {Huang, Ary-Yue and Guljajeva, Varvara},
  year = {2025},
  month = feb,
  number = {arXiv:2503.16442},
  eprint = {2503.16442},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.16442},
  urldate = {2025-03-25},
  abstract = {In the context of artificial life art and agent-based art, this paper draws on Simon Penny's \{{\textbackslash}itshape Aesthetic of Behavior\} theory and Sofian Audry's discussions on behavior computation to examine how artists design agent behaviors and the ensuing aesthetic experiences. We advocate for integrating the environment in which agents operate as the context for behavioral design, positing that the environment emerges through continuous interactions among agents, audiences, and other entities, forming an evolving network of meanings generated by these interactions. Artists create contexts by deploying and guiding these computational systems, audience participation, and agent behaviors through artist strategies. This framework is developed by analysing two categories of agent-based artworks, exploring the intersection of computational systems, audience participation, and artistic strategies in creating aesthetic experiences. This paper seeks to provide a contextual foundation and framework for designing agents' behaviors by conducting a comparative study focused on behavioural design strategies by the artists.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction},
  file = {/home/marnix/Zotero/storage/QPIT7HYW/Huang and Guljajeva - 2025 - Situational Agency The Framework for Designing Behavior in Agent-based art.pdf;/home/marnix/Zotero/storage/7JYW3BUA/2503.html}
}

@article{Hubert2018a,
  title = {A {{Sequential Algorithm}} for {{Signal Segmentation}}},
  author = {Hubert, Paulo and Padovese, Linilson and Stern, Julio Michael},
  year = {2018},
  month = jan,
  journal = {Entropy},
  volume = {20},
  number = {1},
  pages = {55},
  doi = {10.3390/e20010055},
  urldate = {2019-04-23},
  abstract = {The problem of event detection in general noisy signals arises in many applications; usually, either a functional form of the event is available, or a previous annotated sample with instances of the event that can be used to train a classification algorithm. There are situations, however, where neither functional forms nor annotated samples are available; then, it is necessary to apply other strategies to separate and characterize events. In this work, we analyze 15-min samples of an acoustic signal, and are interested in separating sections, or segments, of the signal which are likely to contain significant events. For that, we apply a sequential algorithm with the only assumption that an event alters the energy of the signal. The algorithm is entirely based on Bayesian methods.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {audio segmentation,bayesian methods,hypothesis testing,signal detection},
  file = {/home/marnix/Zotero/storage/QJY7C49T/Hubert et al. - 2018 - A Sequential Algorithm for Signal Segmentation.pdf;/home/marnix/Zotero/storage/MTSQN2PE/55.html}
}

@article{Hughes2019,
  title = {Effects of Formant Analysis Settings and Channel Mismatch on Semi-Automatic Forensic Voice Comparison},
  author = {Hughes, Vincent and Harrison, Philip and Foulkes, Paul and French, {\relax JP} and Gully, A},
  year = {2019},
  journal = {Submitted to ICPhS, Melbourne},
  file = {/home/marnix/Zotero/storage/5AE5NUY8/Hughes et al. - EFFECTS OF FORMANT ANALYSIS SETTINGS AND CHANNEL M.pdf}
}

@article{Hughes2020,
  title = {Towards Reliable Digital Forensics Investigations through Measurement Science},
  author = {Hughes, Nicolas and Karabiyik, Umit},
  year = {2020},
  journal = {WIREs Forensic Science},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/wfs2.1367},
  pages = {e1367},
  doi = {10.1002/wfs2.1367},
  abstract = {Abstract Validation is the scientifically accepted methodology for demonstrating the accuracy and reliability of a process. Currently, digital forensics discipline does not conduct the sort of validation testing needed to develop statistical confidence in tool performance, or to generate the empirical data needed to bring objectivity to examiner conclusions. Recently recommended standards provide more reliable results, and tool testing programs provide a better understanding of tools' limitations. However, these efforts do not constitute testing over the full range of expected target device conditions, product functionality, and target device configurations which are needed for validation testing. Several major impediments to validation testing include a lack of validation methods, a lack of reference data, and a lack of the definitional precision and low-level specificity needed to establish a system of measurement in digital forensics. This study attempts to identify the remaining needs that prevent comprehensive validation in digital forensics and identify potential solutions to address those needs. This article is characterized under: Digital and Multimedia Science {\textquestiondown} Cyber Threat Intelligence Jurisprudence and Regulatory Oversight {\textquestiondown} Interdisciplinary Collaboration},
  keywords = {digital forensics,measurement science,tool testing,uncertainty,validation}
}

@book{Huizinga1938,
  title = {Homo Ludens: A Study of the Play-Element in Culture},
  author = {Huizinga, Johan},
  year = {1955},
  publisher = {Beacon Press},
  address = {Boston, MA},
  isbn = {978-0-8070-4681-4}
}

@techreport{Huser2015,
  type = {{{SAFE Working Paper Series}}},
  title = {Too Interconnected to Fail: {{A}} Survey of the Interbank Networks Literature},
  booktitle = {The Journal of Network Theory in Finance},
  author = {H{\"u}ser, Anne-Caroline},
  year = {2015},
  volume = {1},
  number = {91},
  pages = {1--50},
  institution = {Research Center SAFE - Sustainable Architecture for Finance in Europe, Goethe University Frankfurt},
  issn = {20557795},
  doi = {10.21314/JNTF.2015.001},
  abstract = {The banking system is highly interconnected and these connections can be conveniently represented as an interbank network. This survey presents a systematic overview of the recent advances in the theoretical literature on interbank networks. We assess our current understanding of the structure of interbank networks, of how network characteristics affect contagion in the banking system and of how banks form connections when faced with the possibility of contagion and systemic risk. In particular, we highlight how the theoretical literature on interbank networks offers a coherent way of studying interconnections, contagion processes and systemic risk, while emphasizing at the same time the challenges that must be addressed before general results on the link between the structure of the interbank network and financial stability can be established. The survey concludes with a discussion of the policy relevance of interbank network models with a special focus on macro-prudential policies and monetary policy.},
  keywords = {banking,contagion,interbank networks,macro-prudential policy,systemic risk},
  file = {/home/marnix/Zotero/storage/HALQNNPW/Hüser - 2015 - Too interconnected to fail A survey of the interbank networks literature.pdf}
}

@article{Iacovacci2015,
  title = {Mesoscopic {{Structures Reveal}} the {{Network Between}} the {{Layers}} of {{Multiplex Datasets}}},
  author = {Iacovacci, Jacopo and Wu, Zhihao and Bianconi, Ginestra},
  year = {2015},
  month = oct,
  journal = {Phys. Rev. E},
  volume = {92},
  number = {4},
  eprint = {1505.03824},
  pages = {1--14},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.92.042806},
  abstract = {Multiplex networks describe a large variety of complex systems, whose elements (nodes) can be connected by different types of interactions forming different layers (networks) of the multiplex. Multiplex networks include social networks, transportation networks or biological networks in the cell or in the brain. Extracting relevant information from these networks is of crucial importance for solving challenging inference problems and for characterizing the multiplex networks microscopic and mesoscopic structure. Here we propose an information theory method to extract the network between the layers of multiplex datasets, forming a "network of networks". We build an indicator function, based on the entropy of network ensembles, to characterize the mesoscopic similarities between the layers of a multiplex network and we use clustering techniques to characterize the communities present in this network of networks. We apply the proposed method to study the Multiplex Collaboration Network formed by scientists collaborating on different subjects and publishing in the Americal Physical Society (APS) journals. The analysis of this dataset reveals the interplay between the collaboration networks and the organization of knowledge in physics.},
  archiveprefix = {arXiv},
  arxivid = {1505.03824},
  file = {/home/marnix/Zotero/storage/A247J7S9/Iacovacci, Wu, Bianconi - 2015 - Mesoscopic Structures Reveal the Network Between the Layers of Multiplex Datasets.pdf}
}

@article{Jadoul2016,
  title = {Seeking {{Temporal Predictability}} in {{Speech}}: {{Comparing Statistical Approaches}} on 18 {{World Languages}}},
  author = {Jadoul, Yannick and Ravignani, Andrea and Thompson, Bill and Filippi, Piera and {de Boer}, Bart},
  year = {2016},
  abstract = {Temporal regularities in speech, such as interdependencies in the timing of speech events, are thought to scaffold early acquisition of the building blocks in speech. By providing on-line clues to the location and duration of upcoming syllables, temporal structure may aid segmentation and clustering of continuous speech into separable units. This hypothesis tacitly assumes that learners exploitpredictabilityin the temporal structure of speech. Existing measures of speech timing tend to focus on first-order regularities among adjacent units, and are overly sensitive to idiosyncrasies in the data they describe. Here, we compare several statistical methods on a sample of 18 languages, testing whether syllable occurrence is predictable over time. Rather than looking for differences between languages, we aim to find across languages (using clearly defined acoustic, rather than orthographic, measures), temporal predictability in the speech signal which could be exploited by a language learner. First, we analyse distributional regularities using two novel techniques: a Bayesian ideal learner analysis, and a simple distributional measure. Second, we modelhigher-ordertemporal structure-regularities arising in an orderedseriesof syllable timings-testing the hypothesis that non-adjacent temporal structures may explain the gap between subjectively-perceived temporal regularities, and the absence of universally-accepted lower-order objective measures. Together, our analyses provide limited evidence for predictability at different time scales, though higher-order predictability is difficult to reliably infer. We conclude that temporal predictability in speech may well arise from a combination of individually weak perceptual cues at multiple structural levels, but is challenging to pinpoint.},
  isbn = {1662-5161},
  pmid = {27994544},
  keywords = {autoregressive models,Bay,bayesian,npvi,rhythm,speech perception,temporal structure,time series},
  file = {/home/marnix/Zotero/storage/XCZ868HU/Jadoul2016 Seeking Temporal Predictability in Speech.pdf}
}

@article{Jadoul2018,
  title = {Introducing {{Parselmouth}}: {{A Python}} Interface to {{Praat}}},
  author = {Jadoul, Yannick and Thompson, Bill and {de Boer}, Bart},
  year = {2018},
  journal = {Journal of Phonetics},
  volume = {71},
  pages = {1--15},
  doi = {10.1016/j.wocn.2018.07.001}
}

@article{Jasra2005,
  title = {Markov {{Chain Monte Carlo Methods}} and the {{Label Switching Problem}} in {{Bayesian Mixture Modeling}}},
  author = {Jasra, A. and Holmes, C. C. and Stephens, D. A.},
  year = {2005},
  month = feb,
  journal = {Statistical Science},
  volume = {20},
  number = {1},
  pages = {50--67},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/088342305000000016},
  urldate = {2021-03-26},
  abstract = {In the past ten years there has been a dramatic increase of interest in the Bayesian analysis of finite mixture models. This is primarily because of the emergence of Markov chain Monte Carlo (MCMC) methods. While MCMC provides a convenient way to draw inference from complicated statistical models, there are many, perhaps underappreciated, problems associated with the MCMC analysis of mixtures. The problems are mainly caused by the nonidentifiability of the components under symmetric priors, which leads to so-called label switching in the MCMC output. This means that ergodic averages of component specific quantities will be identical and thus useless for inference. We review the solutions to the label switching problem, such as artificial identifiability constraints, relabelling algorithms and label invariant loss functions. We also review various MCMC sampling schemes that have been suggested for mixture models and discuss posterior sensitivity to prior specification.},
  keywords = {Bayesian statistics,Identifiability,label switching,MCMC,mixture modeling,sensitivity analysis},
  file = {/home/marnix/Zotero/storage/JB3HAYJD/Jasra et al. - 2005 - Markov Chain Monte Carlo Methods and the Label Swi.pdf;/home/marnix/Zotero/storage/HLIIRILP/088342305000000016.html}
}

@article{Javid2020,
  title = {Compromise-Free {{Bayesian}} Neural Networks},
  author = {Javid, Kamran and Handley, Will and Hobson, Mike and Lasenby, Anthony},
  year = {2020},
  month = jun,
  journal = {arXiv:2004.12211 [cs, stat]},
  eprint = {2004.12211},
  primaryclass = {cs, stat},
  urldate = {2021-08-19},
  abstract = {We conduct a thorough analysis of the relationship between the out-of-sample performance and the Bayesian evidence (marginal likelihood) of Bayesian neural networks (BNNs), as well as looking at the performance of ensembles of BNNs, both using the Boston housing dataset. Using the state-of-the-art in nested sampling, we numerically sample the full (non-Gaussian and multimodal) network posterior and obtain numerical estimates of the Bayesian evidence, considering network models with up to 156 trainable parameters. The networks have between zero and four hidden layers, either \${\textbackslash}tanh\$ or \$ReLU\$ activation functions, and with and without hierarchical priors. The ensembles of BNNs are obtained by determining the posterior distribution over networks, from the posterior samples of individual BNNs re-weighted by the associated Bayesian evidence values. There is good correlation between out-of-sample performance and evidence, as well as a remarkable symmetry between the evidence versus model size and out-of-sample performance versus model size planes. Networks with \$ReLU\$ activation functions have consistently higher evidences than those with \${\textbackslash}tanh\$ functions, and this is reflected in their out-of-sample performance. Ensembling over architectures acts to further improve performance relative to the individual BNNs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Applications,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/marnix/Zotero/storage/L76XHMBY/Javid et al. - 2020 - Compromise-free Bayesian neural networks.pdf;/home/marnix/Zotero/storage/9789GLSZ/2004.html}
}

@book{Jaynes,
  title = {Thermodynamics},
  author = {Jaynes, E T},
  file = {/home/marnix/Zotero/storage/VNVHYRSX/Jaynes - Thermodynamics.pdf}
}

@article{Jaynes1957,
  title = {Information {{Theory}} and {{Statistical Mechanics}}},
  booktitle = {Physical Review},
  author = {Jaynes, E. T.},
  year = {1957},
  journal = {Physical Review},
  volume = {106},
  number = {4},
  pages = {620--630},
  file = {/home/marnix/Zotero/storage/B62ZU2A8/Jaynes - 1957 - Information Theory and Statistical Mechanics.pdf}
}

@inproceedings{Jaynes1967,
  title = {Foundations of Probability Theory and Statistical Mechanics},
  booktitle = {Delaware Seminar in the Foundations of Physics},
  author = {Jaynes, Edwin T},
  year = {1967},
  pages = {77--101},
  organization = {Springer},
  file = {/home/marnix/Zotero/storage/TJ8TA6EN/Jaynes - 1967 - Foundations of probability theory and statistical .pdf}
}

@article{Jaynes1968,
  title = {Prior {{Probabilities}}},
  author = {Jaynes},
  year = {1968},
  journal = {IEEE Transactions on Systems Science and Cybernetics},
  volume = {4},
  number = {3},
  pages = {227--241},
  issn = {0536-1567},
  doi = {10.1109/TSSC.1968.300117},
  abstract = {In decision theory, mathematical analysis shows that once the sampling distribution, loss function, and sample are specified, the only remaining basis for a choice among different admissible decisions lies in the prior probabilities. Therefore, the logical foundations of decision theory cannot be put in fully satisfactory form until the old problem of arbitrariness (sometimes called "subjectiveness") in assigning prior probabilities is resolved. The principle of maximum entropy represents one step in this direction. Its use is illustrated, and a correspondence property between maximum-entropy probabilities and frequencies is demonstrated. The consistency of this principle with the principles of conventional "direct probability" analysis is illustrated by showing that many known results may be derived by either method. However, an ambiguity remains in setting up a prior on a continuous parameter space because the results lack invariance under a change of parameters; thus a further principle is needed. It is shown that in many problems, including some of the most important in practice, this ambiguity can be removed by applying methods of group theoretical reasoning which have long been used in theoretical physics. By finding the group of transformations on the parameter space which convert the problem into an equivalent one, a basic desideratum of consistency can be stated in the form of functional equations which impose conditions on, and in some cases fully determine, an "invariant measure" on the parameter space.},
  isbn = {0536-1567},
  file = {/home/marnix/Zotero/storage/DFIQLXLI/Jaynes1968 Prior probabilities.pdf}
}

@article{Jaynes1973,
  title = {The Well-Posed Problem},
  author = {Jaynes, Edwin T},
  year = {1973},
  journal = {Foundations of Physics},
  volume = {3},
  number = {4},
  pages = {477--492},
  publisher = {Springer},
  file = {/home/marnix/Zotero/storage/DMNBNPG6/Jaynes - 1973 - The well-posed problem.pdf}
}

@incollection{Jaynes1976,
  title = {Confidence {{Intervals}} vs {{Bayesian Intervals}}},
  booktitle = {Foundations of {{Probability Theory}}, {{Statistical Inference}}, and {{Statistical Theories}} of {{Science}}: {{Proceedings}} of an {{International Research Colloquium}} Held at the {{University}} of {{Western Ontario}}, {{London}}, {{Canada}}, 10--13 {{May}} 1973 {{Volume II Foundations}} and {{Philosophy}} of {{Statistical Inference}}},
  author = {Jaynes, E. T.},
  editor = {Harper, William Leonard and Hooker, Clifford Alan},
  year = {1976},
  series = {The {{University}} of {{Western Ontario Series}} in {{Philosophy}} of {{Science}}},
  pages = {175--257},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-010-1436-6_6},
  urldate = {2019-05-11},
  abstract = {For many years, statistics textbooks have followed this `canonical' procedure: (1) the reader is warned not to use the discredited methods of Bayes and Laplace, (2) an orthodox method is extolled as superior and applied to a few simple problems, (3) the corresponding Bayesian solutions are not worked out or described in any way. The net result is that no evidence whatsoever is offered to substantiate the claim of superiority of the orthodox method.To correct this situation we exhibit the Bayesian and orthodox solutions to six common statistical problems involving confidence intervals (including significance tests based on the same reasoning). In every case, we find that the situation is exactly the opposite; i.e., the Bayesian method is easier to apply and yields the same or better results. Indeed, the orthodox results are satisfactory only when they agree closely (or exactly) with the Bayesian results. No contrary example has yet been produced.By a refinement of the orthodox statistician's own criterion of performance, the best confidence interval for any location or scale parameter is proved to be the Bayesian posterior probability interval. In the cases of point estimation and hypothesis testing, similar proofs have long been known. We conclude that orthodox claims of superiority are totally unjustified; today, the original statistical methods of Bayes and Laplace stand in a position of proven superiority in actual performance, that places them beyond the reach of mere ideological or philosophical attacks. It is the continued teaching and use of orthodox methods that is in need of justification and defense.},
  isbn = {978-94-010-1436-6},
  langid = {english},
  keywords = {Ancillary Statistic,Bayesian Method,Bayesian Solution,Bayesian Test,Common Sense},
  file = {/home/marnix/Zotero/storage/EQN2XHF8/Jaynes - 1976 - Confidence Intervals vs Bayesian Intervals.pdf}
}

@article{Jaynes1978,
  title = {Where Do We Stand on Maximum Entropy?},
  author = {Jaynes, Edwin T},
  year = {1978},
  journal = {FEBS Letters},
  volume = {88},
  number = {2},
  pages = {273--274},
  file = {/home/marnix/Zotero/storage/57N83KMR/Jaynes1978 Where do we stand on maximum entropy.pdf}
}

@article{Jaynes1982,
  title = {On the Rationale of Maximum-Entropy Methods},
  author = {Jaynes, E. T.},
  year = {1982},
  month = sep,
  journal = {Proceedings of the IEEE},
  volume = {70},
  number = {9},
  pages = {939--952},
  issn = {0018-9219},
  doi = {10.1109/PROC.1982.12425},
  abstract = {We discuss the relations between maximum-entropy (MAXENT) and other methods of spectral analysis such as the Schuster, Blackman-Tukey, maximum-likelihood, Bayesian, and Autoregressive (AR, ARMA, or ARIMA) models, emphasizing that they are not in conflict, but rather are appropriate in different problems. We conclude that: 1) "Orthodox" sampling theory methods are useful in problems where we have a known model (sampling distribution) for the properties of the noise, but no appreciable prior information about the quantities being estimated. 2) MAXENT is optimal in problems where we have prior information about multiplicities, but no noise. 3) The full Bayesian solution includes both of these as special cases and is needed in problems where we have both prior information and noise. 4) AR models are in one sense a special case of MAXENT, but in another sense they are ubiquitous in all spectral analysis problems with discrete time series. 5) Empirical methods such as Blackman-Tukey, which do not invoke even a likelihood function, are useful in the preliminary, exploratory phase of a problem where our knowledge is sufficient to permit intuitive judgments about how to organize a calculation (smoothing, decimation, windows, prewhitening, padding with zeroes, etc.) but insufficient to set up a quantitative model which would do the proper things for us automatically and optimally.},
  keywords = {Bayesian methods,Feeds,Maximum likelihood estimation,Phase estimation,Phase noise,Physics,Sampling methods,Smoothing methods,Spectral analysis,Statistical distributions},
  file = {/home/marnix/Zotero/storage/79E7G3MN/Jaynes - 1982 - On the rationale of maximum-entropy methods.pdf;/home/marnix/Zotero/storage/96BFPFA3/1456693.html}
}

@article{Jaynes1984,
  title = {The Intuitive Inadequacy of Classical Statistics},
  author = {Jaynes, Edwin T},
  year = {1984},
  journal = {Epistemologia},
  volume = {7},
  number = {43},
  pages = {43--74},
  file = {/home/marnix/Zotero/storage/NBADWTJD/Jaynes - 1984 - The intuitive inadequacy of classical statistics.pdf}
}

@article{Jaynes1984a,
  title = {Prior Information and Ambiguity in Inverse Problems},
  author = {Jaynes, Edwin T},
  year = {1984},
  journal = {Inverse Problems. An International Journal on the Theory and Practice of Inverse Problems, Inverse Methods and Computerized Inversion of Data},
  volume = {14},
  pages = {151--166},
  publisher = {Providence, Rhode Island: American Mathematical Society},
  file = {/home/marnix/Zotero/storage/P8ZV5HBT/Jaynes - 1984 - Prior information and ambiguity in inverse problem.pdf}
}

@article{Jaynes1985,
  title = {Highly Informative Priors},
  author = {Jaynes, E. T.},
  year = {1985},
  journal = {Bayesian Statistics},
  number = {2},
  pages = {329--360},
  publisher = {Citeseer},
  file = {/home/marnix/Zotero/storage/C5ICFT2E/Jaynes - 1985 - Highly informative priors.pdf}
}

@article{Jaynes1987,
  title = {Bayesian {{Spectrum}} and {{Chirp Analysis}}},
  author = {Jaynes, E T},
  year = {1987},
  pages = {1--29},
  file = {/home/marnix/Zotero/storage/3DDEUWYK/Jaynes - 1987 - Bayesian Spectrum and Chirp Analysis.pdf}
}

@incollection{Jaynes1988,
  title = {Detection of Extra-Solar System Planets},
  booktitle = {Maximum-{{Entropy}} and {{Bayesian Methods}} in {{Science}} and {{Engineering}}},
  author = {Jaynes, {\relax ET}},
  year = {1988},
  pages = {147--160},
  publisher = {Springer},
  file = {/home/marnix/Zotero/storage/N8HULW68/Jaynes - 1988 - Detection of extra-solar system planets.pdf}
}

@article{Jaynes1989,
  title = {Clearing up {{Mysteries}} --- {{The Original Goal}}},
  author = {Jaynes, E. T.},
  year = {1989},
  journal = {Maximum Entropy and Bayesian Methods},
  eprint = {1011.1669v3},
  pages = {1--27},
  issn = {1435-8115},
  doi = {10.1007/978-94-015-7860-8_1},
  abstract = {We show how the character of a scientific theory depends on one's attitude towards probability. Many circumstances seem mysterious or paradoxical to one who thinks that probabilities are real physical properties existing in Nature. But when we adopt the ``Bayesian Inference'' viewpoint of Harold Jeffreys, paradoxes often become simple platitudes and we have a more powerful tool for useful calculations. This is illustrated by three examples from widely different fields: diffusion in kinetic theory, the Einstein-Podolsky-Rosen (EPR) paradox in quantum theory, and the second law of thermodynamics in biology.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1011.1669v3},
  isbn = {978-90-481-4044-2},
  pmid = {21281554},
  file = {/home/marnix/Zotero/storage/M4SIYALB/cmystery.pdf}
}

@incollection{Jaynes1989a,
  title = {Marginalization and {{Prior Probabilities}} (1980)},
  booktitle = {E. {{T}}. {{Jaynes}}: {{Papers}} on {{Probability}}, {{Statistics}} and {{Statistical Physics}}},
  author = {Jaynes, E. T},
  editor = {Rosenkrantz, R. D.},
  year = {1989},
  pages = {337--375},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-009-6581-2_12},
  urldate = {2020-10-22},
  isbn = {978-0-7923-0213-1 978-94-009-6581-2},
  langid = {english},
  file = {/home/marnix/Zotero/storage/TVCLCUD3/Jaynes - 1989 - Marginalization and Prior Probabilities (1980).pdf}
}

@incollection{Jaynes1991,
  title = {Notes on {{Present Status}} and {{Future Prospects}}},
  booktitle = {Maximum {{Entropy}} and {{Bayesian Methods}}},
  author = {Jaynes, E. T.},
  editor = {Grandy, W. T. and Schick, L. H.},
  year = {1991},
  pages = {1--13},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-011-3460-6_1},
  urldate = {2019-04-25},
  isbn = {978-94-010-5531-4 978-94-011-3460-6},
  langid = {english},
  file = {/home/marnix/Zotero/storage/FFSEEKPI/Jaynes - 1991 - Notes on Present Status and Future Prospects.pdf}
}

@incollection{Jaynes1993,
  title = {A {{Backward Look}} to the {{Future}}},
  booktitle = {Physics and {{Probability}}},
  author = {Jaynes, E. T.},
  editor = {Grandy, W. T. Jr and Milonni, P. W.},
  year = {1993},
  pages = {261--276},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9780511524448.024},
  urldate = {2019-08-19},
  abstract = {We survey brie y some fty years of thinking about physics and probability with the aim of explaining: 1 What I did not know then, but know now; 2 What I have been trying to accomplish in science and education, and to what extent these e orts have succeeded; 3 What remains un nished, but where I think the greatest future opportunities lie; and 4 What personal and professional advice I can now give to young people and wish someone had given me fty years ago.},
  isbn = {978-0-511-52444-8},
  langid = {english},
  file = {/home/marnix/Zotero/storage/CXIHATCC/Jaynes - 1993 - A Backward Look to the Future.pdf}
}

@article{Jaynes1996,
  title = {Probability in {{Quantum Theory}}},
  author = {Jaynes, E. T.},
  year = {1996},
  abstract = {For some sixty years it has appeared to many physicists that probability plays a fundamentally di erent role in quantum theory than it does in statistical mechanics and analysis of measurement errors. It is a commonly heard statement that probabilities calculated within a pure state have a di erent character than the probabilities with which di erent pure states appear in a mixture, or density matrix. As Pauli put it, the former represents {\textbackslash}Eine prinzipielle Unbestimmtheit, nicht nur Unbekanntheit". But this viewpoint leads to so many paradoxes and mysteries that we explore the consequences of the uni ed view, that all probability signi es only incomplete human information. We examine in detail only one of the issues this raises: the reality of zero point energy.},
  file = {/home/marnix/Zotero/storage/4X62DRYS/Jaynes - Probability in Quantum Theory.pdf}
}

@misc{Jaynes1999,
  title = {Straight Line Fitting - a {{Bayesian}} Solution},
  author = {Jaynes, E T},
  year = {1999},
  file = {/home/marnix/Zotero/storage/UQYK3DXK/Jaynes - 1999 - Straight line fitting - a Bayesian solution.pdf}
}

@book{Jaynes2003,
  title = {Probability Theory: {{The}} Logic of Science},
  shorttitle = {Probability Theory},
  author = {Jaynes, E. T},
  year = {2003},
  publisher = {Cambridge University Press},
  address = {Cambridge, UK; New York, NY},
  urldate = {2019-05-11},
  abstract = {A comprehensive introduction to the role of probability theory in general scientific endeavour. This book provides an original interpretation of probability theory, showing the subject to be an extension of logic, and presenting new results and applications. Ideal for scientists working in any area involving inference from incomplete information.},
  isbn = {978-0-511-06589-7 978-0-521-59271-0 978-0-511-06802-7 978-0-511-79042-3 978-1-139-63632-2 978-1-280-41722-1},
  langid = {english},
  annotation = {Edited by G. Larry Bretthorst},
  file = {/home/marnix/Zotero/storage/DBPHSBKQ/Jaynes and Bretthorst - 2003 - Probability theory the logic of science.pdf}
}

@article{Jaynes2003a,
  title = {Note on Thermal Heating Efficiency},
  author = {Jaynes, E. T.},
  year = {2003},
  month = feb,
  journal = {American Journal of Physics},
  volume = {71},
  number = {2},
  pages = {180--182},
  issn = {0002-9505},
  doi = {10.1119/1.1508446},
  urldate = {2023-07-24},
  abstract = {Kelvin showed the maximum efficiency with which heat can be converted into work; but there is a dual theorem about the maximum efficiency with which heat at one temperature can be converted into heat at another temperature. It has some surprising implications, in particular that the efficiency with which we heat our buildings could in principle be improved by a large factor. This long known, but still little known, fact is of current pedagogical interest and practical importance.},
  file = {/home/marnix/Zotero/storage/6U96JC77/Jaynes - 2003 - Note on thermal heating efficiency.pdf;/home/marnix/Zotero/storage/V36RA7HA/Jaynes - 2003 - Note on thermal heating efficiency.pdf;/home/marnix/Zotero/storage/KZLBGXS9/Note-on-thermal-heating-efficiency.html}
}

@book{Jeffreys1967,
  title = {Theory of Probability},
  author = {Jeffreys, Harold},
  year = {1967},
  publisher = {Clarendon Press},
  address = {Oxford},
  langid = {english},
  annotation = {OCLC: 13471303},
  file = {/home/marnix/Zotero/storage/JNLQLQJE/Harold - Theory Of Probability.pdf}
}

@article{Jenet2000,
  title = {Detection of {{Variable Frequency Signals Using}} a {{Fast Chirp Transform}}},
  author = {Jenet, F A and Prince, T A},
  year = {2000},
  number = {3},
  eprintclass = {arXiv:gr-qc},
  pages = {1--13},
  arxiv = {0012029v1},
  arxivid = {arXiv:gr-qc/0012029v1},
  file = {/home/marnix/Zotero/storage/44KKBZRL/Jenet2000 Fast Chirp Transform.pdf}
}

@article{Jidling2017,
  title = {Linearly Constrained {{Gaussian}} Processes},
  author = {Jidling, Carl and Wahlstr{\"o}m, Niklas and Wills, Adrian and Sch{\"o}n, Thomas B.},
  year = {2017},
  month = sep,
  journal = {arXiv:1703.00787 [stat]},
  eprint = {1703.00787},
  primaryclass = {stat},
  urldate = {2021-08-13},
  abstract = {We consider a modification of the covariance function in Gaussian processes to correctly account for known linear constraints. By modelling the target function as a transformation of an underlying function, the constraints are explicitly incorporated in the model such that they are guaranteed to be fulfilled by any sample drawn or prediction made. We also propose a constructive procedure for designing the transformation operator and illustrate the result on both simulated and real-data examples.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/YEDZ33VA/Jidling et al. - 2017 - Linearly constrained Gaussian processes.pdf;/home/marnix/Zotero/storage/VMJNWV6S/1703.html}
}

@article{Johnson2016,
  title = {` {{Many}} to {{One}} ' in the {{Articulation}} to {{Acoustics Map}}},
  author = {Johnson, Keith},
  year = {2016},
  pages = {64--70},
  file = {/home/marnix/Zotero/storage/XGGC458N/Bakst2016 Many to One in the Articulation to Acoustics Map.pdf}
}

@article{Johnson2018,
  title = {Vocal Tract Length Normalization},
  author = {Johnson, Keith},
  year = {2018},
  journal = {UC Berkeley PhonLab Annual Report},
  volume = {14},
  number = {1},
  file = {/home/marnix/Zotero/storage/HS45X4X4/Johnson - 2018 - Vocal tract length normalization.pdf}
}

@article{Jones2018,
  title = {Inverse Eigenvalue Problems for Checkerboard {{Toeplitz}} Matrices},
  author = {Jones, T. H. and Willms, N. B.},
  year = {2018},
  month = jun,
  volume = {1047},
  pages = {012016},
  publisher = {IOP Publishing},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/1047/1/012016},
  urldate = {2021-10-16},
  abstract = {The inverse eigenvalue problem for real symmetric Toeplitz matrices motivates this investigation. The existence of solutions is known, but the proof, due to H. Landau, is not constructive. Thus a restriction, namely the required eigenvalues are to be equally spaced, is considered here. Two types of structured matrices arise, herein termed, ``checkerboard'' and ``outer-banded''. Examples are presented. Properties of these structured matrices are explored and a full characterization of checkerboard matrices is given. The inverse eigenvalue problem is solved within the class of odd checkerboard matrices. In addition the ``symmetric-spectrum'' inverse eigenvalue problem is solved within a subclass of Hankel matrices. A regularity conjecture of H. Landau for the Toeplitz inverse eigenvalue problem is discussed and a similar conjecture for checkerboard Toeplitz matrices is given.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/MVQSI8D8/Jones and Willms - 2018 - Inverse eigenvalue problems for checkerboard Toepl.pdf}
}

@phdthesis{Jopling2009,
  title = {Voice Initiation and Voice Offset Patterns in Normal Females: Investigated by High Speed Digital Imaging},
  author = {Jopling, Rebecca LeBlanc},
  year = {2009},
  school = {Louisiana State University and Agricultural \& Mechanical College}
}

@incollection{Jordan1998,
  title = {An {{Introduction}} to {{Variational Methods}} for {{Graphical Models}}},
  booktitle = {Learning in {{Graphical Models}}},
  author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
  editor = {Jordan, Michael I.},
  year = {1998},
  pages = {105--161},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-011-5014-9_5},
  urldate = {2020-08-25},
  abstract = {This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.},
  isbn = {978-94-010-6104-9 978-94-011-5014-9},
  langid = {english},
  file = {/home/marnix/Zotero/storage/EGP2BPQR/Jordan et al. - 1998 - An Introduction to Variational Methods for Graphic.pdf}
}

@article{Jukic2005,
  title = {Least Squares Fitting {{Gaussian}} Type Curve},
  author = {Juki{\'c}, Dragan and Scitovski, Rudolf},
  year = {2005},
  journal = {Applied Mathematics and Computation},
  volume = {167},
  number = {1},
  pages = {286--298},
  issn = {00963003},
  doi = {10.1016/j.amc.2004.06.084},
  abstract = {Given the data (pi, ti, yi), i = 1, ..., m, m {$\geq$} 3, we give necessary and sufficient conditions which guarantee the existence of the weighted least squares estimate for a Gaussian type function. To this end, we suggest a choice of the suitable initial approximation for an iterative minimization, and give some numerical examples. {\copyright} 2004 Elsevier Inc. All rights reserved.},
  keywords = {Existence problem,Gaussian function,Least squares estimate,Nonlinear least squares},
  file = {/home/marnix/Zotero/storage/39D4IW3M/Jukic2005 Least squares fitting Gaussian type curve.pdf}
}

@article{Kadiri2021,
  title = {Extraction and {{Utilization}} of {{Excitation Information}} of {{Speech}}: {{A Review}}},
  shorttitle = {Extraction and {{Utilization}} of {{Excitation Information}} of {{Speech}}},
  author = {Kadiri, Sudarsana Reddy and Alku, Paavo and Yegnanarayana, B.},
  year = {2021},
  month = dec,
  journal = {Proceedings of the IEEE},
  volume = {109},
  number = {12},
  pages = {1920--1941},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2021.3126493},
  urldate = {2023-01-18},
  abstract = {Speech production can be regarded as a process where a time-varying vocal tract system (filter) is excited by a time-varying excitation. In addition to its linguistic message, the speech signal also carries information about, for example, the gender and age of the speaker. Moreover, the speech signal includes acoustical cues about several speaker traits, such as the emotional state and the state of health of the speaker. In order to understand the production of these acoustical cues by the human speech production mechanism and utilize this information in speech technology, it is necessary to extract features describing both the excitation and the filter of the human speech production mechanism. While the methods to estimate and parameterize the vocal tract system are well established, the excitation appears less studied. This article provides a review of signal processing approaches used for the extraction of excitation information from speech. This article highlights the importance of excitation information in the analysis and classification of phonation type and vocal emotions, in the analysis of nonverbal laughter sounds, and in studying pathological voices. Furthermore, recent developments of deep learning techniques in the context of extraction and utilization of the excitation information are discussed.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/7X2TCH2N/Kadiri et al. - 2021 - Extraction and Utilization of Excitation Informati.pdf}
}

@article{Kadiri2025,
  title = {Formant {{Tracking}} by {{Combining Deep Neural Network}} and {{Linear Prediction}}},
  author = {Kadiri, Sudarsana Reddy and Huang, Kevin and Hagedorn, Christina and Byrd, Dani and Alku, Paavo and Narayanan, Shrikanth},
  year = {2025},
  journal = {IEEE Open Journal of Signal Processing},
  volume = {6},
  pages = {222--230},
  issn = {2644-1322},
  doi = {10.1109/OJSP.2025.3530876},
  urldate = {2025-04-22},
  abstract = {Formant tracking is an area of speech science that has recently undergone a technology shift from classical model-driven signal processing methods to modern data-driven deep learning methods. In this study, these two domains are combined in formant tracking by refining the formants estimated by a data-driven deep neural network (DNN) with formant estimates given by a model-driven linear prediction (LP) method. In the refinement process, the three lowest formants, initially estimated by the DNN-based method, are frame-wise replaced with local spectral peaks identified by the LP method. The LP-based refinement stage can be seamlessly integrated into the DNN without any training. As an LP method, the study advocates the use of quasiclosed phase forward-backward (QCP-FB) analysis. Three spectral representations are compared as DNN inputs: mel-frequency cepstral coefficients (MFCCs), the spectrogram, and the complex spectrogram. Formant tracking performance was evaluated by comparing the proposed refined DNN tracker with seven reference trackers, which included both signal processing and deep learning based methods. As evaluation data, ground truth formants of the Vocal Tract Resonance (VTR) corpus were used. The results demonstrate that the refined DNN trackers outperformed all conventional trackers. The best results were obtained by using the MFCC input for the DNN. The proposed MFCC refinement (MFCC-DNNQCP-FB) reduced estimation errors by 0.8 Hz, 12.9 Hz, and 11.7 Hz for the first (F1), second (F2), and third (F3) formants, respectively, compared to the Deep Formants refinement (DeepFQCP-FB). When compared to the model-driven KARMA tracking method, the proposed refinement reduced estimation errors by 2.3 Hz, 55.5 Hz, and 143.4 Hz for F1, F2, and F3, respectively. A detailed evaluation across various phonetic categories and gender groups showed that the proposed hybrid refinement approach improves formanttracking performance across most test conditions.},
  keywords = {Accuracy,Artificial neural networks,Computational modeling,Data models,deep learning,Estimation,Formant tracking,linear prediction,machine learning,MFCCs,Overfitting,Predictive models,Signal processing,spectrogram,Spectrogram,Training},
  file = {/home/marnix/Zotero/storage/QSLW9G9A/Kadiri et al. - 2025 - Formant Tracking by Combining Deep Neural Network and Linear Prediction.pdf}
}

@article{Kalambet2018,
  title = {Comparison of Integration Rules in the Case of Very Narrow Chromatographic Peaks},
  author = {Kalambet, Yuri and Kozmin, Yuri and Samokhin, Andrey},
  year = {2018},
  month = aug,
  journal = {Chemometrics and Intelligent Laboratory Systems},
  volume = {179},
  pages = {22--30},
  issn = {0169-7439},
  doi = {10.1016/j.chemolab.2018.06.001},
  urldate = {2019-01-30},
  abstract = {Theory of peak integration is revised for very narrow peaks. It is shown, that Trapezoidal rule area is efficient estimate of full peak area with extraordinary low error. Simpson's rule is less efficient in full area integration. Theoretical conclusions are illustrated by digital simulation and processing of experimental data. It was shown that for Gaussian peak Trapezoidal rule requires 0.62 points per standard deviation (2.5 points per peak width at baseline) to achieve integration error of only 0.1\%, while Simpson's rule requires 1.8 times higher data rates. Asymmetric peaks require higher data rates as well. Reasons of poor behavior of Simpson's rule are discussed; averaged Simpson's rules are constructed, these rules coincide with those based on Euler-Maclaurin formula. Euler-Maclaurin rules can reduce error in the case of partial peak integration. Higher peak moments (average retention time, dispersion, skewness, etc.) also exhibit extraordinary low errors and can potentially be used for evaluation of peak shape.},
  keywords = {Data sampling error,Euler-Maclaurin formula,Integration,Narrow chromatographic peak,Simpson's rule,Trapezoidal rule},
  file = {/home/marnix/Zotero/storage/N4GB2T3A/Kalambet et al. - 2018 - Comparison of integration rules in the case of ver.pdf;/home/marnix/Zotero/storage/27Y6KH9D/S0169743917305555.html}
}

@article{Kameoka2010,
  title = {Speech {{Spectrum Modeling}} for {{Joint Estimation}} of {{Spectral Envelope}} and {{Fundamental Frequency}}},
  author = {Kameoka, Hirokazu and Ono, Nobutaka and Sagayama, Shigeki},
  year = {2010},
  month = aug,
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {18},
  number = {6},
  pages = {1507--1516},
  issn = {1558-7924},
  doi = {10.1109/TASL.2009.2036287},
  urldate = {2025-04-07},
  abstract = {Although considerable effort has been devoted to both fundamental frequency (F0) and spectral envelope estimation in the field of speech processing, the problem of determining F0 and spectral envelopes has largely been tackled independently. If F0 were known in advance, then the spectral envelope could be estimated very reliably. On the other hand, if the spectral envelope were known in advance, then we could obtain a reliable F0 estimate. F0 and the spectral envelope, each of which is a prerequisite of the other, should thus be estimated jointly rather than independently in succession. On this basis, we develop a parametric speech spectrum model that allows us to estimate the F0 and spectral envelope simultaneously. We confirmed experimentally the significant advantage of this joint estimation approach for both F0 estimation and spectral envelope estimation.},
  keywords = {Distortion measurement,Expectation--maximization (EM) algorithm,F{$_0$} estimation,Frequency estimation,Laboratories,Linear predictive coding,Power harmonic filters,Power system modeling,spectral envelope estimation,speech analysis,Speech analysis,Speech processing,Speech synthesis,Statistical distributions},
  file = {/home/marnix/Zotero/storage/TLMW9G65/Kameoka et al. - 2010 - Speech Spectrum Modeling for Joint Estimation of Spectral Envelope and Fundamental Frequency.pdf}
}

@inproceedings{Kameoka2010a,
  title = {Speech Analysis with Multi-Kernel Linear Prediction},
  booktitle = {Proceedings of the Acoustical Society of Japan ({{ASJ}}) Spring Meeting},
  author = {Kameoka, Hirokazu and Ohishi, Yasunori and Mochihashi, Daichi and Le Roux, Jonathan},
  year = {2010},
  pages = {2--Q--24},
  address = {Tokyo, Japan},
  file = {/home/marnix/Zotero/storage/4GG4GK2F/Kameoka2010ASJ03.pdf;/home/marnix/Zotero/storage/QS3Z5QMJ/Kameoka et al. - 2010 - Speech analysis with multi-kernel linear prediction.pdf}
}

@article{Kamper2017,
  title = {An Embedded Segmental {{K-means}} Model for Unsupervised Segmentation and Clustering of Speech},
  author = {Kamper, Herman and Livescu, Karen and Goldwater, Sharon},
  year = {2017},
  eprint = {1703.08135},
  doi = {10.1109/ASRU.2017.8269008},
  abstract = {Unsupervised segmentation and clustering of unlabelled speech are core problems in zero-resource speech processing. Most approaches lie at methodological extremes: some use probabilistic Bayesian models with convergence guarantees, while others opt for more efficient heuristic techniques. Despite competitive performance in previous work, the full Bayesian approach is difficult to scale to large speech corpora. We introduce an approximation to a recent Bayesian model that still has a clear objective function but improves efficiency by using hard clustering and segmentation rather than full Bayesian inference. Like its Bayesian counterpart, this embedded segmental K-means model (ES-KMeans) represents arbitrary-length word segments as fixed-dimensional acoustic word embeddings. We first compare ES-KMeans to previous approaches on common English and Xitsonga data sets (5 and 2.5 hours of speech): ES-KMeans outperforms a leading heuristic method in word segmentation, giving similar scores to the Bayesian model while being 5 times faster with fewer hyperparameters. However, its clusters are less pure than those of the other models. We then show that ES-KMeans scales to larger corpora by applying it to the 5 languages of the Zero Resource Speech Challenge 2017 (up to 45 hours), where it performs competitively compared to the challenge baseline.},
  archiveprefix = {arXiv},
  arxivid = {1703.08135},
  isbn = {978-1-5090-4788-8},
  file = {/home/marnix/Zotero/storage/EED3RRD4/Kamper2017 Bayesian approaches to zero-resource speech.pdf}
}

@article{Karas,
  title = {Systemic Illiquidity in the {{Russian}} Interbank Market {{Alexei Karas Koen Schoors Background}}  {{Russia}} Faced 3 Severe Interbank Market Crises -- {{Systemic}} Instability},
  author = {Karas, Alexei and Lanine, Gleb and Schoors, Koen},
  file = {/home/marnix/Zotero/storage/QQ85YMUJ/Karas, Lanine, Schoors - Unknown - Systemic illiquidity in the Russian interbank market Alexei Karas Koen Schoors Background  Russia.pdf}
}

@article{Karas2005,
  title = {Heracles or {{Sisyfus}}? {{Finding}}, {{Cleaning}} and {{Reconstructing}} a {{Database}} of {{Russian Banks}}},
  author = {Karas, Alexei and Schoors, Koen J L},
  year = {2005},
  publisher = {Gent University Working Paper}
}

@article{Karas2008,
  title = {Russian {{Interbank Market}}},
  author = {Karas, Alexei},
  year = {2008},
  file = {/home/marnix/Zotero/storage/3BKCQUFN/Karas - 2008 - Russian Interbank Market.pdf}
}

@article{Karas2008a,
  title = {Liquidity Matters: {{Evidence}} from the {{Russian}} Interbank Market},
  author = {Karas, A. and Schoors, K. and Lanine, G.},
  year = {2008},
  journal = {Ghent University Working Paper Series},
  volume = {520},
  abstract = {We suggest a new transmission channel of contagion on the interbank market, namely the liquidity channel. We apply this idea to the Russian banking sector and .nd that the liquidity channel contributes signi.cantly to a better understanding and prediction of actual interbank market crises. Interbank market stability Granger causes the interbank market struc- ture, while the opposite causality is rejected. This emboldens the case for viewing the interbank market structure as endogenous. The results corroborate the thesis that prudential regulation at individual bank level is insu¢ cient to prevent systemic crises. We demonstrate that liquidity injections of a classical LOLR can e¤ectively mitigate coordination fail- ures on the interbank market not only in theory, but also in practice. In short: liquidity matters.},
  isbn = {9789524629294},
  keywords = {contagion,interbank market stability,lender of last resort,liquidity channel,Russia},
  file = {/home/marnix/Zotero/storage/K2IAV9SJ/Karas, Schoors, Lanine - 2008 - Liquidity matters Evidence from the Russian interbank market.pdf}
}

@article{Karas2010,
  title = {A {{Guide}} to {{Russian Banks Data}}},
  author = {Karas, Alexei and Schoors, Koen},
  year = {2010},
  journal = {SSRN: http://ssrn.com/paper-1658468},
  file = {/home/marnix/Zotero/storage/MSYUMW5F/Karas, Schoors - 2010 - A Guide to Russian Banks Data.pdf}
}

@article{Karas2012,
  title = {Bank Networks, Interbank Liquidity Runs and the Identification of Banks That Are Too Interconnected to Fail},
  author = {Karas, Alexei and Schoors, Koen and {Others}},
  year = {2012},
  journal = {Second CInST Banking Workshop, Moscow}
}

@article{Karrer2011,
  title = {Stochastic Blockmodels and Community Structure in Networks},
  author = {Karrer, Brian and Newman, M. E. J.},
  year = {2011},
  month = jan,
  journal = {Phys. Rev. E},
  volume = {83},
  number = {1},
  eprint = {1008.3926},
  pages = {16107},
  publisher = {American Physical Society},
  issn = {15393755},
  doi = {10.1103/PhysRevE.83.016107},
  abstract = {Stochastic blockmodels have been proposed as a tool for detecting community structure in networks as well as for generating synthetic networks for use as benchmarks. Most blockmodels, however, ignore variation in vertex degree, making them unsuitable for applications to real-world networks, which typically display broad degree distributions that can significantly distort the results. Here we demonstrate how the generalization of blockmodels to incorporate this missing element leads to an improved objective function for community detection in complex networks. We also propose a heuristic algorithm for community detection using this objective function or its non-degree-corrected counterpart and show that the degree-corrected version dramatically outperforms the uncorrected one in both real-world and synthetic networks.},
  archiveprefix = {arXiv},
  arxivid = {1008.3926},
  isbn = {1539-3755},
  pmid = {21405744},
  file = {/home/marnix/Zotero/storage/VUCZYSYI/Karrer, Newman - 2011 - Stochastic blockmodels and community structure in networks.pdf}
}

@article{Kass1998,
  title = {Markov {{Chain Monte Carlo}} in {{Practice}}: {{A Roundtable Discussion}}},
  shorttitle = {Markov {{Chain Monte Carlo}} in {{Practice}}},
  author = {Kass, Robert E. and Carlin, Bradley P. and Gelman, Andrew and Neal, Radford M.},
  year = {1998},
  month = may,
  journal = {The American Statistician},
  volume = {52},
  number = {2},
  pages = {93--100},
  issn = {0003-1305},
  doi = {10.1080/00031305.1998.10480547},
  urldate = {2019-03-14},
  abstract = {Markov chain Monte Carlo (MCMC) methods make possible the use of flexible Bayesian models that would otherwise be computationally infeasible. In recent years, a great variety of such applications have been described in the literature. Applied statisticians who are new to these methods may have several questions and concerns, however: How much effort and expertise are needed to design and use a Markov chain sampler? How much confidence can one have in the answers that MCMC produces? How does the use of MCMC affect the rest of the model-building process? At the Joint Statistical Meetings in August, 1996, a panel of experienced MCMC users discussed these and other issues, as well as various ``tricks of the trade'' This article is an edited recreation of that discussion. Its purpose is to offer advice and guidance to novice users of MCMC---and to not-so-novice users as well. Topics include building confidence in simulation results, methods for speeding and assessing convergence, estimating standard errors, identification of models for which good MCMC algorithms exist, and the current state of software development.},
  file = {/home/marnix/Zotero/storage/HI3IWXR4/Kass et al. - 1998 - Markov Chain Monte Carlo in Practice A Roundtable.pdf;/home/marnix/Zotero/storage/MSM9IDD9/00031305.1998.html}
}

@inproceedings{Kay1991,
  title = {Spectral Analysis Based on the Canonical Autoregressive Decomposition},
  booktitle = {[{{Proceedings}}] {{ICASSP}} 91: 1991 {{International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Kay, S. M. and Nagesha, V.},
  year = {1991},
  month = apr,
  pages = {3137-3140 vol.5},
  doi = {10.1109/ICASSP.1991.150120},
  abstract = {Time series modeling as the sum of an autoregressive (AR) process and sinusoids is proposed. When the AR model order is infinite, it is called the canonical autoregressive decomposition and is equivalent to the Wold decomposition. Maximum likelihood estimation of the sinusoidal and AR parameters is shown to require minimization with respect to only the unknown frequencies. Although the estimation problem is nonlinear in the sinusoidal amplitudes and AR parameters, it is reduced to a linear least-squares problem by using a nonlinear parameter transformation. Similar results are derived for AR processes in polynomial or polynomial-times-exponential signals. Applications include frequency estimation/transient analysis in unknown colored noise.{$<<$}ETX{$>>$}},
  keywords = {Autoregressive processes,canonical autoregressive decomposition,Contracts,Frequency estimation,frequency estimation/transient analysis,Least squares methods,linear least-squares problem,Maximum likelihood estimation,Narrowband,nonlinear parameter transformation,parameter estimation,polynomial-times-exponential signals,polynomials,Polynomials,Random processes,Signal processing,sinusoidal amplitudes,spectral analysis,Spectral analysis,time series,time series modeling,unknown colored noise,Wold decomposition},
  file = {/home/marnix/Zotero/storage/5MCA2FZV/Kay and Nagesha - 1991 - Spectral analysis based on the canonical autoregre.pdf;/home/marnix/Zotero/storage/WCBJ2Z6F/150120.html}
}

@article{Kazlauskaite2018,
  title = {Gaussian Process Latent Variable Alignment Learning},
  author = {Kazlauskaite, Ieva and Ek, Carl Henrik and Campbell, Neill DF},
  year = {2018},
  journal = {arXiv preprint arXiv:1803.02603},
  eprint = {1803.02603},
  archiveprefix = {arXiv},
  file = {/home/marnix/Zotero/storage/MQTSRK44/Kazlauskaite et al. - 2018 - Gaussian process latent variable alignment learnin.pdf}
}

@article{Kelly2014,
  title = {Flexible and {{Scalable Methods}} for {{Quantifying Stochastic Variability}} in the {{Era}} of {{Massive Time-Domain Astronomical Data Sets}}},
  author = {Kelly, Brandon C. and Becker, Andrew C. and Sobolewska, Malgosia and Siemiginowska, Aneta and Uttley, Phil},
  year = {2014},
  month = may,
  journal = {The Astrophysical Journal},
  volume = {788},
  number = {1},
  eprint = {1402.5978},
  primaryclass = {astro-ph},
  pages = {33},
  issn = {0004-637X, 1538-4357},
  doi = {10.1088/0004-637X/788/1/33},
  urldate = {2025-08-27},
  abstract = {We present the use of continuous-time autoregressive moving average (CARMA) models as a method for estimating the variability features of a light curve, and in particular its power spectral density (PSD). CARMA models fully account for irregular sampling and measurement errors, making them valuable for quantifying variability, forecasting and interpolating light curves, and for variability-based classification. We show that the PSD of a CARMA model can be expressed as a sum of Lorentzian functions, which makes them extremely flexible and able to model a broad range of PSDs. We present the likelihood function for light curves sampled from CARMA processes, placing them on a statistically rigorous foundation, and we present a Bayesian method to infer the probability distribution of the PSD given the measured lightcurve. Because calculation of the likelihood function scales linearly with the number of data points, CARMA modeling scales to current and future massive time-domain data sets. We conclude by applying our CARMA modeling approach to light curves for an X-ray binary, two AGN, a long-period variable star, and an RR-Lyrae star, in order to illustrate their use, applicability, and interpretation.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
  file = {/home/marnix/Zotero/storage/GTELWLQQ/Kelly et al. - 2014 - Flexible and Scalable Methods for Quantifying Stochastic Variability in the Era of Massive Time-Doma.pdf;/home/marnix/Zotero/storage/USSBEAG6/1402.html}
}

@article{Kent2018,
  title = {Static Measurements of Vowel Formant Frequencies and Bandwidths: {{A}} Review},
  shorttitle = {Static Measurements of Vowel Formant Frequencies and Bandwidths},
  author = {Kent, Raymond D. and Vorperian, Houri K.},
  year = {2018},
  month = jul,
  journal = {Journal of Communication Disorders},
  volume = {74},
  pages = {74--97},
  issn = {00219924},
  doi = {10.1016/j.jcomdis.2018.05.004},
  urldate = {2019-04-29},
  langid = {english},
  file = {/home/marnix/Zotero/storage/GSNTBPF4/Kent and Vorperian - 2018 - Static measurements of vowel formant frequencies a.pdf}
}

@misc{Khan2022,
  title = {The {{Bayesian Learning Rule}}},
  author = {Khan, Mohammad Emtiyaz and Rue, H{\aa}vard},
  year = {2022},
  month = mar,
  number = {arXiv:2107.04562},
  eprint = {2107.04562},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.04562},
  urldate = {2022-07-19},
  abstract = {We show that many machine-learning algorithms are specific instances of a single algorithm called the Bayesian learning rule. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, RMSprop, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/8LBCTQR3/Khan and Rue - 2022 - The Bayesian Learning Rule.pdf;/home/marnix/Zotero/storage/MA465X5W/2107.html}
}

@article{Khashanah2016,
  title = {Network Theory and Behavioral Finance in a Heterogeneous Market Environment},
  author = {Khashanah, Khaldoun and Alsulaiman, Talal},
  year = {2016},
  journal = {Complexity},
  volume = {21},
  number = {S2},
  pages = {530--554},
  issn = {1099-0526},
  doi = {10.1002/cplx.21834},
  keywords = {agent-based simulation,artificial stock market,calibration,financial networks,investor behavior,meta-model,sensitivity analysis,validation}
}

@article{King2007,
  title = {Speech Production Knowledge in Automatic Speech Recognition},
  author = {King, Simon and Frankel, Joe and Livescu, Karen and McDermott, Erik and Richmond, Korin and Wester, Mirjam},
  year = {2007},
  journal = {The Journal of the Acoustical Society of America},
  volume = {121},
  number = {2},
  pages = {723--742},
  issn = {0001-4966},
  doi = {10.1121/1.2404622},
  abstract = {Although much is known about how speech is produced, and research into speech production has resulted in measured articulatory data, feature systems of different kinds, and numerous models, speech production knowledge is almost totally ignored in current mainstream approaches to automatic speech recognition. Representations of speech production allow simple explanations for many phenomena observed in speech which cannot be easily analyzed from either acoustic signal or phonetic transcription alone. In this article, a survey of a growing body of work in which such representations are used to improve automatic speech recognition is provided.},
  pmid = {17348495},
  keywords = {articulatory features,articulatory inversion,automatic speech recognition,speech production},
  file = {/home/marnix/Zotero/storage/DPH83SVS/King2007 Speech production knowledge in automatic speech recognition.pdf}
}

@article{Kivela2014,
  title = {Multilayer Networks},
  author = {Kivel{\"a}, Mikko and Arenas, Alex and Barthelemy, Marc and Gleeson, James P and Moreno, Yamir and Porter, Mason A},
  year = {2014},
  month = sep,
  journal = {Journal of Complex Networks},
  volume = {2},
  number = {3},
  pages = {203--271},
  publisher = {Oxford University Press},
  issn = {2051-1329},
  doi = {10.1093/comnet/cnu016},
  abstract = {In most natural and engineered systems, a set of entities interact with each other in complicated patterns that can encompass multiple types of relationships, change in time and include other types of complications. Such systems include multiple subsystems and layers of connectivity, and it is important to take such 'multilayer' features into account to try to improve our understanding of complex systems. Consequently, it is necessary to generalize 'traditional' network theory by developing (and validating) a framework and associated tools to study multilayer systems in a comprehensive fashion. The origins of such efforts date back several decades and arose in multiple disciplines, and now the study of multilayer networks has become one of the most important directions in network science. In this paper, we discuss the history of multilayer networks (and related concepts) and review the exploding body of work on such networks. To unify the disparate terminology in the large body of recent work, we discuss a general framework for multilayer networks, construct a dictionary of terminology to relate the numerous existing concepts to each other and provide a thorough discussion that compares, contrasts and translates between related notions such as multilayer networks, multiplex networks, interdependent networks, networks of networks and many others. We also survey and discuss existing data sets that can be represented as multilayer networks. We review attempts to generalize single-layer-network diagnostics to multilayer networks. We also discuss the rapidly expanding research on multilayer-network models and notions like community structure, connected components, tensor decompositions and various types of dynamical processes on multilayer networks. We conclude with a summary and an outlook.},
  keywords = {multilayer},
  file = {/home/marnix/Zotero/storage/LNYXPBKG/Kivelä et al. - 2014 - Multilayer networks.pdf}
}

@article{Klatt1980,
  title = {Software for a Cascade/Parallel Formant Synthesizer},
  author = {Klatt, Dennis H.},
  year = {1980},
  month = mar,
  journal = {The Journal of the Acoustical Society of America},
  volume = {67},
  number = {3},
  pages = {971--995},
  issn = {0001-4966},
  doi = {10.1121/1.383940},
  urldate = {2019-09-10},
  langid = {english},
  file = {/home/marnix/Zotero/storage/TD5DYK87/Klatt - 1980 - Software for a cascadeparallel formant synthesize.pdf}
}

@article{Klatt1986,
  title = {Representation of the First Formant in Speech Recognition and in Models of the Auditory Periphery},
  author = {Klatt, Dennis H},
  year = {1986},
  journal = {Canadian Acoustics},
  volume = {14},
  number = {3 bis},
  pages = {5--7},
  file = {/home/marnix/Zotero/storage/XX3JEF86/Klatt - 1986 - Representation of the first formant in speech reco.pdf}
}

@article{Kleibergen2000,
  title = {Bayesian Analysis of {{ARMA}} Models},
  author = {Kleibergen, Frank R and Hoek, Henk},
  year = {2000},
  file = {/home/marnix/Zotero/storage/Y8GS6ELK/Kleibergen and Hoek - 2000 - Bayesian analysis of ARMA models.pdf}
}

@book{Kline1980,
  title = {Mathematics: {{The Loss}} of {{Certainty}}},
  author = {Kline, Morris},
  year = {1980},
  file = {/home/marnix/Zotero/storage/XC82W62A/Kline1980 Mathematics The Loss of Certainty (from BookFi).djvu}
}

@article{Knuth2012,
  title = {Foundations of {{Inference}}},
  author = {Knuth, Kevin H. and Skilling, John},
  year = {2012},
  month = jun,
  journal = {Axioms},
  volume = {1},
  number = {1},
  pages = {38--73},
  issn = {2075-1680},
  doi = {10.3390/axioms1010038},
  urldate = {2019-11-25},
  abstract = {We present a simple and clear foundation for finite inference that unites and significantly extends the approaches of Kolmogorov and Cox. Our approach is based on quantifying lattices of logical statements in a way that satisfies general lattice symmetries. With other applications such as measure theory in mind, our derivations assume minimal symmetries, relying on neither negation nor continuity nor differentiability. Each relevant symmetry corresponds to an axiom of quantification, and these axioms are used to derive a unique set of quantifying rules that form the familiar probability calculus. We also derive a unique quantification of divergence, entropy and information.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/ADPX76JE/Knuth and Skilling - 2012 - Foundations of Inference.pdf}
}

@article{Knuth2015,
  title = {Bayesian Evidence and Model Selection},
  author = {Knuth, Kevin H. and Habeck, Michael and Malakar, Nabin K. and Mubeen, Asim M. and Placek, Ben},
  year = {2015},
  month = dec,
  journal = {Digital Signal Processing},
  series = {Special {{Issue}} in {{Honour}} of {{William J}}. ({{Bill}}) {{Fitzgerald}}},
  volume = {47},
  pages = {50--67},
  issn = {1051-2004},
  doi = {10.1016/j.dsp.2015.06.012},
  urldate = {2021-03-19},
  abstract = {In this paper we review the concepts of Bayesian evidence and Bayes factors, also known as log odds ratios, and their application to model selection. The theory is presented along with a discussion of analytic, approximate and numerical techniques. Specific attention is paid to the Laplace approximation, variational Bayes, importance sampling, thermodynamic integration, and nested sampling and its recent variants. Analogies to statistical physics, from which many of these techniques originate, are discussed in order to provide readers with deeper insights that may lead to new techniques. The utility of Bayesian model testing in the domain sciences is demonstrated by presenting four specific practical examples considered within the context of signal processing in the areas of signal detection, sensor characterization, scientific model selection and molecular force characterization.},
  langid = {english},
  keywords = {Bayesian evidence,Bayesian signal processing,Model testing,Nested sampling,Odds ratio},
  file = {/home/marnix/Zotero/storage/P8PLALFR/Knuth et al. - 2015 - Bayesian evidence and model selection.pdf;/home/marnix/Zotero/storage/PQRZN7TQ/S1051200415001980.html}
}

@book{Koller2009,
  title = {Probabilistic {{Graphical Models}}: {{Principles}} and {{Techniques}}},
  booktitle = {Foundations},
  author = {Koller, Daphne and Friedman, Nir},
  year = {2009},
  volume = {2009},
  issn = {15582264},
  doi = {10.1016/j.ccl.2010.07.006},
  abstract = {Most tasks require a person or an automated system to reasonto reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality.Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty.The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss empirical cases related to the approach described in the text, including applications in computer vision, robotics, natural language understanding, and computational biology; and concept boxes, which present significant concepts drawn from the material in the chapter. Instructors (and readers) can group chapters in various combinations, from core topics to more technically advanced material, to suit their particular needs.Adaptive Computation and Machine Learning series},
  isbn = {0-262-01319-3},
  pmid = {20937442}
}

@inproceedings{Kominek2004,
  title = {The {{CMU Arctic}} Speech Databases},
  booktitle = {Fifth {{ISCA}} Workshop on Speech Synthesis},
  author = {Kominek, John and Black, Alan W},
  year = {2004},
  file = {/home/marnix/Zotero/storage/36DGQRN7/Kominek and Black - 2004 - The CMU Arctic speech databases.pdf}
}

@misc{Korbak2023,
  title = {Pretraining {{Language Models}} with {{Human Preferences}}},
  author = {Korbak, Tomasz and Shi, Kejian and Chen, Angelica and Bhalerao, Rasika and Buckley, Christopher L. and Phang, Jason and Bowman, Samuel R. and Perez, Ethan},
  year = {2023},
  month = jun,
  number = {arXiv:2302.08582},
  eprint = {2302.08582},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-14},
  abstract = {Language models (LMs) are pretrained to imitate internet text, including content that would violate human preferences if generated by an LM: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, and more. Here, we explore alternative objectives for pretraining LMs in a way that also guides them to generate text aligned with human preferences. We benchmark five objectives for pretraining with human feedback across three tasks and study how they affect the trade-off between alignment and capabilities of pretrained LMs. We find a Pareto-optimal and simple approach among those we explored: conditional training, or learning distribution over tokens conditional on their human preference scores given by a reward model. Conditional training reduces the rate of undesirable content by up to an order of magnitude, both when generating without a prompt and with an adversarially-chosen prompt. Moreover, conditional training maintains the downstream task performance of standard LM pretraining, both before and after task-specific finetuning. Pretraining with human feedback results in much better preference satisfaction than standard LM pretraining followed by finetuning with feedback, i.e., learning and then unlearning undesirable behavior. Our results suggest that we should move beyond imitation learning when pretraining LMs and incorporate human preferences from the start of training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/marnix/Zotero/storage/LUBF3NK9/Korbak et al. - 2023 - Pretraining Language Models with Human Preferences.pdf;/home/marnix/Zotero/storage/I8UUAGXV/2302.html}
}

@article{Koriyama2020,
  title = {An Introduction of {{Gaussian}} Processes and Deep {{Gaussian}} Processes and Their Applications to Speech Processing},
  author = {Koriyama, Tomoki},
  year = {2020},
  month = mar,
  journal = {Acoustical Science and Technology},
  volume = {41},
  number = {2},
  pages = {457--464},
  issn = {1346-3969, 1347-5177},
  doi = {10.1250/ast.41.457},
  urldate = {2020-04-24},
  abstract = {Gaussian process (GP) is a distribution of functions, which can be used for a machine learning framework. GP regression has characteristics of Bayesian model, which can predict uncertainty of outputs, and kernel methods, which enables nonlinear function with a small number of parameters. In this paper, we first describe the basic of GP regression, and introduce recent notable advances of GP. Specifically, we focus on stochastic variational GP that is an approximation method available for a huge amount of training data, and explain a GP-based deep architecture model called deep Gaussian process. Since GP regression is a general-purpose machine learning framework, there are many applications. In this paper, we introduce GP-based applications to speech information processing including speech synthesis.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/INXTLEYG/Koriyama - 2020 - An introduction of Gaussian processes and deep Gau.pdf}
}

@inproceedings{Kotek2023,
  title = {Gender Bias and Stereotypes in {{Large Language Models}}},
  booktitle = {Proceedings of {{The ACM Collective Intelligence Conference}}},
  author = {Kotek, Hadas and Dockum, Rikker and Sun, David},
  year = {2023},
  month = nov,
  series = {{{CI}} '23},
  pages = {12--24},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3582269.3615599},
  urldate = {2023-11-27},
  abstract = {Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs' behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women's occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person's gender; (b) these choices align with people's perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in sentence structure 95\% of the time in our study items, but when explicitly prompted, they recognize the ambiguity; (e) LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior. This highlights a key property of these models: LLMs are trained on imbalanced datasets; as such, even with the recent successes of reinforcement learning with human feedback, they tend to reflect those imbalances back at us. As with other types of societal biases, we suggest that LLMs must be carefully tested to ensure that they treat minoritized individuals and communities equitably.},
  isbn = {979-8-4007-0113-9},
  keywords = {bias,ethics,explanations,gender,large language models,occupations,stereotypes},
  file = {/home/marnix/Zotero/storage/7WEZDVUJ/Kotek et al. - 2023 - Gender bias and stereotypes in Large Language Mode.pdf}
}

@article{Kreiman2007,
  title = {Measures of the {{Glottal Source Spectrum}}},
  author = {Kreiman, Jody and Gerratt, Bruce R. and {Anto{\~n}anzas-Barroso}, Norma},
  year = {2007},
  month = jun,
  journal = {Journal of Speech, Language, and Hearing Research},
  volume = {50},
  number = {3},
  pages = {595--610},
  issn = {1092-4388, 1558-9102},
  doi = {10.1044/1092-4388(2007/042)},
  urldate = {2025-10-09},
  abstract = {Purpose: Many researchers have studied the acoustics, physiology, and perceptual characteristics of the voice source, but despite significant attention, it remains unclear which aspects of the source should be quantified and how measurements should be made. In this study, the authors examined the relationships among a number of existing measures of the glottal source spectrum, along with the association of these measures to overall spectral shapes and to glottal pulse shapes, to determine which measures of the source best capture information about the shapes of glottal pulses and glottal source spectra. Method: Seventy-eight different measures of source spectral shapes were made on the voices of 70 speakers. Principal components analysis was applied to measurement data, and the resulting factors were compared with factors similarly derived from oral speech spectra and glottal pulses. Results: Results revealed high levels of duplication and overlap among existing measures of source spectral slope. Further, existing measures were not well aligned with patterns of spectral variability. In particular, existing spectral measures do not appear to model the higher frequency parts of the source spectrum adequately. Conclusion: The failure of existing measures to adequately quantify spectral variability may explain why results of studies examining the perceptual importance of spectral slope have not produced consistent results. Because variability in the speech signal is often perceptually salient, these results suggest that most existing measures of source spectral slope are unlikely to be good predictors of voice quality.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/R63CXBPF/Kreiman et al. - 2007 - Measures of the Glottal Source Spectrum.pdf}
}

@article{Krijnders2010,
  title = {Sound Event Recognition through Expectancy-Based Evaluation of Signal-Driven Hypotheses},
  author = {Krijnders, J. D. and Niessen, M. E. and Andringa, T. C.},
  year = {2010},
  journal = {Pattern Recognition Letters},
  volume = {31},
  number = {12},
  pages = {1552--1559},
  publisher = {Elsevier B.V.},
  issn = {01678655},
  doi = {10.1016/j.patrec.2009.11.004},
  abstract = {A recognition system for environmental sounds is presented. Signal-driven classification is performed by applying machine-learning techniques on features extracted from a cochleogram. These possibly unreliable classifications are improved by creating expectancies of sound events based on context information. {\copyright} 2009 Elsevier B.V. All rights reserved.},
  keywords = {Audio signal processing,Bottom-up and top-down processing,Environmental sound recognition,Expectancy-based checking,Signal-driven processing,Spreading activation network}
}

@article{Krstulovic1999,
  title = {{{LPC-based}} Inversion of the {{DRM}} Articulatory Model},
  author = {Krstulovic, S},
  year = {1999},
  journal = {Proceedings of the 7th European Conference on Speech Communication and Technology (\{EUROSPEECH\})},
  volume = {1},
  pages = {125--128},
  file = {/home/marnix/Zotero/storage/YTRPUZZA/Krstulovic1999 LPC-based inversion.pdf}
}

@article{Kuipers2015,
  title = {Uniform Random Generation of Large Acyclic Digraphs},
  author = {Kuipers, Jack and Moffa, Giusi},
  year = {2015},
  month = mar,
  journal = {Statistics and Computing},
  volume = {25},
  number = {2},
  pages = {227--242},
  issn = {1573-1375},
  doi = {10.1007/s11222-013-9428-y},
  urldate = {2022-09-17},
  abstract = {Directed acyclic graphs are the basic representation of the structure underlying Bayesian networks, which represent multivariate probability distributions. In many practical applications, such as the reverse engineering of gene regulatory networks, not only the estimation of model parameters but the reconstruction of the structure itself is of great interest. As well as for the assessment of different structure learning algorithms in simulation studies, a~uniform sample from the space of directed acyclic graphs is required to evaluate the prevalence of certain structural features. Here we analyse how to sample acyclic digraphs uniformly at random through recursive enumeration, an approach previously thought too computationally involved. Based on complexity considerations, we discuss in particular how the enumeration directly provides an exact method, which avoids the convergence issues of the alternative Markov chain methods and is actually computationally much faster. The limiting behaviour of the distribution of acyclic digraphs then allows us to sample arbitrarily large graphs. Building on the ideas of recursive enumeration based sampling we also introduce a novel hybrid Markov chain with much faster convergence than current alternatives while still being easy to adapt to various restrictions. Finally we discuss how to include such restrictions in the combinatorial enumeration and the new hybrid Markov chain method for efficient uniform sampling of the corresponding graphs.},
  langid = {english},
  keywords = {Acyclic digraphs,Bayesian networks,MCMC,Random graph generation,Recursive enumeration},
  file = {/home/marnix/Zotero/storage/PBNVX2X3/Kuipers and Moffa - 2015 - Uniform random generation of large acyclic digraph.pdf}
}

@book{Ladefoged1996,
  title = {Elements of Acoustic Phonetics},
  author = {Ladefoged, Peter},
  year = {1996},
  publisher = {University of Chicago Press},
  file = {/home/marnix/Zotero/storage/ISEYE2NA/Ladefoged - 1996 - Elements of acoustic phonetics.pdf}
}

@article{Ladyman2011,
  title = {What Is a Complex System ?},
  author = {Ladyman, James; and Lambert, James; and Wiesner, Karoline},
  year = {2011},
  journal = {European Journal for philosophy of Science},
  volume = {3},
  number = {1},
  pages = {33--67},
  publisher = {Springer},
  issn = {1879-4912},
  doi = {10.1007/s13194-012-0056-8},
  abstract = {Complex systems research is becoming ever more important in both the natural and social sciences. It is commonly implied that there is such a thing as a complex system, different examples of which are studied across many disciplines. However, there is no concise definition of a complex system, let alone a definition on which all scientists agree. We review various attempts to characterize a complex system, and consider a core set of features that are widely associated with complex systems in the literature and by those in the field. We argue that some of these features are neither necessary nor sufficient for complexity, and that some of them are too vague or confused to be of any analytical use. In order to bring mathematical rigour to the issue we then review some standard measures of complexity from the scientific literature, and offer a taxonomy for them, before arguing that the one that best captures the qualitative notion of the order produced by complex systems is that of the Statistical Complexity. Finally, we offer our own list of necessary conditions as a characterization of complexity. These conditions are qualitative and may not be jointly sufficient for complexity. We close with some suggestions for future work.},
  isbn = {1319401200568},
  file = {/home/marnix/Zotero/storage/ZBGR9IF3/Ladyman, Lambert, Wiesner - 2011 - What is a complex system.pdf}
}

@inproceedings{Laine1982,
  title = {Modelling of Lip Radiation Impedance in {{Z-domain}}},
  booktitle = {{{ICASSP}} '82. {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Laine, U.},
  year = {1982},
  month = may,
  volume = {7},
  pages = {1992--1995},
  doi = {10.1109/ICASSP.1982.1171841},
  abstract = {Three z-domain models for lip radiaton impedance are introduced. Two of them are based on the observation that the normalized acoustic impedance can be modelled asz({\o}mega) = C*[1.-{\textbackslash}cos({\o}megaT)] + j*B*{\textbackslash}sin({\o}megaT). The values of the parameters C and B for different sampling frequencies and radiation areas are found by minimizing the mean square error (MSE) between the modelled and the acoustic impedance. Owing to the sine and cosine functions used, the modelled impedance can be transformed exactly and straightforwardly into the z-domain. The third model described is a pole-zero-model, the parameters of which are also optimized by MSE-criterion. The limits for acceptable errors of the modelled impedances are studied and the models compared to the well known Flanagan's model.},
  keywords = {Acoustics,Digital simulation,Equations,Frequency estimation,Impedance,Mathematical model,Mouth,Pistons,Speech,Transforms},
  file = {/home/marnix/Zotero/storage/JRZAGAUQ/Laine - 1982 - Modelling of LIP radiation impedance in Z-domain.pdf;/home/marnix/Zotero/storage/L4HBXIW8/1171841.html}
}

@article{Landau1983,
  title = {The {{Inverse Problem}} for the {{Vocal Tract}} and the {{Moment Problem}}},
  author = {Landau, H J},
  year = {1983},
  journal = {SIAM Journal on Mathematical Analysis},
  volume = {14},
  number = {5},
  pages = {1019--1035},
  doi = {10.1137/0514082},
  file = {/home/marnix/Zotero/storage/WAAILW72/Landau - 1983 - The Inverse Problem for the Vocal Tract and the Mo.pdf}
}

@article{Langfield2014a,
  title = {Mapping the {{UK}} Interbank System},
  author = {Langfield, Sam and Liu, Zijun and Ota, Tomohiro},
  year = {2014},
  journal = {Journal of Banking \& Finance},
  volume = {45},
  pages = {288--303},
  publisher = {Elsevier}
}

@article{Langfield2016,
  title = {Interbank {{Exposure Networks}}},
  author = {Langfield, Sam and Soram{\"a}ki, Kimmo},
  year = {2016},
  journal = {Computational Economics},
  volume = {47},
  number = {1},
  pages = {3--17},
  issn = {15729974},
  doi = {10.1007/s10614-014-9443-x},
  abstract = {Financial institutions are highly interconnected. Consequently, they form{\textbackslash}ncomplex systemswhich are inherently unstable. This paper reviews empirical research{\textbackslash}non the instability of complex interbank systems. Three network approaches are distinguished:{\textbackslash}ndescriptions of interbank exposure networks; simulation and modelling;{\textbackslash}nand the development of new metrics to describe network topology and individual{\textbackslash}nbanks' relative importance. The paper concludes by inferring policy implications and{\textbackslash}npriorities for future research.},
  isbn = {9780123978752},
  file = {/home/marnix/Zotero/storage/LAR9LYSU/Langfield2016 Interbank exposure networks.pdf}
}

@article{Lataire2016,
  title = {Transfer Function and Transient Estimation by {{Gaussian}} Process Regression in the Frequency Domain},
  author = {Lataire, John and Chen, Tianshi},
  year = {2016},
  month = oct,
  journal = {Automatica},
  volume = {72},
  pages = {217--229},
  issn = {00051098},
  doi = {10.1016/j.automatica.2016.06.009},
  urldate = {2021-11-26},
  abstract = {Inspired by the recent promising developments of Bayesian learning techniques in the context of system identification, this paper proposes a Transfer Function estimator, based on Gaussian process regression. Contrary to existing kernel-based impulse response estimators, a frequency domain approach is adopted. This leads to a formulation and implementation which is seamlessly valid for both continuous- and discrete-time systems, and which conveniently enables the selection of the frequency band of interest. A pragmatic approach is proposed in an output error framework, from sampled input and output data. The transient is dealt with by estimating it simultaneously with the transfer function.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/M64WHSY3/Lataire and Chen - 2016 - Transfer function and transient estimation by Gaus.pdf}
}

@article{Laukkonen2025,
  title = {A Beautiful Loop: {{An}} Active Inference Theory of Consciousness},
  author = {Laukkonen, Ruben and Friston, Karl and Chandaria, Shamil},
  year = {2025},
  journal = {Neuroscience \& Biobehavioral Reviews},
  pages = {106296},
  publisher = {Elsevier},
  file = {/home/marnix/Zotero/storage/24M4YDPK/Laukkonen and Chandaria - An active inference theory of consciousness.pdf}
}

@article{Laurens2008,
  title = {Complex {{Systems}} in {{Financial Economics Applications}} to {{Interbank}} and {{Stock Markets}}},
  author = {Laurens, Daan},
  year = {2008},
  journal = {2015},
  file = {/home/marnix/Zotero/storage/7PYYZRGN/Laurens - 2008 - Complex Systems in Financial Economics Applications to Interbank and Stock Markets.pdf}
}

@article{Lauri1997,
  title = {Effects of {{Prolonged Oral Reading}} on {{Time-Based Glottal Flow Waveform Parameters}} with {{Special Reference}} to {{Gender Differences}}},
  author = {Lauri, Eija-Riita and Alku, Paavo and Vilkman, Erkki and Sala, Eeva and Sihvo, Marketta},
  year = {1997},
  journal = {Folia Phoniatrica et Logopaedica},
  volume = {49},
  number = {5},
  pages = {234--246},
  issn = {1421-9972, 1021-7762},
  doi = {10.1159/000266461},
  urldate = {2019-08-28},
  langid = {english},
  file = {/home/marnix/Zotero/storage/V96ANHL5/Lauri et al. - 1997 - Effects of Prolonged Oral Reading on Time-Based Gl.pdf}
}

@article{Laverny2021,
  title = {Estimation of Multivariate Generalized Gamma Convolutions through {{Laguerre}} Expansions},
  author = {Laverny, Oskar and Masiello, Esterina and {Maume-Deschamps}, V{\'e}ronique and Rulli{\`e}re, Didier},
  year = {2021},
  month = jul,
  journal = {arXiv:2103.03200 [math, stat]},
  eprint = {2103.03200},
  primaryclass = {math, stat},
  urldate = {2021-11-26},
  abstract = {The generalized gamma convolutions class of distributions appeared in Thorin's work while looking for the infinite divisibility of the log-Normal and Pareto distributions. Although these distributions have been extensively studied in the univariate case, the multivariate case and the dependence structures that can arise from it have received little interest in the literature. Furthermore, only one projection procedure for the univariate case was recently constructed, and no estimation procedures are available. By expanding the densities of multivariate generalized gamma convolutions into a tensorized Laguerre basis, we bridge the gap and provide performant estimation procedures for both the univariate and multivariate cases. We provide some insights about performance of these procedures, and a convergent series for the density of multivariate gamma convolutions, which is shown to be more stable than Moschopoulos's and Mathai's univariate series. We furthermore discuss some examples.},
  archiveprefix = {arXiv},
  keywords = {{62H12, 60E07 (Primary) 60E10 (Secondary)},Mathematics - Statistics Theory},
  file = {/home/marnix/Zotero/storage/JZJKPDCP/Laverny et al. - 2021 - Estimation of multivariate generalized gamma convo.pdf;/home/marnix/Zotero/storage/3WPJFXE5/2103.html}
}

@inproceedings{Lazaro-Gredilla2009,
  title = {Inter-Domain {{Gaussian Processes}} for {{Sparse Inference}} Using {{Inducing Features}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{L{\'a}zaro-Gredilla}, Miguel and {Figueiras-Vidal}, An{\'i}bal},
  year = {2009},
  volume = {22},
  publisher = {Curran Associates, Inc.},
  urldate = {2022-03-23},
  file = {/home/marnix/Zotero/storage/YTXYFCW3/Lázaro-Gredilla and Figueiras-Vidal - 2009 - Inter-domain Gaussian Processes for Sparse Inferen.pdf}
}

@incollection{LeBesnerais2010,
  title = {Inverse {{Filtering}} and {{Other Linear Methods}}},
  booktitle = {Bayesian {{Approach}} to {{Inverse Problems}}},
  author = {Le Besnerais, Guy and Giovannelli, Jean-Fran{\c c}ois},
  editor = {Idier, J{\'e}r{\^o}me},
  year = {2010},
  month = jan,
  pages = {80--116},
  publisher = {ISTE},
  address = {London, UK},
  doi = {10.1002/9780470611197.ch4},
  urldate = {2019-08-08},
  isbn = {978-0-470-61119-7 978-1-84821-032-5},
  file = {/home/marnix/Zotero/storage/PJTAWNTD/Le Besnerais and Giovannelli - 2010 - Inverse Filtering and Other Linear Methods.pdf}
}

@inproceedings{Lee1990,
  title = {On {{Bernoulli-Gaussian}} Process Modeling of Speech Excitation Source},
  booktitle = {International {{Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Lee, K.Y. and Lee, B.-G. and Song, I. and Ann, S.},
  year = {1990},
  month = apr,
  pages = {217-220 vol.1},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.1990.115577},
  abstract = {In the multipulse linear predictive coding (LPC) speech synthesis, an autoregressive filter is excited by a multipulse excitation consisting of impulses of various amplitudes and locations. The authors statistically model the excitation signal as a zero mean Bernoulli-Gaussian process. The pulse locations are independently distributed with a probability distribution, and the pulse amplitudes are expressed as a Gaussian sequence with zero mean and finite variance. An algorithm is described for estimation of pulse amplitudes and locations based on the Bernoulli-Gaussian process when the number of pulses is given. Results from computer simulation are presented.{$<>$}},
  keywords = {Amplitude estimation,AR filter,autoregressive filter,Bernoulli-Gaussian process modeling,computer simulation,Computer simulation,filtering and prediction theory,Gaussian sequence,linear predictive coding,Linear predictive coding,multipulse excitation,multipulse LPC,Nonlinear filters,probability,probability distribution,Probability distribution,pulse amplitudes,pulse locations,Signal processing,speech analysis and processing,speech excitation source,Speech processing,speech synthesis,Speech synthesis,statistical analysis,statistical model},
  file = {/home/marnix/Zotero/storage/VN9LBYK5/Lee et al. - 1990 - On Bernoulli-Gaussian process modeling of speech e.pdf;/home/marnix/Zotero/storage/CWVIG5ZG/115577.html}
}

@article{Lee2006,
  title = {Maximum Flow and Topological Structure of Complex Networks},
  author = {Lee, D S and Rieger, H},
  year = {2006},
  journal = {Europhysics Letters},
  volume = {73},
  number = {3},
  eprintclass = {cond-mat},
  pages = {471--477},
  issn = {02955075},
  doi = {10.1209/epl/i2005-10407-5},
  abstract = {The problem of sending the maximum amount of flow q between two arbitrary nodes s and t of complex networks along links with unit capacity is studied, which is equivalent to determining the number of link-disjoint paths between s and t. The average of q over all node pairs with smaller degree k(min) is [q]k(min) similar or equal to c k(min) for large k(min) with c a constant implying that the statistics of q is related to the degree distribution of the network. The disjoint paths between hub nodes are found to be distributed among the links belonging to the same edge-biconnected component, and q can be estimated by the number of pairs of edge-biconnected links incident to the start and terminal node. The relative size of the giant edge-biconnected component of a network approximates to the coefficient c. The applicability of our results to real-world networks is tested for the Internet at the autonomous system level.},
  arxiv = {0503008},
  arxivid = {cond-mat/0503008},
  isbn = {0295-5075},
  file = {/home/marnix/Zotero/storage/GT35ADYC/Lee, Rieger - 2006 - Maximum flow and topological structure of complex networks.pdf}
}

@article{Lee2012,
  title = {A {{Nonparametric Bayesian Approach}} to {{Acoustic Model Discovery}}},
  author = {Lee, Chia-ying and Glass, James},
  year = {2012},
  number = {July},
  pages = {40--49},
  file = {/home/marnix/Zotero/storage/I7BQ6ZTH/Lee2012 A nonparametric Bayesian approach to acoustic model discovery.pdf}
}

@misc{Leibfried2022,
  title = {A {{Tutorial}} on {{Sparse Gaussian Processes}} and {{Variational Inference}}},
  author = {Leibfried, Felix and Dutordoir, Vincent and John, S. T. and Durrande, Nicolas},
  year = {2022},
  month = dec,
  number = {arXiv:2012.13962},
  eprint = {2012.13962},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.13962},
  urldate = {2025-05-29},
  abstract = {Gaussian processes (GPs) provide a framework for Bayesian inference that can offer principled uncertainty estimates for a large range of problems. For example, if we consider regression problems with Gaussian likelihoods, a GP model enjoys a posterior in closed form. However, identifying the posterior GP scales cubically with the number of training examples and requires to store all examples in memory. In order to overcome these obstacles, sparse GPs have been proposed that approximate the true posterior GP with pseudo-training examples. Importantly, the number of pseudo-training examples is user-defined and enables control over computational and memory complexity. In the general case, sparse GPs do not enjoy closed-form solutions and one has to resort to approximate inference. In this context, a convenient choice for approximate inference is variational inference (VI), where the problem of Bayesian inference is cast as an optimization problem -- namely, to maximize a lower bound of the log marginal likelihood. This paves the way for a powerful and versatile framework, where pseudo-training examples are treated as optimization arguments of the approximate posterior that are jointly identified together with hyperparameters of the generative model (i.e. prior and likelihood). The framework can naturally handle a wide scope of supervised learning problems, ranging from regression with heteroscedastic and non-Gaussian likelihoods to classification problems with discrete labels, but also problems with multidimensional labels. The purpose of this tutorial is to provide access to the basic matter for readers without prior knowledge in both GPs and VI. A proper exposition to the subject enables also access to more recent advances (like importance-weighted VI as well as interdomain, multioutput and deep GPs) that can serve as an inspiration for new research ideas.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/6SILEVM3/Leibfried et al. - 2022 - A Tutorial on Sparse Gaussian Processes and Variational Inference.pdf;/home/marnix/Zotero/storage/9D53E584/2012.html}
}

@article{Leonidov2012,
  title = {Russian Interbank Networks: Main Characteristics and Stability with Respect to Contagion},
  author = {Leonidov, A. V. and Rumyantsev, E. L.},
  year = {2012},
  journal = {arXiv preprint arXiv:1210.3814},
  eprint = {1210.3814},
  pages = {0--6},
  abstract = {Systemic risks characterizing the Russian overnight interbank market from the network point of view are analyzed.},
  archiveprefix = {arXiv},
  arxivid = {1210.3814},
  file = {/home/marnix/Zotero/storage/IBLQMXTA/Leonidov, Rumyantsev - 2012 - Russian interbank networks main characteristics and stability with respect to contagion.pdf}
}

@article{Lepage2002,
  title = {Constrained {{Curve Fitting}}},
  author = {Lepage, G. P. and Clark, B. and Davies, C. T. H. and Hornbostel, K. and Mackenzie, P. B. and Morningstar, C. and Trottier, H.},
  year = {2002},
  month = mar,
  journal = {Nuclear Physics B - Proceedings Supplements},
  volume = {106--107},
  eprint = {hep-lat/0110175},
  pages = {12--20},
  issn = {09205632},
  doi = {10.1016/S0920-5632(01)01638-3},
  urldate = {2019-10-17},
  abstract = {We survey techniques for constrained curve fitting, based upon Bayesian statistics, that offer significant advantages over conventional techniques used by lattice field theorists.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {High Energy Physics - Lattice},
  file = {/home/marnix/Zotero/storage/JVYNTQG3/Lepage et al. - 2002 - Constrained Curve Fitting.pdf}
}

@article{Li2005,
  title = {Towards a Theory of Scale-Free Graphs: {{Definition}}, Properties, and Implications},
  author = {Li, Lun and Alderson, David and Doyle, John C. and Willinger, Walter},
  year = {2005},
  journal = {Internet Mathematics},
  volume = {2},
  number = {4},
  eprintclass = {cond-mat},
  pages = {431--523},
  issn = {15427951},
  doi = {10.1080/15427951.2005.10129111},
  abstract = {Although the ``scale-free'' literature is large and growing, it gives neither a precise definition of scale-free graphs nor rigorous proofs of many of their claimed properties. In fact, it is easily shown that the existing theory has many inherent contradictions and verifiably false claims. In this paper, we propose a new, mathematically precise, and structural definition of the extent to which a graph is scale-free, and prove a series of results that recover many of the claimed properties while suggesting the potential for a rich and interesting theory. With this definition, scale-free (or its opposite, scale-rich) is closely related to other structural graph properties such as various notions of self-similarity (or respectively, self-dissimilarity). Scale-free graphs are also shown to be the likely outcome of random construction processes, consistent with the heuristic definitions implicit in existing random graph approaches. Our approach clarifies much of the confusion surrounding the sensational qualitative claims in the scale-free literature, and offers rigorous and quantitative alternatives.},
  arxiv = {0501169},
  arxivid = {cond-mat/0501169},
  isbn = {1542-7951},
  file = {/home/marnix/Zotero/storage/ZZCXER7S/Li et al. - 2005 - Towards a theory of scale-free graphs Definition, properties, and implications.pdf}
}

@article{Li2012,
  title = {Digraph Laplacian and the Degree of Asymmetry},
  author = {Li, Yanhua and Zhang, Zhi-Li},
  year = {2012},
  journal = {Internet Mathematics},
  volume = {8},
  number = {4},
  pages = {381--401},
  publisher = {Taylor & Francis}
}

@misc{Li2023,
  title = {Textbooks {{Are All You Need II}}: Phi-1.5 Technical Report},
  shorttitle = {Textbooks {{Are All You Need II}}},
  author = {Li, Yuanzhi and Bubeck, S{\'e}bastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat},
  year = {2023},
  month = sep,
  number = {arXiv:2309.05463},
  eprint = {2309.05463},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.05463},
  urldate = {2023-09-16},
  abstract = {We continue the investigation into the power of smaller Transformer-based language models as initiated by {\textbackslash}textbf\{TinyStories\} -- a 10 million parameter model that can produce coherent English -- and the follow-up work on {\textbackslash}textbf\{phi-1\}, a 1.3 billion parameter model with Python coding performance close to the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to generate ``textbook quality" data as a way to enhance the learning process compared to traditional web data. We follow the ``Textbooks Are All You Need" approach, focusing this time on common sense reasoning in natural language, and create a new 1.3 billion parameter model named {\textbackslash}textbf\{phi-1.5\}, with performance on natural language tasks comparable to models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding. More generally, {\textbackslash}textbf\{phi-1.5\} exhibits many of the traits of much larger LLMs, both good -- such as the ability to ``think step by step" or perform some rudimentary in-context learning -- and bad, including hallucinations and the potential for toxic and biased generations -- encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source {\textbackslash}textbf\{phi-1.5\} to promote further research on these urgent topics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/marnix/Zotero/storage/56Z373UI/Li et al. - 2023 - Textbooks Are All You Need II phi-1.5 technical r.pdf;/home/marnix/Zotero/storage/7XZKKIMX/2309.html}
}

@article{Liang2008,
  title = {Mixtures of g {{Priors}} for {{Bayesian Variable Selection}}},
  author = {Liang, Feng and Paulo, Rui and Molina, German and Clyde, Merlise A and Berger, Jim O},
  year = {2008},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {103},
  number = {481},
  pages = {410--423},
  issn = {0162-1459},
  doi = {10.1198/016214507000001337},
  urldate = {2020-02-20},
  abstract = {Zellner's g prior remains a popular conventional prior for use in Bayesian variable selection, despite several undesirable consistency issues. In this article we study mixtures of g priors as an alternative to default g priors that resolve many of the problems with the original formulation while maintaining the computational tractability that has made the g prior so popular. We present theoretical properties of the mixture g priors and provide real and simulated examples to compare the mixture formulation with fixed g priors, empirical Bayes approaches, and other default procedures. Please see Arnold Zellner's letter and the author's response.},
  file = {/home/marnix/Zotero/storage/LEEQRZC4/Liang et al. - 2008 - Mixtures of g Priors for Bayesian Variable Selecti.pdf;/home/marnix/Zotero/storage/YBNHXZFE/016214507000001337.html}
}

@inproceedings{Liben-Nowell2003,
  title = {The Link Prediction Problem for Social Networks},
  booktitle = {Proceedings of the Twelfth International Conference on {{Information}} and Knowledge Management},
  author = {{Liben-Nowell}, David and Kleinberg, Jon},
  year = {2003},
  pages = {556--559}
}

@book{Lichty2013,
  title = {Variant Analyses: {{Interrogations}} of New Media Art and Culture},
  author = {Lichty, Patrick},
  year = {2013},
  publisher = {Institute of Network Cultures},
  address = {Amsterdam},
  isbn = {9781300917284}
}

@book{Lighthill1958,
  title = {An Introduction to {{Fourier}} Analysis and Generalised Functions},
  author = {Lighthill, Michael James},
  year = {1958},
  publisher = {Cambridge University Press},
  file = {/home/marnix/Zotero/storage/U9J6KTUI/Lighthill1959 Introduction to Fourier Analysis and Generalised Functions.pdf}
}

@phdthesis{Limcangco1992,
  title = {Efficient {{Derivation}} and {{Approximations}} of {{Cepstral Coefficients}} for {{Speech Coding}}},
  author = {Limcangco, Kimberly Ann},
  year = {1992},
  file = {/home/marnix/Zotero/storage/HMSUG2W8/Limcangco1993 Efficient derivation and approximations of cepstral coefficients for speech coding.pdf}
}

@book{Lin1988,
  title = {Mathematics Applied to Deterministic Problems in the Natural Sciences},
  author = {Lin, Chia-Ch'iao and Segel, Lee A},
  year = {1988},
  volume = {1},
  publisher = {Siam},
  file = {/home/marnix/Zotero/storage/Y4RQMEF8/epdf.pub_mathematics-applied-to-deterministic-problems-in-t.djvu}
}

@misc{Lin2025,
  title = {Continuous {{Autoregressive Modeling}} with {{Stochastic Monotonic Alignment}} for {{Speech Synthesis}}},
  author = {Lin, Weiwei and He, Chenghan},
  year = {2025},
  month = feb,
  number = {arXiv:2502.01084},
  eprint = {2502.01084},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.01084},
  urldate = {2025-02-17},
  abstract = {We propose a novel autoregressive modeling approach for speech synthesis, combining a variational autoencoder (VAE) with a multi-modal latent space and an autoregressive model that uses Gaussian Mixture Models (GMM) as the conditional probability distribution. Unlike previous methods that rely on residual vector quantization, our model leverages continuous speech representations from the VAE's latent space, greatly simplifying the training and inference pipelines. We also introduce a stochastic monotonic alignment mechanism to enforce strict monotonic alignments. Our approach significantly outperforms the state-of-the-art autoregressive model VALL-E in both subjective and objective evaluations, achieving these results with only 10.3{\textbackslash}\% of VALL-E's parameters. This demonstrates the potential of continuous speech language models as a more efficient alternative to existing quantization-based speech language models. Sample audio can be found at https://tinyurl.com/gmm-lm-tts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/marnix/Zotero/storage/NXBLSMAG/Lin and He - 2025 - Continuous Autoregressive Modeling with Stochastic Monotonic Alignment for Speech Synthesis.pdf;/home/marnix/Zotero/storage/834VLRUW/2502.html}
}

@article{Lindsey1987,
  title = {{{SPAR}}'s Archivable Actual-Word Databases},
  author = {Lindsey, G and Breen, A and Nevard, S},
  year = {1987},
  journal = {University College London,'' Technical Report}
}

@article{Lippmann1997,
  title = {Speech Recognition by Machines and Humans},
  author = {Lippmann, Richard P.},
  year = {1997},
  journal = {Speech Communication},
  volume = {22},
  number = {1},
  pages = {1--15},
  issn = {01676393},
  doi = {10.1016/S0167-6393(97)00021-6},
  abstract = {This paper reviews past work comparing modern speech recognition systems and humans to determine how far recent dramatic advances in technology have progressed towards the goal of human-like performance. Comparisons use six modern speech corpora with vocabularies ranging from 10 to more than 65,000 words and content ranging from read isolated words to spontaneous conversations. Error rates of machines are often more than an order of magnitude greater than those of humans for quiet, wideband, read speech. Machine performance degrades further below that of humans in noise, with channel variability, and for spontaneous speech. Humans can also recognize quiet, clearly spoken nonsense syllables and nonsense sentences with little high-level grammatical information. These comparisons suggest that the human-machine performance gap can be reduced by basic research on improving low-level acoustic-phonetic modeling, on improving robustness with noise and channel variability, and on more accurately modeling spontaneous speech. Ce papier pr{\'e}sente un bilan des travaux comparant les performances des syst{\`e}mes de reconnaissance de parole modernes {\`a} celles des locuteurs humains. Les comparaisons sont bas{\'e}es sur six types de corpus de parole avec des vocabulaires allant de 10 {\`a} plus de 65000 mots et des contenus allant des mots isol{\'e}s {\`a} des conversations spontan{\'e}es. Les taux d'erreurs des machines sont souvent sup{\'e}rieures de plus d'un ordre de grandeur {\`a} celles des humains pour la parole lue en atmosph{\`e}re calme et transmise en large-bande. Les performances des machines se d{\'e}gradent encore par rapport {\`a} celles des humains dans les contextes bruit{\'e}s, ou de qualit{\'e} de transmission variable et pour la parole spontan{\'e}e. Les locuteurs humains peuvent {\'e}galement reconnaitre, avec peu d'information linguistique de haut-niveau, des syllabes ou des phrases sans signification quand elles sont prononc{\'e}es clairement dans des atmosph{\`e}res calmes. Ces comparaisons sugg{\`e}rent que l'{\'e}cart important qui subsiste entre les performances des machines et celles des humains peut {\^e}tre r{\'e}duit par des recherches de base sur les sujets suivants: l'am{\'e}lioration de la mod{\'e}lisation acoustico-phon{\'e}tique de bas-niveau, l'am{\'e}lioration de la robustesse au bruit et {\`a} la variabilit{\'e} des conditions de transmission, et la mod{\'e}lisation plus pr{\'e}cise de la parole spontan{\'e}e.},
  isbn = {0167-6393},
  keywords = {automatic speech recognition,machine recognition,noise,nonsense sentences,nonsense syllables,perception,performance,speech,speech perception,speech recognition},
  file = {/home/marnix/Zotero/storage/6S5HB6NH/Lippmann Human Speech Recognition.pdf}
}

@article{Littenberg2015,
  title = {Bayesian Inference for Spectral Estimation of Gravitational Wave Detector Noise},
  author = {Littenberg, Tyson B. and Cornish, Neil J.},
  year = {2015},
  month = apr,
  journal = {Physical Review D},
  volume = {91},
  number = {8},
  pages = {084034},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevD.91.084034},
  urldate = {2021-03-19},
  abstract = {Gravitational wave data from ground-based detectors is dominated by instrument noise. Signals will be comparatively weak, and our understanding of the noise will influence detection confidence and signal characterization. Mismodeled noise can produce large systematic biases in both model selection and parameter estimation. Here we introduce a multicomponent, variable dimension, parametrized model to describe the Gaussian-noise power spectrum for data from ground-based gravitational wave interferometers. Called BayesLine, the algorithm models the noise power spectral density using cubic splines for smoothly varying broadband noise and Lorentzians for narrow-band line features in the spectrum. We describe the algorithm and demonstrate its performance on data from the fifth and sixth LIGO science runs. Once fully integrated into LIGO/Virgo data analysis software, BayesLine will produce accurate spectral estimation and provide a means for marginalizing inferences drawn from the data over all plausible noise spectra.},
  file = {/home/marnix/Zotero/storage/QJR45R57/Littenberg and Cornish - 2015 - Bayesian inference for spectral estimation of grav.pdf;/home/marnix/Zotero/storage/TY7J4KZN/PhysRevD.91.html}
}

@incollection{Little2006,
  title = {A {{Simple}}, {{Quasi-linear}}, {{Discrete Model}} of {{Vocal Fold Dynamics}}},
  booktitle = {Nonlinear {{Analyses}} and {{Algorithms}} for {{Speech Processing}}},
  author = {Little, Max and McSharry, Patrick and Moroz, Irene and Roberts, Stephen},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and {Faundez-Zanuy}, Marcos and Janer, L{\'e}onard and Esposito, Anna and {Satue-Villar}, Antonio and Roure, Josep and {Espinosa-Duro}, Virginia},
  year = {2006},
  volume = {3817},
  pages = {348--356},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11613107_31},
  urldate = {2019-07-16},
  abstract = {In current speech technology, linear prediction dominates. The linear vocal tract model is well justified biomechanically, and linear prediction is a simple and well understood signal processing task. However, it has been established that, in voiced sounds, the vocal folds exhibit a high degree of nonlinearity. Hence there exists the need for an approach to modelling the behaviour of the vocal folds. This paper presents a simple, nonlinear, biophysical vocal fold model. A complementary discrete model is derived that reflects accurately the energy dynamics in the continuous model. This model can be implemented easily on standard digital signal processing hardware, and it is formulated in such a way that a simple form of nonlinear prediction can be carried out on vocal fold signals. This model could be of utility in many speech technological applications where low computational complexity synthesis and analysis of vocal fold dynamics is required.},
  isbn = {978-3-540-31257-4 978-3-540-32586-4},
  langid = {english},
  file = {/home/marnix/Zotero/storage/UG3T96Z6/Little et al. - 2006 - A Simple, Quasi-linear, Discrete Model of Vocal Fo.pdf}
}

@article{Little2006a,
  title = {Testing the Assumptions of Linear Prediction Analysis in Normal Vowels},
  author = {Little, Max and McSharry, Patrick E. and Moroz, Irene M. and Roberts, Stephen J.},
  year = {2006},
  month = jan,
  journal = {The Journal of the Acoustical Society of America},
  volume = {119},
  number = {1},
  eprint = {nlin/0601009},
  pages = {549--558},
  issn = {0001-4966},
  doi = {10.1121/1.2141266},
  urldate = {2019-07-18},
  abstract = {This paper develops an improved surrogate data test to show experimental evidence, for all the simple vowels of US English, for both male and female speakers, that Gaussian linear prediction analysis, a ubiquitous technique in current speech technologies, cannot be used to extract all the dynamical structure of real speech time series. The test provides robust evidence undermining the validity of these linear techniques, supporting the assumptions of either dynamical nonlinearity and/or non-Gaussianity common to more recent, complex, efforts at dynamical modelling speech time series. However, an additional finding is that the classical assumptions cannot be ruled out entirely, and plausible evidence is given to explain the success of the linear Gaussian theory as a weak approximation to the true, nonlinear/non-Gaussian dynamics. This supports the use of appropriate hybrid linear/nonlinear/non-Gaussian modelling. With a calibrated calculation of statistic and particular choice of experimental protocol, some of the known systematic problems of the method of surrogate data testing are circumvented to obtain results to support the conclusions to a high level of significance.},
  archiveprefix = {arXiv},
  keywords = {Nonlinear Sciences - Chaotic Dynamics},
  file = {/home/marnix/Zotero/storage/WF6RHU8E/Little et al. - 2006 - Testing the assumptions of linear prediction analy.pdf;/home/marnix/Zotero/storage/LIKZEDGF/0601009.html}
}

@article{Little2007,
  title = {Exploiting {{Nonlinear Recurrence}} and {{Fractal Scaling Properties}} for {{Voice Disorder Detection}}},
  author = {Little, Max A and McSharry, Patrick E and Roberts, Stephen J and Costello, Declan AE and Moroz, Irene M},
  year = {2007},
  journal = {BioMedical Engineering OnLine},
  volume = {6},
  number = {1},
  pages = {23},
  issn = {1475925X},
  doi = {10.1186/1475-925X-6-23},
  urldate = {2019-07-16},
  abstract = {Background: Voice disorders affect patients profoundly, and acoustic tools can potentially measure voice function objectively. Disordered sustained vowels exhibit wide-ranging phenomena, from nearly periodic to highly complex, aperiodic vibrations, and increased ``breathiness''. Modelling and surrogate data studies have shown significant nonlinear and non-Gaussian random properties in these sounds. Nonetheless, existing tools are limited to analysing voices displaying near periodicity, and do not account for this inherent biophysical nonlinearity and non-Gaussian randomness, often using linear signal processing methods insensitive to these properties. They do not directly measure the two main biophysical symptoms of disorder: complex nonlinear aperiodicity, and turbulent, aeroacoustic, non-Gaussian randomness. Often these tools cannot be applied to more severe disordered voices, limiting their clinical usefulness. Methods: This paper introduces two new tools to speech analysis: recurrence and fractal scaling, which overcome the range limitations of existing tools by addressing directly these two symptoms of disorder, together reproducing a ``hoarseness'' diagram. A simple bootstrapped classifier then uses these two features to distinguish normal from disordered voices.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/M3XGHHPV/Little et al. - 2007 - Exploiting Nonlinear Recurrence and Fractal Scalin.pdf}
}

@incollection{Little2011,
  title = {Mathematical {{Foundations}} of {{Nonlinear}}, {{Non-Gaussian}}, and {{Time-Varying Digital Speech Signal Processing}}},
  booktitle = {Advances in {{Nonlinear Speech Processing}}},
  author = {Little, Max A.},
  editor = {{Travieso-Gonz{\'a}lez}, Carlos M. and {Alonso-Hern{\'a}ndez}, Jes{\'u}s B.},
  year = {2011},
  volume = {7015},
  pages = {9--16},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-25020-0_2},
  urldate = {2019-07-22},
  abstract = {Classical digital speech signal processing assumes linearity, timeinvariance, and Gaussian random variables (LTI-Gaussian theory). In this article, we address the suitability of these mathematical assumptions for realistic speech signals with respect to the biophysics of voice production, finding that the LTI-Gaussian approach has some important accuracy and computational efficiency shortcomings in both theory and practice. Next, we explore the consequences of relaxing the assumptions of time-invariance and Gaussianity, which admits certain potentially useful techniques, including wavelet and sparse representations in computational harmonic analysis, but rules out Fourier analysis and convolution, which could be a disadvantage. Then, we focus on methods that retain time-invariance alone, which admits techniques from nonlinear time series analysis and Markov chains, both of which have shown promise in biomedical applications. We highlight recent examples of non-LTI-Gaussian digital speech signal processing in the literature, and draw conclusions for future prospects in this area.},
  isbn = {978-3-642-25019-4 978-3-642-25020-0},
  langid = {english},
  file = {/home/marnix/Zotero/storage/N8BTW6PL/Little - 2011 - Mathematical Foundations of Nonlinear, Non-Gaussia.pdf}
}

@article{Liu2011,
  title = {Controllability of Complex Networks},
  author = {Liu, Yang-Yu and Slotine, Jean-Jacques and Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  year = {2011},
  journal = {Nature},
  volume = {473},
  number = {7346},
  eprintclass = {http:},
  pages = {167--173},
  issn = {0028-0836},
  doi = {10.1038/nature10011},
  abstract = {The ultimate proof of our understanding of natural or technological systems is reflected in our ability to control them. Although control theory offers mathematical tools for steering engineered and natural systems towards a desired state, a framework to control complex self-organized systems is lacking. Here we develop analytical tools to study the controllability of an arbitrary complex directed network, identifying the set of driver nodes with time-dependent control that can guide the system's entire dynamics. We apply these tools to several real networks, finding that the number of driver nodes is determined mainly by the network's degree distribution. We show that sparse inhomogeneous networks, which emerge in many real complex systems, are the most difficult to control, but that dense and homogeneous networks can be controlled using a few driver nodes. Counterintuitively, we find that in both model and real systems the driver nodes tend to avoid the high-degree nodes.},
  arxiv = {/www.nature.com/nature/journal/v473/n7346/abs/10.1038-nature10011-unlocked.html\{{\textbackslash}\#\}supplementary-information},
  arxivid = {http://www.nature.com/nature/journal/v473/n7346/abs/10.1038-nature10011-unlocked.html#supplementary-information},
  isbn = {1476-4687 (Electronic)\r0028-0836 (Linking)},
  pmid = {21562557},
  file = {/home/marnix/Zotero/storage/MLWI5PC8/Liu, Slotine, Barabási - 2011 - Controllability of complex networks.pdf}
}

@misc{Liu2021,
  title = {Differentiate {{Everything}} with a {{Reversible Embeded Domain-Specific Language}}},
  author = {Liu, Jin-Guo and Zhao, Taine},
  year = {2021},
  month = jan,
  number = {arXiv:2003.04617},
  eprint = {2003.04617},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-06-29},
  abstract = {Reverse-mode automatic differentiation (AD) suffers from the issue of having too much space overhead to trace back intermediate computational states for back-propagation. The traditional method to trace back states is called checkpointing that stores intermediate states into a global stack and restore state through either stack pop or re-computing. The overhead of stack manipulations and re-computing makes the general purposed (not tensor-based) AD engines unable to meet many industrial needs. Instead of checkpointing, we propose to use reverse computing to trace back states by designing and implementing a reversible programming eDSL, where a program can be executed bi-directionally without implicit stack operations. The absence of implicit stack operations makes the program compatible with existing compiler features, including utilizing existing optimization passes and compiling the code as GPU kernels. We implement AD for sparse matrix operations and some machine learning applications to show that our framework has the state-of-the-art performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages},
  file = {/home/marnix/Zotero/storage/YCGZD5T8/Liu and Zhao - 2021 - Differentiate Everything with a Reversible Embeded.pdf;/home/marnix/Zotero/storage/QJCMNIDZ/2003.html}
}

@misc{Liu2023,
  title = {Visual {{Instruction Tuning}}},
  author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  year = {2023},
  month = apr,
  number = {arXiv:2304.08485},
  eprint = {2304.08485},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.08485},
  urldate = {2023-12-08},
  abstract = {Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53\%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/marnix/Zotero/storage/IK8KDJI8/Liu et al. - 2023 - Visual Instruction Tuning.pdf;/home/marnix/Zotero/storage/5QTYLYEE/2304.html}
}

@article{Ljung1981,
  title = {Frequency Domain versus Time Domain Methods in System Identification},
  author = {Ljung, Lennart and Glover, Keith},
  year = {1981},
  month = jan,
  journal = {Automatica},
  volume = {17},
  number = {1},
  pages = {71--86},
  issn = {0005-1098},
  doi = {10.1016/0005-1098(81)90085-6},
  urldate = {2019-04-17},
  abstract = {The basic techniques of time domain and frequency domain identification, including the maximum entropy methods, are outlined. Then connections and distinctions between the methods are explored. This includes the derivation of some analytic relationships together with a discussion of the restrictions inherent in choosing certain methods, and their ease of use in different experimental conditions. It is concluded that these are complementary rather than competing techniques.},
  keywords = {Identification,spectral analysis},
  file = {/home/marnix/Zotero/storage/MUNDF45T/Ljung and Glover - 1981 - Frequency domain versus time domain methods in sys.pdf;/home/marnix/Zotero/storage/SWWKV2PM/0005109881900856.html}
}

@article{Lloyd1890,
  title = {Speech Sounds: {{Their}} Nature and Causation ({{I}})},
  author = {Lloyd, R. J.},
  year = {1890},
  journal = {Phonetische Studien},
  volume = {3},
  pages = {251--278}
}

@article{Loan2000,
  title = {The Ubiquitous {{Kronecker}} Product},
  author = {Loan, Charles F. Van},
  year = {2000},
  month = nov,
  journal = {Journal of Computational and Applied Mathematics},
  series = {Numerical {{Analysis}} 2000. {{Vol}}. {{III}}: {{Linear Algebra}}},
  volume = {123},
  number = {1},
  pages = {85--100},
  issn = {0377-0427},
  doi = {10.1016/S0377-0427(00)00393-9},
  urldate = {2022-10-12},
  abstract = {The Kronecker product has a rich and very pleasing algebra that supports a wide range of fast, elegant, and practical algorithms. Several trends in scientific computing suggest that this important matrix operation will have an increasingly greater role to play in the future. First, the application areas where Kronecker products abound are all thriving. These include signal processing, image processing, semidefinite programming, and quantum computing. Second, sparse factorizations and Kronecker products are proving to be a very effective way to look at fast linear transforms. Researchers have taken the Kronecker methodology as developed for the fast Fourier transform and used it to build exciting alternatives. Third, as computers get more powerful, researchers are more willing to entertain problems of high dimension and this leads to Kronecker products whenever low-dimension techniques are ``tensored'' together.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/FBKLPZ93/Loan - 2000 - The ubiquitous Kronecker product.pdf;/home/marnix/Zotero/storage/VC3JWTPJ/S0377042700003939.html}
}

@incollection{Lopez-Lopera2020,
  title = {Approximating {{Gaussian Process Emulators}} with {{Linear Inequality Constraints}} and {{Noisy Observations}} via {{MC}} and {{MCMC}}},
  author = {{L{\'o}pez-Lopera}, Andr{\'e}s F. and Bachoc, Fran{\c c}ois and Durrande, Nicolas and Rohmer, J{\'e}r{\'e}my and Idier, D{\'e}borah and Roustant, Olivier},
  year = {2020},
  volume = {324},
  eprint = {1901.04827},
  primaryclass = {cs, stat},
  pages = {363--381},
  doi = {10.1007/978-3-030-43465-6_18},
  urldate = {2024-01-07},
  abstract = {Adding inequality constraints (e.g. boundedness, monotonicity, convexity) into Gaussian processes (GPs) can lead to more realistic stochastic emulators. Due to the truncated Gaussianity of the posterior, its distribution has to be approximated. In this work, we consider Monte Carlo (MC) and Markov Chain Monte Carlo (MCMC) methods. However, strictly interpolating the observations may entail expensive computations due to highly restrictive sample spaces. Furthermore, having (constrained) GP emulators when data are actually noisy is also of interest for real-world implementations. Hence, we introduce a noise term for the relaxation of the interpolation conditions, and we develop the corresponding approximation of GP emulators under linear inequality constraints. We show with various toy examples that the performance of MC and MCMC samplers improves when considering noisy observations. Finally, on 2D and 5D coastal flooding applications, we show that more flexible and realistic GP implementations can be obtained by considering noise effects and by enforcing the (linear) inequality constraints.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/Z446BELT/López-Lopera et al. - 2020 - Approximating Gaussian Process Emulators with Line.pdf;/home/marnix/Zotero/storage/KG4MSFF6/1901.html}
}

@article{Lopez-Pastor2023,
  title = {Self-{{Learning Machines Based}} on {{Hamiltonian Echo Backpropagation}}},
  author = {{L{\'o}pez-Pastor}, V{\'i}ctor and Marquardt, Florian},
  year = {2023},
  month = aug,
  journal = {Physical Review X},
  volume = {13},
  number = {3},
  pages = {031020},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevX.13.031020},
  urldate = {2023-09-09},
  abstract = {A physical self-learning machine can be defined as a nonlinear dynamical system that can be trained on data (similar to artificial neural networks) but where the update of the internal degrees of freedom that serve as learnable parameters happens autonomously. In this way, neither external processing and feedback nor knowledge of (and control of) these internal degrees of freedom is required. We introduce a general scheme for self-learning in any time-reversible Hamiltonian system. It relies on implementing a time-reversal operation and injecting a small error signal on top of the echo dynamics. We show how the physical dynamics itself will then lead to the required gradient update of learnable parameters, independent of the details of the Hamiltonian. We illustrate the training of such a self-learning machine numerically for the case of coupled nonlinear wave fields and other examples.},
  file = {/home/marnix/Zotero/storage/YA848ASB/López-Pastor and Marquardt - 2023 - Self-Learning Machines Based on Hamiltonian Echo B.pdf;/home/marnix/Zotero/storage/6NJWJS7F/PhysRevX.13.html}
}

@misc{Loukas2022,
  title = {Entropy-Based {{Characterization}} of {{Modeling Constraints}}},
  author = {Loukas, Orestis and Chung, Ho Ryun},
  year = {2022},
  month = jun,
  number = {arXiv:2206.14105},
  eprint = {2206.14105},
  primaryclass = {hep-th, stat},
  publisher = {arXiv},
  urldate = {2022-07-19},
  abstract = {In most data-scientific approaches, the principle of Maximum Entropy (MaxEnt) is used to a posteriori justify some parametric model which has been already chosen based on experience, prior knowledge or computational simplicity. In a perpendicular formulation to conventional model building, we start from the linear system of phenomenological constraints and asymptotically derive the distribution over all viable distributions that satisfy the provided set of constraints. The MaxEnt distribution plays a special role, as it is the most typical among all phenomenologically viable distributions representing a good expansion point for large-N techniques. This enables us to consistently formulate hypothesis testing in a fully-data driven manner. The appropriate parametric model which is supported by the data can be always deduced at the end of model selection. In the MaxEnt framework, we recover major scores and selection procedures used in multiple applications and assess their ability to capture associations in the data-generating process and identify the most generalizable model. This data-driven counterpart of standard model selection demonstrates the unifying prospective of the deductive logic advocated by MaxEnt principle, while potentially shedding new insights to the inverse problem.},
  archiveprefix = {arXiv},
  keywords = {High Energy Physics - Theory,Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/marnix/Zotero/storage/H26LFQH7/Loukas and Chung - 2022 - Entropy-based Characterization of Modeling Constra.pdf;/home/marnix/Zotero/storage/WHNDNZF3/2206.html}
}

@article{Luberadzka2019,
  title = {Glimpsed {{Periodicity Features}} and {{Recursive Bayesian}}  {{Estimation}} for Modeling Attentive Voice Tracking},
  author = {Luberadzka, Joanna and Kayser, Hendrik and Hohmann, Volker},
  year = {2019},
  journal = {International Congress on Acoustics},
  volume = {9},
  pages = {8},
  abstract = {Computational models are a way of approaching research questions related to auditory perception. One relevant question is how we are able to follow and understand speech in complex acoustic scenes. Previous studies suggest that for tracking a speaker in such conditions, humans use (1) sparse, speaker-related bits of robust information - 'auditory glimpses' and (2) a mechanism of predictive coding with a movable locus of attention. The goal of the present study is to develop a computational model for attentive tracking of voices, which takes these two aspects into account. We model auditory glimpses using Glimpsed Periodicity Features and predictive coding using Recursive Bayesian Estimation. We assume that perception is organized into an attended foreground and unattended background. We propose parallel particle filters - one for each category - to track the concurrent events. In this approach, each incoming glimpse is associated with either foreground or background based on accumulated evidence. Simulations with artificially generated data of a 'glimpsing' nature (sparse, robust) showed that this approach is suitable to track multidimensional parameter trajectories of two competing sources. This suggests the potential of the method to track simultaneously active voices based on the Glimpsed Periodicity Features.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/NNVVBHJQ/Luberadzka et al. - Glimpsed Periodicity Features and Recursive Bayesi.pdf}
}

@book{Lukyanenko2022,
  title = {A {{Realist Ontology}} of {{Digital Objects}} and {{Digitalized Systems}}},
  author = {Lukyanenko, Roman and Weber, Ron},
  year = {2022},
  month = jun,
  abstract = {As digitalized products and services have become more pervasive and complex, the need grows to better facilitate their design, use, and management. This presupposes a deeper understanding of the nature of digital technologies. The paper develops a new ontology-a Realist Ontology of Digital Objects and Digitalized Systems. It is based on Bunge's rigorous theories of semantics and ontology, which we adapted, further formalized, and extended. The ontology defines digital objects and digitalized systems and grounds these constructs into fundamental claims about the nature of physical, mental, and social reality. To show the power of the ontology, we contrast it with the ontology used in a widely cited paper by Yoo et al. (2010). We show how our ontology leads to some different conclusions and predictions from theirs. This analysis reveals the potential of the new ontology to facilitate design, use, and management of digital technology.},
  file = {/home/marnix/Zotero/storage/T4TYJUCH/Lukyanenko and Weber - 2022 - A Realist Ontology of Digital Objects and Digitali.pdf}
}

@article{Luts2014,
  title = {Real-{{Time Semiparametric Regression}}},
  author = {Luts, J. and Broderick, T. and Wand, M. P.},
  year = {2014},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {23},
  number = {3},
  eprint = {43304913},
  eprinttype = {jstor},
  pages = {589--615},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
  issn = {1061-8600},
  urldate = {2021-08-16},
  abstract = {We develop algorithms for performing semiparametric regression analysis in real time, with data processed as it is collected and made immediately available via modern telecommunications technologies. Our definition of semiparametric regression is quite broad and includes, as special cases, generalized linear mixed models, generalized additive models, geostatistical models, wavelet nonparametric regression models and their various combinations. Fast updating of regression fits is achieved by couching semiparametric regression into a Bayesian hierarchical model or, equivalently, graphical model framework and employing online mean field variational ideas. An Internet site attached to this article, realtime-semiparametnc-regression.net, illustrates the methodology for continually arriving stock market, real estate, and airline data. Flexible real-time analyses based on increasingly ubiquitous streaming data sources stand to benefit. This article has online supplementary material.},
  file = {/home/marnix/Zotero/storage/U22U76Y9/Luts et al. - 2014 - Real-Time Semiparametric Regression.pdf}
}

@book{Lyon2017,
  title = {Human and {{Machine Hearing}}: {{Extracting Meaning}} from {{Sound}}},
  shorttitle = {Human and {{Machine Hearing}}},
  author = {Lyon, Richard F.},
  year = {2017},
  month = may,
  edition = {1},
  publisher = {Cambridge University Press},
  doi = {10.1017/9781139051699},
  urldate = {2020-04-26},
  isbn = {978-1-107-00753-6 978-1-139-05169-9},
  langid = {english},
  file = {/home/marnix/Zotero/storage/CEACT6YF/Lyon - 2017 - Human and Machine Hearing Extracting Meaning from.pdf}
}

@misc{Lyu2024,
  title = {Spherical {{Density-Equalizing Map}} for {{Genus-0 Closed Surfaces}}},
  author = {Lyu, Zhiyuan and Lui, Lok Ming and Choi, Gary P. T.},
  year = {2024},
  month = jan,
  number = {arXiv:2401.11795},
  eprint = {2401.11795},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2024-09-19},
  abstract = {Density-equalizing maps are a class of mapping methods in which the shape deformation is driven by prescribed density information. In recent years, they have been widely used for data visualization on planar domains and planar parameterization of open surfaces. However, the theory and computation of density-equalizing maps for closed surfaces are much less explored. In this work, we develop a novel method for computing spherical density-equalizing maps for genus-0 closed surfaces. Specifically, we first compute a conformal parameterization of the given genus-0 closed surface onto the unit sphere. Then, we perform density equalization on the spherical domain based on the given density information to achieve a spherical density-equalizing map. The bijectivity of the mapping is guaranteed using quasi-conformal theory. We further propose a method for incorporating the harmonic energy and landmark constraints into our formulation to achieve landmark-aligned spherical density-equalizing maps balancing different distortion measures. Using the proposed methods, a large variety of spherical parameterizations can be achieved. Applications to surface registration, remeshing, and data visualization are presented to demonstrate the effectiveness of our methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computational Geometry,Computer Science - Graphics,Mathematics - Differential Geometry,Mathematics - Numerical Analysis},
  file = {/home/marnix/Zotero/storage/KF74C6KJ/Lyu et al. - 2024 - Spherical Density-Equalizing Map for Genus-0 Close.pdf}
}

@article{Ma1993,
  title = {Robust Signal Selection for Linear Prediction Analysis of Voiced Speech},
  author = {Ma, Changxue and Kamp, Y. and Willems, L. F.},
  year = {1993},
  month = mar,
  journal = {Speech Communication},
  volume = {12},
  number = {1},
  pages = {69--81},
  issn = {0167-6393},
  doi = {10.1016/0167-6393(93)90019-H},
  urldate = {2023-04-17},
  abstract = {This paper investigates a weighted LPC analysis of voiced speech. In view of the speech production model, the weighting function is either chosen to be the short-time energy function of the preemphasized speech sample sequence with certain delays or is obtained by thresholding the short-time energy function. In this method, speech samples are selectively weighted based on how well they match the speech production model. Therefore, the estimates of the LPC coefficients obtained by this novel LPC analysis are more accurate than those obtained from the conventional LPC analysis. They are also less sensitive to the values of the fundamental frequency than conventional LPC. Zusammenfassung Dieser Beitrag untersucht ein LPC Analyserverfahrem, bei dem das stimmhafte Sprachsignal zuvor gewichtet wird. Bei dieser Methode werden die einzelnen Signalwerte um so st{\"a}rker gewichtet, je besser sie mit dem Sprach-Produktions-Modell {\"u}bereinstimnen. Als Wichtungsfunktion wird dabei entweder die Kurzzeitenergiefunktion der - vorgefilterten - Signalwertfolge mit einer gewissen Verz{\"o}gerung verwendet oder sie ergibt sich durch eine Schwellwertbildung aus der Kurzzeitenergiefunktion. Die Sch{\"a}tzung der LPC Koeffizienten ist bei diesem neuartigen Ansatz genauer als bei einer konventionellen LPC Analyse. Zus{\"a}tzlich werden die Werte der gesch{\"a}tzten Parameter weniger stark durch die Grundfrequenz des Sprachsignals beeinflu{$\beta$}t. R{\'e}sum{\'e} Dans cet article, on pr{\'e}sente une technique de pond{\'e}ration pour effectuer l'analyse LPC d'un signal de parole vois{\'e}e. Les {\'e}chantillons sont pond{\'e}res sur base de leur conformit{\'e} au mod{\`e}le de production de parole vois{\'e}e. Dans les deux techniques de pond{\'e}ration pr{\'e}sen{\'e}es, la premi{\`e}re choisit comme fonction de poids la fonction d'{\'e}nergie {\`a} court-terme du signal de parole pr{\'e}accentu{\'e}, tandis que la seconde s'obtient par seuillage de cette m{\^e}me fonction d'energie. Dans les deux cas, la m{\'e}thode propos{\'e}e a pour effet de pond{\'e}rer s{\'e}lectivement les {\'e}chantillons de parole qui correspondent bien au mod{\`e}le de production. En cons{\'e}quence, on obtient par cette m{\'e}thode une estimation des param{\`e}tres LPC qui est {\`a} la fois plus pr{\'e}cise et aussi moins sensible {\`a} la fr{\'e}quence fondamentale que celle fournie par l'analyse LPC classique.},
  langid = {english},
  keywords = {LPC,sample selection,Speech analysis},
  file = {/home/marnix/Zotero/storage/K56JQ8A3/Ma et al. - 1993 - Robust signal selection for linear prediction anal.pdf;/home/marnix/Zotero/storage/U3665CWW/016763939390019H.html}
}

@article{Ma2020,
  title = {Voice Changes in {{Parkinson}}'s Disease: {{What}} Are They Telling Us?},
  shorttitle = {Voice Changes in {{Parkinson}}'s Disease},
  author = {Ma, Andrew and Lau, Kenneth K and Thyagarajan, Dominic},
  year = {2020},
  month = feb,
  journal = {Journal of Clinical Neuroscience},
  volume = {72},
  pages = {1--7},
  issn = {09675868},
  doi = {10.1016/j.jocn.2019.12.029},
  urldate = {2025-09-27},
  abstract = {Emerging evidence suggests voice dysfunction is the earliest sign of motor impairment in Parkinson's disease (PD). The complexity and fine motor control involved in vocalization may result in dysfunction here before the limbs. The voice in PD demonstrates characteristic changes on perceptual and acoustic analyses. The physiological and anatomical correlates of these have been investigated through laryngoscopy, stroboscopy, photoglottography, laryngeal electromyography, computed-tomography, pulmonary function testing and aerodynamic assessments. These have revealed numerous abnormalities including incomplete glottic closure and vocal fold hypoadduction/bowing to account for these voice changes. Many of these phenomena are likely related to rigidity or bradykinesia of the laryngeal muscles. The early onset of voice changes is resonant with the pathophysiological insights offered by Braak's hypothesis and murine models of the disease. These physiological abnormalities and pathological models largely stand to support dopaminergic and non-dopaminergic mechanisms being implicated in the pathogenesis of voice dysfunction. This review focuses on characterizing the voice changes in PD. These stand as a promising area of enquiry to further our understanding of the pathophysiology of the disease and offer potential to be utilized as an early diagnostic biomarker or marker of disease progression.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/9YFRAU9U/Ma et al. - 2020 - Voice changes in Parkinson’s disease What are they telling us.pdf}
}

@article{MacKay1992,
  title = {Bayesian Interpolation},
  author = {MacKay, David JC},
  year = {1992},
  journal = {Neural computation},
  volume = {4},
  number = {3},
  pages = {415--447},
  publisher = {MIT Press},
  file = {/home/marnix/Zotero/storage/PRSDGVQN/MacKay - 1992 - Bayesian interpolation.pdf}
}

@phdthesis{MacKay1992a,
  title = {Bayesian Methods for Adaptive Models},
  author = {MacKay, David J. C.},
  year = {1992},
  doi = {10.7907/H3A1-WM07},
  urldate = {2022-10-29},
  abstract = {The Bayesian framework for model comparison and regularisation is demonstrated by studying interpolation and classification problems modelled with both linear and non-linear models. This framework quantitatively embodies 'Occam's razor'. Over-complex and under-regularised models are automatically inferred to be less probable, even though their flexibility allows them to fit the data better. When applied to 'neural networks', the Bayesian framework makes possible (1) objective comparison of solutions using alternative network architectures; (2) objective stopping rules for network pruning or growing procedures; (3) objective choice of type of weight decay terms (or regularisers); (4) on-line techniques for optimising weight decay (or regularisation constant) magnitude; (5) a measure of the effective number of well-determined parameters in a model; (6) quantified estimates of the error bars on network parameters and on network output. In the case of classification models, it is shown that the careful incorporation of error bar information into a classifier's predictions yields improved performance. Comparisons of the inferences of the Bayesian Framework with more traditional cross-validation methods help detect poor underlying assumptions in learning models. The relationship of the Bayesian learning framework to 'active learning' is examined. Objective functions are discussed which measure the expected informativeness data measurements, in the context of both interpolation and classification problems. The concepts and methods described in this thesis are quite general and will be applicable to other data modelling problems whether they involve regression, classification or density estimation.},
  copyright = {other},
  langid = {english},
  school = {California Institute of Technology},
  file = {/home/marnix/Zotero/storage/9LKJVQW9/MacKay_djc_1992_revised_by_author.pdf;/home/marnix/Zotero/storage/DXEGMZMA/25.html}
}

@article{MacKay1994,
  title = {A {{Hierarchical Dirichlet Language Model}}},
  author = {MacKay, David J C and Bauman Peto, Linda C},
  year = {1994},
  volume = {1},
  number = {1},
  pages = {1--19},
  file = {/home/marnix/Zotero/storage/LFAJ5BL8/MacKay1994.pdf}
}

@techreport{MacKay1995,
  type = {Technical {{Report}}},
  title = {Ensemble Learning and Evidence Maximization},
  author = {MacKay, David J.C.},
  year = {1995},
  address = {Cambridge, U.K.},
  institution = {Cavendish Laboratory, Department of Physics, University of Cambridge},
  file = {/home/marnix/Zotero/storage/ILSHUICN/document.pdf}
}

@article{MacKay1996,
  title = {Hyperparameters: {{Optimize}}, or Integrate Out?},
  author = {MacKay, David J. C.},
  year = {1996},
  journal = {Maximum entropy and bayesian methods},
  pages = {43--59},
  issn = {9048144078},
  doi = {10.1007/978-94-015-8729-7},
  isbn = {0792328515},
  pmid = {8993147},
  file = {/home/marnix/Zotero/storage/9VNNNFLU/MacKay - 1996 - Hyperparameters Optimize, or integrate out.pdf}
}

@article{MacKay1998,
  title = {Introduction to {{Gaussian}} Processes},
  author = {MacKay, David JC},
  year = {1998},
  journal = {NATO ASI Series F Computer and Systems Sciences},
  volume = {168},
  pages = {133--166},
  publisher = {Citeseer},
  file = {/home/marnix/Zotero/storage/HL5UZAE5/MacKay - 1998 - Introduction to Gaussian processes.pdf}
}

@book{MacKay2005,
  title = {Information {{Theory}}, {{Inference}}, and {{Learning Algorithms}}},
  booktitle = {Learning},
  author = {MacKay, David J C},
  year = {2005},
  eprint = {1011.1669v3},
  issn = {01621459},
  doi = {10.1198/jasa.2005.s54},
  abstract = {This book is aimed at senior undergraduates and graduate students in Engineering, Science, Mathematics, and Computing. It expects familiarity with calculus, probability theory, and linear algebra as taught in a rst- or secondyear undergraduate course on mathematics for scientists and engineers. Conventional courses on information theory cover not only the beautiful theoretical ideas of Shannon, but also practical solutions to communication problems. This book goes further, bringing in Bayesian data modelling, Monte Carlo methods, variational methods, clustering algorithms, and neural networks. Why unify information theory and machine learning? Because they are two sides of the same coin. In the 1960s, a single eld, cybernetics, was populated by information theorists, computer scientists, and neuroscientists, all studying common problems. Information theory and machine learning still belong together. Brains are the ultimate compression and communication systems. And the state-of-the-art algorithms for both data compression and error-correcting codes use the same tools as machine learning.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1011.1669v3},
  isbn = {978-0-521-64298-9},
  pmid = {13217055},
  file = {/home/marnix/Zotero/storage/KD5YSE6B/MacKay - 2005 - Information Theory, Inference, and Learning Algorithms David J.C. MacKay.pdf}
}

@inproceedings{Maddox2021,
  title = {Bayesian {{Optimization}} with {{High-Dimensional Outputs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Maddox, Wesley J and Balandat, Maximilian and Wilson, Andrew G and Bakshy, Eytan},
  year = {2021},
  volume = {34},
  pages = {19274--19287},
  publisher = {Curran Associates, Inc.},
  urldate = {2022-10-18},
  abstract = {Bayesian optimization is a sample-efficient black-box optimization procedure that is typically applied to a small number of independent objectives. However, in practice we often wish to optimize objectives defined over many correlated outcomes (or ``tasks''). For example, scientists may want to optimize the coverage of a cell tower network across a dense grid of locations. Similarly, engineers may seek to balance the performance of a robot across dozens of different environments via constrained or robust optimization. However, the Gaussian Process (GP) models typically used as probabilistic surrogates for multi-task Bayesian optimization scale poorly with the number of outcomes, greatly limiting applicability. We devise an efficient technique for exact multi-task GP sampling that combines exploiting Kronecker structure in the covariance matrices with Matheron's identity, allowing us to perform Bayesian optimization using exact multi-task GP models with tens of thousands of correlated outputs. In doing so, we achieve substantial improvements in sample efficiency compared to existing approaches that model solely the outcome metrics. We demonstrate how this unlocks a new class of applications for Bayesian optimization across a range of tasks in science and engineering, including optimizing interference patterns of an optical interferometer with 65,000 outputs.},
  file = {/home/marnix/Zotero/storage/A5YBPWNY/Maddox et al. - 2021 - Bayesian Optimization with High-Dimensional Output.pdf}
}

@article{Magli2003,
  title = {{{MAP}} Harmonic Retrieval in {{Gaussian}} Colored Noise with Prior Information},
  author = {Magli, Enrico and Olmo, Gabriella},
  year = {2003},
  month = jul,
  journal = {Digital Signal Processing},
  volume = {13},
  number = {3},
  pages = {530--551},
  issn = {1051-2004},
  doi = {10.1016/S1051-2004(03)00007-1},
  urldate = {2019-03-27},
  abstract = {In this paper we deal with the MAP estimation of the parameters of a single complex sinusoid in Gaussian white and colored noise. We analyze the effects of modeling the frequency and amplitude as random variables, deriving the expressions of the MAP estimators, and we analytically compute the CRB's for a tone with Gaussian frequency and amplitude distributions. Moreover, we explicitly derive closed-form expressions of the optimal estimators in the case of Gaussian and Rayleigh distributed amplitude. In particular, we show that, in the colored noise case, the amplitude information can also help improving the performance of the frequency estimator, and vice versa. We propose an efficient FFT-based implementation structure of the optimal estimators, which can be further simplified when the noise can be modeled as an AR process; besides, when the estimators cannot be expressed in closed form, we propose a simple univariate optimization algorithm in order to compute the estimates. Simulation results are reported, which validate the theoretical results and witness the significant performance improvement that can be achieved embodying the parameter priors into the frequency and amplitude estimators.},
  keywords = {DFT,Frequency estimation,Harmonic retrieval,MAP estimation,Random parameters},
  file = {/home/marnix/Zotero/storage/W6NBJFSU/Magli and Olmo - 2003 - MAP harmonic retrieval in Gaussian colored noise w.pdf;/home/marnix/Zotero/storage/R2XRFEYA/S1051200403000071.html}
}

@article{Maher2009,
  title = {Bayesian Probability},
  author = {Maher, Patrick},
  year = {2009},
  journal = {Synthese},
  volume = {172},
  number = {1},
  pages = {119--127},
  issn = {00397857},
  doi = {10.1007/s11229-009-9471-6},
  abstract = {Bayesian decision theory is here construed as explicating a particular concept of rational choice and Bayesian probability is taken to be the concept of probability used in that theory. Bayesian probability is usually identified with the agent{\"A}{\^o}s degrees of belief but that interpretation makes Bayesian decision theory a poor explication of the relevant concept of rational choice. A satisfactory conception of Bayesian decision theory is obtained by taking Bayesian probability to be an explicatum for inductive probability given the agent{\"A}{\^o}s evidence.},
  isbn = {0039-7857},
  pmid = {19897100},
  keywords = {Bayesian probability,Carnap,Decision theory,Degrees of belief,Expected utility,Explication,Inductive probability,Logical probability,Subjective probability},
  file = {/home/marnix/Zotero/storage/KNGGEVNH/Bayesian probability (KULeuven).pdf}
}

@article{Malliaros2013,
  title = {Clustering and {{Community Detection}} in {{Directed Networks}}: {{A Survey}}},
  author = {Malliaros, Fragkiskos D. and Vazirgiannis, Michalis},
  year = {2013},
  eprint = {1308.0971},
  pages = {1--86},
  issn = {03701573},
  doi = {10.1016/j.physrep.2013.08.002},
  abstract = {Networks (or graphs) appear as dominant structures in diverse domains, including sociology, biology, neuroscience and computer science. In most of the aforementioned cases graphs are directed - in the sense that there is directionality on the edges, making the semantics of the edges non symmetric. An interesting feature that real networks present is the clustering or community structure property, under which the graph topology is organized into modules commonly called communities or clusters. The essence here is that nodes of the same community are highly similar while on the contrary, nodes across communities present low similarity. Revealing the underlying community structure of directed complex networks has become a crucial and interdisciplinary topic with a plethora of applications. Therefore, naturally there is a recent wealth of research production in the area of mining directed graphs - with clustering being the primary method and tool for community detection and evaluation. The goal of this paper is to offer an in-depth review of the methods presented so far for clustering directed networks along with the relevant necessary methodological background and also related applications. The survey commences by offering a concise review of the fundamental concepts and methodological base on which graph clustering algorithms capitalize on. Then we present the relevant work along two orthogonal classifications. The first one is mostly concerned with the methodological principles of the clustering algorithms, while the second one approaches the methods from the viewpoint regarding the properties of a good cluster in a directed network. Further, we present methods and metrics for evaluating graph clustering results, demonstrate interesting application domains and provide promising future research directions.},
  archiveprefix = {arXiv},
  arxivid = {1308.0971},
  keywords = {1 rue honor,atiment alan turing,b{\textasciicircum},community detection,complex networks,corresponding author,directed networks,full postal address,graph clustering,graph mining,informatique,laboratoire d,lix},
  file = {/home/marnix/Zotero/storage/3JZMS2B4/Malliaros, Vazirgiannis - 2013 - Clustering and Community Detection in Directed Networks A Survey.pdf}
}

@inproceedings{Mandt2016,
  title = {Variational Tempering},
  booktitle = {Artificial Intelligence and Statistics},
  author = {Mandt, Stephan and McInerney, James and Abrol, Farhan and Ranganath, Rajesh and Blei, David},
  year = {2016},
  pages = {704--712},
  publisher = {PMLR},
  file = {/home/marnix/Zotero/storage/BPJMPQJW/Mandt et al. - 2016 - Variational tempering.pdf}
}

@book{Manilow2023,
  title = {Towards an {{Authorial Leverage Evaluation Framework}} for {{Expressive Benefits}} of {{Deep Generative Models}} in {{Story Writing}}},
  editor = {Manilow, Ethan and McDowell, Kenric and Mathewson, Kory and Mirowski, Piotr and Zhang, Richard and Chen, Sherol},
  year = {2023}
}

@article{Maragos1991,
  title = {Speech Nonlinearities, Modulations, and Energy Operators},
  author = {Maragos, Petros and Quatierit, Thomas F and Kaise, James F},
  year = {1991},
  number = {8},
  file = {/home/marnix/Zotero/storage/FU4QIMUW/Maragos1991 Speech nonlinearities, modulations, and energy operators.pdf}
}

@article{Marian2006,
  title = {Adaptive Chirp-Based Time -- Frequency Analysis of Speech Signals},
  author = {Mari{\'a}n, K. and Weruaga, Luis},
  year = {2006},
  volume = {48},
  pages = {474--492},
  doi = {10.1016/j.specom.2005.08.004},
  keywords = {fan-chirp transform,frequency analysis,harmonically related chirps,time},
  file = {/home/marnix/Zotero/storage/99QUPS7X/Kepesi2005 Adaptive chirp-based time–frequency analysis of speech signals.pdf}
}

@inproceedings{Mark2003,
  title = {Particle {{Filtering Approach}} to {{Bayesian Formant Tracking}}},
  booktitle = {Proc. {{IEEE Workshop}} on {{Statistical Signal Processing}}},
  author = {Mark, Yanli Zheng and {Hasegawa-johnson}, Mark},
  year = {2003},
  abstract = {This paper presents Particle Filtering Approach to Bayesian Formant Tracking. Explicit nonlinear formulas have been developed to map psd (power spectral density) of speech signal to formant frequencies. Formant tracking is formulated as a nonlinear Bayesian tracking problem and solved by particle filtering approach},
  file = {/home/marnix/Zotero/storage/EWWS2HSY/Yanli Zheng and Hasegawa-Johnson - 2003 - Particle filtering approach to bayesian formant tr.pdf;/home/marnix/Zotero/storage/IKSAF2P2/summary.html}
}

@book{Markel1976,
  title = {Linear {{Prediction}} of {{Speech}}},
  author = {Markel, John D. and Gray, Augustine H.},
  year = {1976},
  series = {Communication and {{Cybernetics}}},
  volume = {12},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-66286-7},
  urldate = {2019-08-02},
  isbn = {978-3-642-66288-1 978-3-642-66286-7},
  file = {/home/marnix/Zotero/storage/GSLDY95D/Markel and Gray - 1976 - Linear Prediction of Speech.pdf}
}

@article{Marriott1995,
  title = {Bayesian Analysis of {{ARMA}} Processes: {{Complete}} Sampling-Based Inferences under Full Likelihood},
  author = {Marriott, J},
  year = {1995},
  journal = {Bayesian Statistics and Econometrics: Essays in Honor of Arnold Zellner},
  file = {/home/marnix/Zotero/storage/YCAHZVG3/Marriott - 1995 - Bayesian analysis of ARMA processes Complete samp.pdf}
}

@article{Mathews1961,
  title = {Pitch {{Synchronous Analysis}} of {{Voiced Sounds}}},
  author = {Mathews, M. V. and Miller, Joan E. and David, E. E.},
  year = {1961},
  month = feb,
  journal = {The Journal of the Acoustical Society of America},
  volume = {33},
  number = {2},
  pages = {179--186},
  issn = {0001-4966},
  doi = {10.1121/1.1908614},
  urldate = {2023-03-08},
  langid = {english},
  file = {/home/marnix/Zotero/storage/7P44KVK3/Mathews et al. - 1961 - Pitch Synchronous Analysis of Voiced Sounds.pdf}
}

@misc{Matthews2018,
  title = {Gaussian {{Process Behaviour}} in {{Wide Deep Neural Networks}}},
  author = {Matthews, Alexander G. de G. and Rowland, Mark and Hron, Jiri and Turner, Richard E. and Ghahramani, Zoubin},
  year = {2018},
  month = aug,
  number = {arXiv:1804.11271},
  eprint = {1804.11271},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1804.11271},
  urldate = {2025-09-01},
  abstract = {Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between random, wide, fully connected, feedforward networks with more than one hidden layer and Gaussian processes with a recursive kernel definition. We show that, under broad conditions, as we make the architecture increasingly wide, the implied random function converges in distribution to a Gaussian process, formalising and extending existing results by Neal (1996) to deep networks. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then compare finite Bayesian deep networks from the literature to Gaussian processes in terms of the key predictive quantities of interest, finding that in some cases the agreement can be very close. We discuss the desirability of Gaussian process behaviour and review non-Gaussian alternative models from the literature.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/Z54FQSYA/Matthews et al. - 2018 - Gaussian Process Behaviour in Wide Deep Neural Networks.pdf;/home/marnix/Zotero/storage/LMC6J7I6/1804.html}
}

@book{Maurer2016,
  title = {Acoustics of the {{Vowel}}},
  author = {Maurer, Dieter},
  year = {2016},
  publisher = {Peter Lang},
  address = {Bern, Switzerland},
  isbn = {978-3-0343-2617-9},
  file = {/home/marnix/Zotero/storage/XNF7YR4S/Maurer - 2016 - Acoustics of the Vowel-Preliminaries.pdf}
}

@article{May2010,
  title = {Systemic Risk: The Dynamics of Model Banking Systems},
  author = {May, Robert M and Arinaminpathy, Nimalan},
  year = {2010},
  journal = {Journal Of The Royal Society Interface},
  volume = {7},
  number = {46},
  pages = {823--838},
  issn = {1742-5662},
  doi = {10.1098/rsif.2009.0359},
  abstract = {The recent banking crises have made it clear that increasingly complex strategies for managing risk in individual banks have not been matched by corresponding attention to overall systemic risks. We explore some simple mathematical caricatures for `banking ecosystems', with emphasis on the interplay between the characteristics of individual banks (capital reserves in relation to total assets, etc.) and the overall dynamical behaviour of the system. The results are discussed in relation to potential regulations aimed at reducing systemic risk.},
  isbn = {1742-5662 (Electronic)\r1742-5662 (Linking)},
  pmid = {19864264},
  keywords = {banking system,financial networks,systemic risk},
  file = {/home/marnix/Zotero/storage/F3JX2X9I/May, Arinaminpathy - 2010 - Systemic risk the dynamics of model banking systems.pdf}
}

@article{McCandless1974,
  title = {An Algorithm for Automatic Formant Extraction Using Linear Prediction Spectra},
  author = {McCandless, S.},
  year = {1974},
  month = apr,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {22},
  number = {2},
  pages = {135--141},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1974.1162559},
  abstract = {An algorithm is presented which finds the frequency and amplitude of the first three formants during all vowel-like segments of continuous speech. It uses as input the peaks of the linear prediction spectra and a segmentation parameter to indicate energy and voicing. Ideally, the first three peaks are the first three formants. Frequently, however, two peaks merge, or spurious peaks appear, and the difficult part is to recognize such situations and deal with them. The general method is to fill formant slots with the available peaks at each frame, based on frequency position relative to an educated guess. Then, if a peak is left over and/or a slot is unfilled, special routines are called to decide how to deal with them. Included is a formant enhancement technique, analogous to a similar technique which has been implemented via the chirp-z transform [8], which usually succeeds in separating two merged formants. Processing begins at the middle of each high volume voiced segment, where formants are most likely to be correct, and branches outward from there in both directions in time, using the most recently found formant frequencies as the educated guess for the current frame. The algorithm has been implemented at Lincoln Laboratory on the Univac 1219 and the Fast Digital Processor, a programmable processor [9], and has been tested on a large number of unrestricted sentences.},
  keywords = {Chirp,Frequency synthesizers,Laboratories,Spectral shape,Speech analysis,Speech processing,Speech recognition,Speech synthesis,Testing},
  file = {/home/marnix/Zotero/storage/TY7JVQR2/McCandless - 1974 - An algorithm for automatic formant extraction usin.pdf;/home/marnix/Zotero/storage/82KISZH3/1162559.html}
}

@book{McGilchrist2019,
  title = {The Master and His Emissary: {{The}} Divided Brain and the Making of the {{Western}} World: {{New}} Expanded Edition, 2nd Ed},
  shorttitle = {The Master and His Emissary},
  author = {McGilchrist, Iain},
  year = {2019},
  series = {The Master and His Emissary: {{The}} Divided Brain and the Making of the {{Western}} World: {{New}} Expanded Edition, 2nd Ed},
  pages = {xxvi, 588},
  publisher = {Yale University Press},
  address = {New Haven, CT, US},
  abstract = {This book sets out to understand the structure of the human brain---the place where mind meets matter. Until recently, the left hemisphere of our brain has been seen as the 'rational' side, the superior partner to the right. Drawing on a vast body of experimental research, the book argues that while our left brain makes for a wonderful servant, it is a very poor master. As it shows, it is the right side which is the more reliable and insightful. Without it, our world would be mechanistic--stripped of depth, colour and value. The book tells a story about ourselves and the world, and about how we got to be where we are now. While much of it is about the structure of the human brain---the place where mind meets matter---ultimately it is an attempt to understand the structure of the world that the brain has in part created. The book is divided, like the brain it describes, into two parts. Part I, focuses on the brain itself, and what it can tell us. It looks at the evolution of the brain, its divided and asymmetrical nature, and the implications of the development of music and language, and what one knows about what goes on in each side of the brain. Part II of the book looks at the history of Western culture in the light of what is believed about the hemispheres. These thoughts are inevitably contingent, to some extent fragmentary and rudimental. But if the world is not independent of our observation of it attention to it and interaction with it and if the mind is at least mediated by the brain, it seems a reasonable bet that the brain will have left its mark on the world that one have brought about. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  isbn = {978-0-300-24592-9},
  keywords = {Attention,Brain,Creativity,Culture (Anthropological),Language,Left Hemisphere,Mind,Music,Right Hemisphere},
  file = {/home/marnix/Zotero/storage/5BKBEKMR/2019-39475-000.html}
}

@article{McGoff2015,
  title = {Consistency of Maximum Likelihood Estimation for Some Dynamical Systems},
  author = {McGoff, Kevin and Mukherjee, Sayan and Nobel, Andrew and Pillai, Natesh},
  year = {2015},
  journal = {The Annals of Statistics},
  volume = {43},
  number = {1},
  eprint = {1306.5603},
  pages = {1--29},
  issn = {0090-5364},
  doi = {10.1214/14-AOS1259},
  abstract = {We consider the asymptotic consistency of maximum likelihood parameter estimation for dynamical systems observed with noise. Under suitable conditions on the dynamical systems and the observations, we show that maximum likelihood parameter estimation is consistent. Our proof involves ideas from both information theory and dynamical systems. Furthermore, we show how some well-studied properties of dynamical systems imply the general statistical properties related to maximum likelihood estimation. Finally, we exhibit classical families of dynamical systems for which maximum likelihood estimation is consistent. Examples include shifts of finite type with Gibbs measures and Axiom A attractors with SRB measures.},
  archiveprefix = {arXiv},
  arxivid = {1306.5603},
  keywords = {37A25,37A50,37D20,6,60F10,62B10,62F12,62M09},
  file = {/home/marnix/Zotero/storage/387FSHG7/McGoff2015 CONSISTENCY OF MAXIMUM LIKELIHOOD ESTIMATION FOR SOME DYNAMICAL SYSTEMS.pdf}
}

@article{McKay2003,
  title = {Cycle Graphs and Random Permutations},
  author = {McKay, David J. C.},
  year = {2003},
  journal = {Information Theory, Inference, and Learning Algorithms},
  pages = {1--4},
  issn = {0924-9265},
  doi = {10.1515/DMA.2008.002},
  file = {/home/marnix/Zotero/storage/UYN7NTQ6/Properties of random permutations.pdf}
}

@article{McKay2003a,
  title = {Acyclic Digraphs and Eigenvalues of (0, 1)-Matrices},
  author = {McKay, Brendan D and Oggier, Fr{\'e}d{\'e}rique E and Royle, Gordon F and Sloane, {\relax NJA} and Wanless, Ian M and Wilf, Herbert S},
  year = {2003},
  journal = {arXiv preprint math/0310423},
  eprint = {math/0310423},
  archiveprefix = {arXiv},
  file = {/home/marnix/Zotero/storage/EHZ4RA9G/McKay et al. - Acyclic Digraphs and Eigenvalues of (0, 1)–Matrice.pdf}
}

@article{Mcneil,
  title = {Risk in Perspective},
  author = {Mcneil, Alexander J and Frey, R{\"u}diger and Embrechts, Paul},
  file = {/home/marnix/Zotero/storage/NEYNUFBZ/Mcneil, Frey, Embrechts - Unknown - No Title.pdf}
}

@article{Mehta2012,
  title = {Kalman-Based Autoregressive Moving Average Modeling and Inference for Formant and Antiformant Tracking},
  author = {Mehta, Daryush D. and Rudoy, Daniel and Wolfe, Patrick J.},
  year = {2012},
  journal = {The Journal of the Acoustical Society of America},
  volume = {132},
  number = {3},
  eprint = {https://doi.org/10.1121/1.4739462},
  pages = {1732--1746},
  doi = {10.1121/1.4739462},
  file = {/home/marnix/Zotero/storage/KI9WCVTJ/Mehta et al. - 2012 - Kalman-based autoregressive moving average modelin.pdf}
}

@article{Mehta2014,
  title = {Erratum: {{Kalman-based}} Autoregressive Moving Average Modeling and Inference for Formant and Antiformant Tracking [{{J}}. {{Acoust}}. {{Soc}}. {{Am}}. 132(3), 1732--1746 (2012)]},
  shorttitle = {Erratum},
  author = {Mehta, Daryush D. and Rudoy, Daniel and Wolfe, Patrick J.},
  year = {2014},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {135},
  number = {5},
  pages = {3128--3128},
  publisher = {Acoustical Society of America},
  issn = {0001-4966},
  doi = {10.1121/1.4869982},
  urldate = {2021-01-12},
  file = {/home/marnix/Zotero/storage/NET43VHA/1.html}
}

@article{Mehta2015,
  title = {Statistical Properties of Linear Prediction Analysis Underlying the Challenge of Formant Bandwidth Estimation},
  author = {Mehta, Daryush D. and Wolfe, Patrick J.},
  year = {2015},
  month = feb,
  journal = {The Journal of the Acoustical Society of America},
  volume = {137},
  number = {2},
  pages = {944--950},
  issn = {0001-4966},
  doi = {10.1121/1.4906840},
  urldate = {2019-08-28},
  langid = {english},
  file = {/home/marnix/Zotero/storage/LA5JJ6WN/Mehta and Wolfe - 2015 - Statistical properties of linear prediction analys.pdf}
}

@misc{Mialon2023,
  title = {Augmented {{Language Models}}: A {{Survey}}},
  shorttitle = {Augmented {{Language Models}}},
  author = {Mialon, Gr{\'e}goire and Dess{\`i}, Roberto and Lomeli, Maria and Nalmpantis, Christoforos and Pasunuru, Ram and Raileanu, Roberta and Rozi{\`e}re, Baptiste and Schick, Timo and {Dwivedi-Yu}, Jane and Celikyilmaz, Asli and Grave, Edouard and LeCun, Yann and Scialom, Thomas},
  year = {2023},
  month = feb,
  number = {arXiv:2302.07842},
  eprint = {2302.07842},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.07842},
  urldate = {2023-05-10},
  abstract = {This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/marnix/Zotero/storage/688EFI4V/Mialon et al. - 2023 - Augmented Language Models a Survey.pdf;/home/marnix/Zotero/storage/N59UCNKX/2302.html}
}

@article{Milenkovic1986,
  title = {Glottal Inverse Filtering by Joint Estimation of an {{AR}} System with a Linear Input Model},
  author = {Milenkovic, P.},
  year = {1986},
  month = feb,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {34},
  number = {1},
  pages = {28--42},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1986.1164778},
  abstract = {Glottal inverse filtering is a technique by which the flow past the time varying glottal constriction is estimated by a filtering operation on the acoustic signal in human speech. The filtering operation removes the effects of the vocal tract resonances to reveal the underlying voice source signal. A linear model of a glottal pulse waveform is described along with a procedure for jointly determining an AR model of the vocal tract response together with the parameters of the glottal pulse model. This technique is applied to inverse filtering in human speech and the results are compared to covariance method LPC analysis.},
  keywords = {Algorithm design and analysis,Filtering,Humans,Linear predictive coding,Nonlinear filters,Pulse shaping methods,Resonance,Shape,Speech analysis,Speech synthesis},
  file = {/home/marnix/Zotero/storage/YS5HF6EL/Milenkovic - 1986 - Glottal inverse filtering by joint estimation of a.pdf;/home/marnix/Zotero/storage/WS2CENWY/stamp.html}
}

@article{Miller1959,
  title = {Nature of the {{Vocal Cord Wave}}},
  author = {Miller, R. L.},
  year = {1959},
  month = jun,
  journal = {The Journal of the Acoustical Society of America},
  volume = {31},
  number = {6},
  pages = {667--677},
  issn = {0001-4966},
  doi = {10.1121/1.1907771},
  urldate = {2019-08-02},
  langid = {english},
  file = {/home/marnix/Zotero/storage/NMZHUCRE/Miller - 1959 - Nature of the Vocal Cord Wave.pdf}
}

@article{Miller1987,
  title = {Auditory-perceptual Interpretation of the Vowel},
  author = {Miller, James D.},
  year = {1987},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {81},
  number = {S1},
  pages = {S16-S16},
  issn = {0001-4966},
  doi = {10.1121/1.2024119},
  urldate = {2021-01-06},
  langid = {english},
  file = {/home/marnix/Zotero/storage/VQRANJQW/Miller - 1987 - Auditory‐perceptual interpretation of the vowel.pdf}
}

@article{Ming2007,
  title = {Robust {{Speaker Recognition}} in {{Noisy Conditions}}},
  author = {Ming, Ji and Hazen, Timothy J. and Glass, James R. and Reynolds, Douglas A. and {Ming}},
  year = {2007},
  journal = {IEEE Transactions on Audio, Speech and Language Processing},
  volume = {15},
  number = {5},
  pages = {1711--1723},
  issn = {15587916},
  doi = {10.1109/TASL.2007.899278},
  abstract = {para This paper investigates the problem of speaker identification and verification in noisy conditions, assuming that speech signals are corrupted by environmental noise, but knowledge about the noise characteristics is not available. This research is motivated in part by the potential application of speaker recognition technologies on handheld devices or the Internet. While the technologies promise an additional biometric layer of security to protect the user, the practical implementation of such systems faces many challenges. One of these is environmental noise. Due to the mobile nature of such systems, the noise sources can be highly time-varying and potentially unknown. This raises the requirement for noise robustness in the absence of information about the noise. This paper describes a method that combines multicondition model training and missing-feature theory to model noise with unknown temporal-spectral characteristics. Multicondition training is conducted using simulated noisy data with limited noise variation, providing a \&ldquo;coarse\&rdquo; compensation for the noise, and missing-feature theory is applied to refine the compensation by ignoring noise variation outside the given training conditions, thereby reducing the training and testing mismatch. This paper is focused on several issues relating to the implementation of the new model for real-world applications. These include the generation of multicondition training data to model noisy speech, the combination of different training data to optimize the recognition performance, and the reduction of the model's complexity. The new algorithm was tested using two databases with simulated and realistic noisy speech data. The first database is a redevelopment of the TIMIT database by rerecording the data in the presence of various noise types, used to test the model for speaker identification with a focus on the varieties of noise. The second database is a handheld-device database collected in realisti- c noisy conditions, used to further validate the model for real-world speaker verification. The new model is compared to baseline systems and is found to achieve lower error rates. /para},
  isbn = {9783319071299},
  keywords = {Missing-feature theory,Multicondition training,Noise compensation,Noise modeling,Speaker recognition},
  file = {/home/marnix/Zotero/storage/BPV47IAB/Ming2007 Robust Speaker Recognition in Noisy Conditions.pdf;/home/marnix/Zotero/storage/FDNRJTXT/Ming2007 Robust Speaker Recognition in Noisy Conditions.pdf}
}

@article{Mitra2011,
  title = {Articulatory Information for Noise Robust Speech Recognition},
  author = {Mitra, Vikramjit and Nam, Hosung and {Espy-Wilson}, Carol Y. and Saltzman, Elliot and Goldstein, Louis},
  year = {2011},
  journal = {IEEE Transactions on Audio, Speech and Language Processing},
  volume = {19},
  number = {7},
  pages = {1913--1924},
  issn = {15587916},
  doi = {10.1109/TASL.2010.2103058},
  abstract = {Prior research has shown that articulatory information, if extracted properly from the speech signal, can improve the performance of automatic speech recognition systems. However, such information is not readily available in the signal. The challenge posed by the estimation of articulatory information from speech acoustics has led to a new line of research known as \&\#x201C;acoustic-to-articulatory inversion\&\#x201D; or \&\#x201C;speech-inversion.\&\#x201D; While most of the research in this area has focused on estimating articulatory information more accurately, few have explored ways to apply this information in speech recognition tasks. In this paper, we first estimated articulatory information in the form of vocal tract constriction variables (abbreviated as TVs) from the Aurora-2 speech corpus using a neural network based speech-inversion model. Word recognition tasks were then performed for both noisy and clean speech using articulatory information in conjunction with traditional acoustic features. Our results indicate that incorporating TVs can significantly improve word recognition rates when used in conjunction with traditional acoustic features.},
  keywords = {Articulatory phonology,articulatory speech recognition,artificial neural networks (ANNs),noise-robust speech recognition,speech inversion,task dynamic model,vocal-tract variables},
  file = {/home/marnix/Zotero/storage/S9R39PQ9/Mitra2011 Articulatory Information for Noise Robust SR.pdf}
}

@article{Moffat1998,
  title = {Arithmetic Coding Revisited},
  author = {Moffat, Alistair and Neal, Radford M and Witten, Ian H},
  year = {1998},
  journal = {ACM Transactions on Information Systems (TOIS)},
  volume = {16},
  number = {3},
  pages = {256--294},
  publisher = {ACM New York, NY, USA},
  file = {/home/marnix/Zotero/storage/DBQCXZYC/Moffat et al. - 1998 - Arithmetic coding revisited.pdf}
}

@article{Mohammad-Djafari2015,
  title = {Entropy, {{Information Theory}}, {{Information Geometry}} and {{Bayesian Inference}} in {{Data}}, {{Signal}} and {{Image Processing}} and {{Inverse Problems}}},
  author = {{Mohammad-Djafari}, Ali},
  year = {2015},
  month = jun,
  journal = {Entropy},
  volume = {17},
  number = {6},
  pages = {3989--4027},
  doi = {10.3390/e17063989},
  urldate = {2019-04-23},
  abstract = {The main content of this review article is first to review the main inference tools using Bayes rule, the maximum entropy principle (MEP), information theory, relative entropy and the Kullback--Leibler (KL) divergence, Fisher information and its corresponding geometries. For each of these tools, the precise context of their use is described. The second part of the paper is focused on the ways these tools have been used in data, signal and image processing and in the inverse problems, which arise in different physical sciences and engineering applications. A few examples of the applications are described: entropy in independent components analysis (ICA) and in blind source separation, Fisher information in data model selection, different maximum entropy-based methods in time series spectral estimation and in linear inverse problems and, finally, the Bayesian inference for general inverse problems. Some original materials concerning the approximate Bayesian computation (ABC) and, in particular, the variational Bayesian approximation (VBA) methods are also presented. VBA is used for proposing an alternative Bayesian computational tool to the classical Markov chain Monte Carlo (MCMC) methods. We will also see that VBA englobes joint maximum a posteriori (MAP), as well as the different expectation-maximization (EM) algorithms as particular cases.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Bayes,Bayesian inference,entropy,Fisher information,geometrical science of information,information theory,inverse problems,Kullback--Leibler divergence,Laplace,maximum entropy principle},
  file = {/home/marnix/Zotero/storage/72YFL8II/Mohammad-Djafari - 2015 - Entropy, Information Theory, Information Geometry .pdf;/home/marnix/Zotero/storage/QXLMS5X6/htm.html}
}

@inproceedings{Mohar1991,
  title = {The {{Laplacian}} Spectrum of Graphs},
  booktitle = {Graph {{Theory}}, {{Combinatorics}}, and {{Applications}}},
  author = {Mohar, Bojan},
  year = {1991},
  volume = {2},
  eprint = {0801.0278},
  pages = {871--898},
  issn = {01956698},
  doi = {10.1006/eujc.2002.0592},
  abstract = {Abstract. The paper is essentially a survey of known results about the spectrum of the Laplacian matrix of graphs with special emphasis on the second smallest Lapla-cian eigenvalue {$\lambda$}2 and its relation to numerous graph invariants, including connectivity, expanding properties, isoperimetric number, maximum cut, independence number, genus, diameter, mean distance, and bandwidth-type parameters of a graph. Some new results and generalizations are added. \{{\textbackslash}dag\} This article appeared in '' Graph Theory, Combinatorics, and Applications'', Vol. 2,},
  archiveprefix = {arXiv},
  arxivid = {0801.0278},
  isbn = {978-0-471-53245-3},
  keywords = {fiedler,graph-partitioning,laplacian spectrum of graphs,ref00241},
  file = {/home/marnix/Zotero/storage/QLIF32JR/Mohar - 1991 - The Laplacian spectrum of graphs.pdf}
}

@article{Molina-Borboa2015,
  title = {A Multiplex Network Analysis of the {{Mexican}} Banking System : Link Persistence , Overlap},
  author = {{Molina-Borboa}, J. and {Mar{\'t}inez-Jaramillo}, S. and {Lopez-Gallo}, F.},
  year = {2015},
  journal = {Journal of Network Thoery in Finance},
  volume = {1},
  number = {1},
  pages = {99--138},
  abstract = {This paper analyzes the persistence and overlap of relationships between banks in{\textbackslash}na multiplex decomposition of the exposures network. Our analysis may be useful{\textbackslash}nfor researchers designing stress tests or models in which the behavior of banks is{\textbackslash}nmodeled explicitly. This has not been looked at previously, considering the time{\textbackslash}nperiod involved and the different types of exposures and interactions used.We show{\textbackslash}nthat trading relationships overlap for some pairs of banks, and link persistence is{\textbackslash}nhigher in the secured than the unsecured market. Moreover, link persistence in the{\textbackslash}nsecurities cross-holding network is much higher than in other funding networks, and{\textbackslash}noverlap with the other segments of activity is low, despite being persistent over time.Additionally, unsecured loans received by large banks have the shortest waiting times{\textbackslash}n(that is, for a given borrower and a given lender, the number of days elapsed before a{\textbackslash}nnew loan is observed) regardless of counterparty size, which suggests quicker access{\textbackslash}nto liquidity. Large banks lend (unsecured) with shorter waiting times to medium-sized{\textbackslash}nbanks than to small banks; this is not the case when they lend in the secured layer{\textbackslash}nof the network. Small banks have quicker access to liquidity in the secured lending{\textbackslash}nlayer when borrowing from medium-sized banks.},
  file = {/home/marnix/Zotero/storage/LKH2S3Z4/Molina-Borboa, López-Gallo, van der Leij - 2015 - A multiplex network analysis of the mexican banking system link persistence, overlap.pdf}
}

@article{Monahan1984,
  title = {A Note on Enforcing Stationarity in Autoregressive-Moving Average Models},
  author = {Monahan, John F.},
  year = {1984},
  month = aug,
  journal = {Biometrika},
  volume = {71},
  number = {2},
  pages = {403--404},
  issn = {0006-3444},
  doi = {10.1093/biomet/71.2.403},
  urldate = {2024-02-22},
  abstract = {A simple reparameterization is given that can implicitly restrict the autoregressive moving average parameters to the stationary and invertible region.},
  file = {/home/marnix/Zotero/storage/4F4BU5EG/Monahan - 1984 - A note on enforcing stationarity in autoregressive.pdf;/home/marnix/Zotero/storage/MBNKVS7A/233548.html}
}

@article{Montagna2013,
  title = {Multi-Layered Interbank Model for Assessing Systemic Risk},
  author = {Montagna, Mattia and Kok, Christoffer and {Others} and Montagna, Mattia},
  year = {2013},
  number = {1873},
  pages = {1--42},
  publisher = {European Central Bank},
  abstract = {In this paper, we develop an agent-based multi-layered interbank network model based on a sample of large EU banks. The model allows for taking a more holistic approach to interbank contagion than is standard in the literature. A key finding of the paper is that there are non-negligible non-linearities in the propagation of shocks to individual banks when taking into account that banks are related to each other in various market segments. In a nutshell, the contagion effects when considering the shock propagation simultaneously across multiple layers of interbank networks can be substantially larger than the sum of the contagion-induced losses when considering the network layers individually. In addition, a bank ``systemic importance'' measure based on the multi-layered network model is developed and is shown to outperform standard network centrality indicators.},
  isbn = {9789289921923},
  keywords = {financial contagion,interbank market,network theory},
  file = {/home/marnix/Zotero/storage/3E6B25NN/Montagna, Kok, others - 2013 - Multi-layered interbank model for assessing systemic risk.pdf}
}

@article{Moore2001,
  title = {Constraints on {{Theories}} of {{Human}} vs. {{Machine Recognition}} of {{Speech}}},
  author = {Moore, Roger K. and Cutler, Anne},
  year = {2001},
  journal = {SPRAAC Workshop on Speech Recognition as Pattern Classification},
  pages = {145--150},
  abstract = {The central issues in the study of speech recognition by human listeners (HSR) and of automatic speech recognition (ASR) are clearly comparable; nevertheless the research communities that concern themselves with ASR and HSR are largely distinct. This paper compares the research objectives of the two fields, and attempts to draw informative lessons from one to the other.},
  file = {/home/marnix/Zotero/storage/REEDLN6C/Moore human versus machine.pdf}
}

@article{Moore2003,
  title = {A Comparison of the Data Requirements of Automatic Speech Recognition Systems and Human Listeners},
  author = {Moore, R. K.},
  year = {2003},
  journal = {8th European Conference on Speech Communication and Technology (Eurospeech)},
  volume = {3},
  pages = {2582--2584},
  abstract = {Since the introduction of hidden Markov modelling there has been an increasing emphasis on data-driven approaches to automatic speech recognition. This derives from the fact that systems trained on substantial corpora readily outperform those that rely on more phonetic or linguistic priors. Similarly, extra training data almost always results in a reduction in word error rate - {\dbend}there's no data like more data{\dbend}. However, despite this progress, contemporary systems are not able to fulfill the requirements demanded by many potential applications, and performance is still significantly short of the capabilities exhibited by human listeners. For these reasons, the R\&D community continues to call for even greater quantities of data in order to train their systems. This paper addresses the issue of just how much data might be required in order to bring the performance of an automatic speech recognition system up to that of a human listener.},
  file = {/home/marnix/Zotero/storage/LAHNEIG4/moore human machine comparison.pdf}
}

@article{Moore2016,
  title = {Fast Methods for Training {{Gaussian}} Processes on Large Data Sets},
  author = {Moore, Christopher J. and Chua, Alvin J. K. and Berry, Christopher P. L. and Gair, Jonathan R.},
  year = {2016},
  month = may,
  journal = {Royal Society Open Science},
  volume = {3},
  number = {5},
  eprint = {1604.01250},
  pages = {160125},
  issn = {2054-5703, 2054-5703},
  doi = {10.1098/rsos.160125},
  urldate = {2020-04-22},
  abstract = {Gaussian process regression (GPR) is a non-parametric Bayesian technique for interpolating or fitting data. The main barrier to further uptake of this powerful tool rests in the computational costs associated with the matrices which arise when dealing with large data sets. Here, we derive some simple results which we have found useful for speeding up the learning stage in the GPR algorithm, and especially for performing Bayesian model comparison between different covariance functions. We apply our techniques to both synthetic and real data and quantify the speed-up relative to using nested sampling to numerically evaluate model evidences.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/marnix/Zotero/storage/7ZFXGRSQ/Moore et al. - 2016 - Fast methods for training Gaussian processes on la.pdf;/home/marnix/Zotero/storage/VPPWBDAH/1604.html}
}

@inproceedings{Mudd2022,
  title = {The Effect of Network Structure on Lexical Variablity in Sign Language Communities},
  booktitle = {The Evolution of Language: {{Proceedings}} of the Joint Conference on Language Evolution ({{JCoLE}})},
  author = {Mudd, Katie and Van Soom, Marnix and {de Vos}, Connie and {de Boer}, Bart},
  editor = {Ravignani, Andrea and Asano, Rie and Valente, Daria and Ferretti, Francesco and Hartmann, Stefan and Hayashi, Misato and Jadoul, Yannick and Martins, Mauricio and Oseki, Yohei and Rodrigues, Evelina Daniela},
  year = {2022},
  publisher = {Joint Conference on Language Evolution (JCoLE)},
  address = {Nijmegen},
  doi = {10.17617/2.3398549},
  organization = {Joint Conference on Language Evolution (JCoLE)},
  file = {/home/marnix/Zotero/storage/HFCCLE3W/Mudd et al. - 2022 - The effect of network structure on lexical variabl.pdf}
}

@article{Mumford2000,
  title = {The Dawning of the Age of Stochasticity},
  author = {Mumford, David},
  year = {2000},
  journal = {Mathematics: frontiers and perspectives},
  pages = {197--218},
  publisher = {American Mathematical Society},
  issn = {03118002},
  doi = {10.1038/35044500},
  abstract = {Summary: \{"For\} over two millennia, Aristotle's logic has ruled over the thinking of{\textbackslash}nwestern intellectuals. All precise theories, all scientific models, even models of the process of thinking itself,{\textbackslash}nhave in principle conformed to the straight-jacket of logic. But from its shady beginnings devising gambling strategies{\textbackslash}nand counting corpses in medieval London, probability theory and statistical inference now emerge as better foundations{\textbackslash}nfor scientific models, especially those of the process of thinking, and as essential ingredients of{\textbackslash}ntheoretical mathematics, even the foundations of mathematics itself. We propose that this sea change in{\textbackslash}nour perspective will affect virtually all of mathematics in the next century."{\textbackslash}n{\textbackslash}nContents: 1. Introduction. 2. The taxonomy of mathematics. 3. A brief history of logic vs. statistics.{\textbackslash}n4. What is a "random variable"? 5. Putting random variables into the foundations (this \${\dbend}\$ rests on Jaynes'{\textbackslash}nand Freiling's ideas). 6. Stochastic methods have invaded classical mathematics. 7. Thinking as Bayesian inference.{\textbackslash}n{\textbackslash}n\{\{For\} the entire collection see \{MR1754762\} (2000m:00017).\}},
  isbn = {9780821826973},
  pmid = {11257898},
  file = {/home/marnix/Zotero/storage/XG4V3WFV/Mumford - 2000 - The dawning of the age of stochasticity.pdf}
}

@article{Murphy2002,
  title = {Dynamic {{Bayesian Networks}}: {{Representation}}, {{Inference}} and {{Learning}}},
  author = {Murphy, Kp},
  year = {2002},
  journal = {Chemistry \& {\dots}},
  abstract = {Modelling sequential data is important in many areas of science and engineering. Hidden Markov models (HMMs) and Kalman filter models (KFMs) are popular for this because they are simple and flexible. For example, HMMs have been used for speech recognition and bio-sequence analysis, and KFMs have been used for problems ranging from tracking planes and missiles to predicting the economy. However, HMMs and KFMs are limited in their ``expressive power''. Dynamic Bayesian Networks (DBNs) generalize HMMs by allowing the state space to be represented in factored form, instead of as a single discrete random variable. DBNs generalize KFMs by allowing arbitrary probability distributions, not just (unimodal) linear-Gaussian. In this thesis, I will discuss how to represent many different kinds of models as DBNs, how to perform exact and approximate inference in DBNs, and how to learn DBN models from sequential data. In particular, the main novel technical contributions of this thesis are as follows: a way of representing Hierarchical HMMs as DBNs, which enables inference to be done in O(T) time instead of O(T 3), where T is the length of the sequence; an exact smoothing algorithm that takes O(log T) space instead of O(T); a simple way of using the junction tree algorithm for online inference in DBNs; new complexity bounds on exact online inference in DBNs; a new deterministic approximate inference algorithm called factored frontier; an analysis of the relationship between the BK algorithm and loopy belief propagation; a way of applying Rao-Blackwellised particle filtering to DBNs in general, and the SLAM (simultaneous localization and mapping) problem in particular; a way of extending the structural EM algorithm to DBNs; and a variety of different applications of DBNs. However, perhaps the main value of the thesis is its catholic presentation of the field of sequential data modelling. 1 ACKNOWLEDGMENTS I would like to thank my advisor, Stuart Russell, for supporting me over the years, and for giving me so much freedom to explore and discover new areas of probabilistic AI. My other committee members have also been very supportive. Michael Jordan has long been an inspiration to me. His classes and weekly meetings have proved to be one of my best learning experiences at Berkeley. Jeff Bilmes proved to be a most thorough reviewer, as I expected, and has kept me honest about all the details. Peter Bickel brought a useful outsider's perspective to the thesis, and encouraged me to make it more accessible to non computer scientists (although any failings in this regard are of course my fault). I would like to thank my many friends and colleagues at Berkeley with whom I have had the pleasure of working over the years. These include Eyal Amir, David Andre, Serge Belongie, Jeff Bilmes, Nancy Chang, Nando de Freitas, Nir Friedman, Paul Horton, Srini Narayanan, Andrew Ng, Mark Paskin, Sekhar Tatikonda, Yair Weiss, Erix Xing, Geoff Zweig, and all the members of the RUGS and IR groups. I would like to thank Jim Rehg for hiring me as an intern at DEC/Compaq/HP Cambridge Research Lab in 1997, where my Bayes Net Toolbox (BNT) was born. I would like to thank Gary Bradski for hiring me as an intern at Intel in 2000 to work on BNT, and for providing me with the opportunity to work with people spanning three countries formerly known as superpowers --- USA, China and Russia. In particular, I would like to thankWei Hu and Yimin Zhang, of ICRC, for their help with BNT. I would also like to thank the many people on the web who have contributed bug fixes to BNT. By chance, I was able to work with Sebastian Thrun during part of my time with Intel, for which I am very grateful. I would like to thank my friends in Jennie Nation and beyond for providing a welcome distraction from school. Finally, I would like to thank my wife Margaret for putting up with my weekends in the office, for listening to my sagas from Soda land, and for giving me the motivation to finish this thesis.},
  keywords = {dbn},
  file = {/home/marnix/Zotero/storage/6NFDJCWI/Murphy2002 Dynamic Bayesian Networks.pdf}
}

@article{Murphy2007,
  title = {Conjugate {{Bayesian}} Analysis of the {{Gaussian}} Distribution},
  author = {Murphy, Kevin P},
  year = {2007},
  journal = {DEF : Development education forum},
  volume = {1},
  number = {2},
  pages = {16},
  file = {/home/marnix/Zotero/storage/JGGF3SBL/Murphy - 2007 - Conjugate Bayesian analysis of the Gaussian distri.pdf}
}

@book{Murphy2012,
  title = {Machine Learning: A Probabilistic Perspective},
  shorttitle = {Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2012},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  isbn = {978-0-262-01802-9},
  langid = {english},
  lccn = {Q325.5 .M87 2012},
  keywords = {Machine learning,Probabilities},
  file = {/home/marnix/Zotero/storage/FVJ7YAHS/Murphy - 2012 - Machine learning a probabilistic perspective.pdf}
}

@book{Murphy2022,
  title = {Probabilistic Machine Learning: {{An}} Introduction},
  author = {Murphy, Kevin P.},
  year = {2022},
  publisher = {MIT Press},
  file = {/home/marnix/Zotero/storage/L8YFMTKF/Murphy - 2022 - Probabilistic machine learning An introduction.pdf}
}

@inproceedings{Murray-Smith2001,
  title = {Gaussian {{Process}} Priors with {{ARMA}} Noise Models},
  booktitle = {Irish Signals and Systems Conference, Maynooth},
  author = {{Murray-Smith}, Roderick and Girard, Agathe},
  year = {2001},
  volume = {147},
  pages = {152},
  file = {/home/marnix/Zotero/storage/D7BSRH65/Murray-Smith and Girard - 2001 - Gaussian Process priors with ARMA noise models.pdf}
}

@inproceedings{Murray2005,
  title = {Nested Sampling for {{Potts}} Models},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Murray, Iain and MacKay, David and Ghahramani, Zoubin and Skilling, John},
  year = {2005},
  volume = {18},
  publisher = {MIT Press},
  urldate = {2022-04-23},
  abstract = {Nested sampling is a new Monte Carlo method by Skilling [1] intended for general Bayesian computation. Nested sampling provides a robust alternative to annealing-based methods for computing normalizing constants. It can also generate estimates of other quantities such as posterior expectations. The key technical requirement is an ability to draw samples uniformly from the prior sub ject to a constraint on the likelihood. We provide a demonstration with the Potts model, an undirected graphical model.},
  file = {/home/marnix/Zotero/storage/TL3HFBYX/Murray et al. - 2005 - Nested sampling for Potts models.pdf}
}

@inproceedings{Murray2010,
  title = {Elliptical Slice Sampling},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Murray, Iain and Adams, Ryan and MacKay, David},
  year = {2010},
  month = mar,
  pages = {541--548},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  urldate = {2022-05-19},
  abstract = {Many probabilistic models introduce strong dependencies between variables using a latent multivariate Gaussian distribution or a Gaussian process. We present a new Markov chain Monte Carlo algorithm for performing inference in models with multivariate Gaussian priors. Its key properties are: 1) it has simple, generic code applicable to many models, 2) it has no free parameters, 3) it works well for a variety of Gaussian process based models. These properties make our method ideal for use while model building, removing the need to spend time deriving and tuning updates for more complex algorithms.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/GPVUBEGC/Murray et al. - 2010 - Elliptical slice sampling.pdf}
}

@inproceedings{Murray2010a,
  title = {Slice Sampling Covariance Hyperparameters of Latent {{Gaussian}} Models},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Murray, Iain and Adams, Ryan P},
  year = {2010},
  volume = {23},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-03-02},
  abstract = {The Gaussian process (GP) is a popular way to specify dependencies between random variables in a probabilistic model. In the Bayesian framework the covariance structure can be specified using unknown hyperparameters. Integrating over these hyperparameters considers different possible explanations for the data when making predictions. This integration is often performed using Markov chain Monte Carlo (MCMC) sampling. However, with non-Gaussian observations standard hyperparameter sampling approaches require careful tuning and may converge slowly. In this paper we present a slice sampling approach that requires little tuning while mixing well in both strong- and weak-data regimes.},
  file = {/home/marnix/Zotero/storage/A7WSBFGJ/Murray and Adams - 2010 - Slice sampling covariance hyperparameters of laten.pdf}
}

@misc{Murray2016,
  title = {Differentiation of the {{Cholesky}} Decomposition},
  author = {Murray, Iain},
  year = {2016},
  month = feb,
  number = {arXiv:1602.07527},
  eprint = {1602.07527},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-03-02},
  abstract = {We review strategies for differentiating matrix-based computations, and derive symbolic and algorithmic update rules for differentiating expressions containing the Cholesky decomposition. We recommend new `blocked' algorithms, based on differentiating the Cholesky algorithm DPOTRF in the LAPACK library, which uses `Level 3' matrix-matrix operations from BLAS, and so is cache-friendly and easy to parallelize. For large matrices, the resulting algorithms are the fastest way to compute Cholesky derivatives, and are an order of magnitude faster than the algorithms in common usage. In some computing environments, symbolically-derived updates are faster for small matrices than those based on differentiating Cholesky algorithms. The symbolic and algorithmic approaches can be combined to get the best of both worlds.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Mathematical Software,Statistics - Computation},
  file = {/home/marnix/Zotero/storage/7IDN3XST/Murray - 2016 - Differentiation of the Cholesky decomposition.pdf;/home/marnix/Zotero/storage/VHNJAYWS/1602.html}
}

@inproceedings{Murray2016a,
  title = {Pseudo-Marginal Slice Sampling},
  booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  author = {Murray, Iain and Graham, Matthew},
  editor = {Gretton, Arthur and Robert, Christian C.},
  year = {2016},
  month = may,
  series = {{{JMLR}}: {{W}}\&{{CP}}},
  volume = {51},
  pages = {911--919},
  address = {Cadiz, Spain},
  file = {/home/marnix/Zotero/storage/KXT2T3S6/Murray and Graham - Pseudo-Marginal Slice Sampling.pdf}
}

@article{Naylor2007,
  title = {Estimation of {{Glottal Closure Instants}} in {{Voiced Speech Using}} the {{DYPSA Algorithm}}},
  author = {Naylor, Patrick A. and Kounoudes, Anastasis and Gudnason, Jon and Brookes, Mike},
  year = {2007},
  month = jan,
  journal = {IEEE Transactions on Audio, Speech and Language Processing},
  volume = {15},
  number = {1},
  pages = {34--43},
  issn = {1558-7916},
  doi = {10.1109/TASL.2006.876878},
  urldate = {2022-04-14},
  abstract = {We present the Dynamic Programming Projected Phase-Slope Algorithm (DYPSA) for automatic estimation of glottal closure instants (GCIs) in voiced speech. Accurate estimation of GCIs is an important tool that can be applied to a wide range of speech processing tasks including speech analysis, synthesis and coding. DYPSA is automatic and operates using the speech signal alone without the need for an EGG signal. The algorithm employs the phase-slope function and a novel phase-slope projection technique for estimating GCI candidates from the speech signal. The most likely candidates are then selected using a dynamic programming technique to minimize a cost function that we define. We review and evaluate three existing methods of GCI estimation and compare the new DYPSA algorithm to them. Results are presented for the APLAWD and SAM databases for which 95.7\% and 93.1\% of GCIs are correctly identified.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/TJURRK5U/Naylor et al. - 2007 - Estimation of Glottal Closure Instants in Voiced S.pdf}
}

@article{Neal1993,
  title = {Probabilistic Inference Using {{Markov}} Chain {{Monte Carlo}} Methods},
  author = {Neal, Radford M},
  year = {1993},
  publisher = {Department of Computer Science, University of Toronto Toronto, Ontario, Canada},
  file = {/home/marnix/Zotero/storage/V34AC53D/Neal - 1993 - Probabilistic inference using Markov chain Monte C.pdf}
}

@incollection{Neal1996,
  title = {Priors for Infinite Networks},
  booktitle = {Bayesian Learning for Neural Networks},
  author = {Neal, Radford M},
  year = {1996},
  pages = {29--53},
  publisher = {Springer},
  file = {/home/marnix/Zotero/storage/C3QK8JP9/Neal - 1996 - Priors for infinite networks.pdf}
}

@article{Neal2003,
  title = {Slice Sampling},
  author = {Neal, Radford M.},
  year = {2003},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {31},
  number = {3},
  pages = {705--767},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1056562461},
  urldate = {2021-04-15},
  abstract = {Markov chain sampling methods that adapt to characteristics of the distribution being sampled can be constructed using the principle that one can ample from a distribution by sampling uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal "slice" defined by the current vertical position, or more generally, with some update that leaves the uniform distribution over this slice invariant. Such "slice sampling" methods are easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling and more efficient than simple Metropolis updates, due to the ability of slice sampling to adaptively choose the magnitude of changes made. It is therefore attractive for routine and automated use. Slice sampling methods that update all variables simultaneously are also possible. These methods can adaptively choose the magnitudes of changes made to each variable, based on the local properties of the density function. More ambitiously, such methods could potentially adapt to the dependencies between variables by constructing local quadratic approximations. Another approach is to improve sampling efficiency by suppressing random walks. This can be done for univariate slice sampling by "overrelaxation," and for multivariate slice sampling by "reflection" from the edges of the slice.},
  keywords = {65C05,65C60,Adaptive methods,auxiliary variables,dynamical methods,Gibbs sampling,Markov chain Monte Carlo,Metropolis algorithm,overrelaxation},
  file = {/home/marnix/Zotero/storage/29EXBYGQ/Neal - 2003 - Slice sampling.pdf;/home/marnix/Zotero/storage/IGJ2U37Q/1056562461.html}
}

@article{Newman2003,
  title = {Finding and Evaluating Community Structure in Networks},
  author = {Newman, M. E. J. and Girvan, M.},
  year = {2003},
  eprintclass = {cond-mat},
  pages = {1--16},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.69.026113},
  abstract = {We propose and study a set of algorithms for discovering community structure in networks -- natural divisions of network nodes into densely connected subgroups. Our algorithms all share two definitive features: first, they involve iterative removal of edges from the network to split it into communities, the edges removed being identified using one of a number of possible "betweenness" measures, and second, these measures are, crucially, recalculated after each removal. We also propose a measure for the strength of the community structure found by our algorithms, which gives us an objective metric for choosing the number of communities into which a network should be divided. We demonstrate that our algorithms are highly effective at discovering community structure in both computer-generated and real-world network data, and show how they can be used to shed light on the sometimes dauntingly complex structure of networked systems.},
  arxiv = {0308217},
  arxivid = {cond-mat/0308217},
  isbn = {1539-3755\r1550-2376},
  file = {/home/marnix/Zotero/storage/5BZW7PH4/Newman2003 Finding and evaluating community structure in networks.pdf}
}

@article{Newman2003a,
  title = {The Structure and Function of Complex Networks},
  author = {Newman, M. E. J.},
  year = {2003},
  month = jan,
  journal = {SIAM Review},
  volume = {45},
  eprintclass = {cond-mat},
  pages = {167--256},
  issn = {0036-1445},
  doi = {10.1137/S003614450342480},
  abstract = {Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.},
  arxiv = {0303516},
  arxivid = {cond-mat/0303516},
  isbn = {00361445},
  pmid = {24174898},
  keywords = {complex systems,computer networks,graph theory,networks,percolation theory,random graphs,social networks},
  file = {/home/marnix/Zotero/storage/9EY37BDI/Newman - 2003 - The Structure and Function of Complex Networks.pdf}
}

@article{Newman2003b,
  title = {Mixing Patterns in Networks},
  author = {Newman, M. E. J.},
  year = {2003},
  month = feb,
  journal = {Physical Review E},
  volume = {67},
  number = {2},
  eprintclass = {cond-mat},
  pages = {026126+},
  publisher = {American Physical Society},
  issn = {1063-651X},
  doi = {10.1103/physreve.67.026126},
  abstract = {We study assortative mixing in networks, the tendency for vertices in networks to be connected to other vertices that are like (or unlike) them in some way. We consider mixing according to discrete characteristics such as language or race in social networks and scalar characteristics such as age. As a special example of the latter we consider mixing according to vertex degree, i.e., according to the number of connections vertices have to other vertices: do gregarious people tend to associate with other gregarious people? We propose a number of measures of assortative mixing appropriate to the various mixing types, and apply them to a variety of real-world networks, showing that assortative mixing is a pervasive phenomenon found in many networks. We also propose several models of assortatively mixed networks, both analytic ones based on generating function methods, and numerical ones based on Monte Carlo graph generation techniques. We use these models to probe the properties of networks as their level of assortativity is varied. In the particular case of mixing by degree, we find strong variation with assortativity in the connectivity of the network and in the resilience of the network to the removal of vertices.},
  arxiv = {0209450},
  arxivid = {cond-mat/0209450},
  keywords = {networks},
  file = {/home/marnix/Zotero/storage/DGNTHMFS/Newman - 2003 - Mixing patterns in networks.pdf}
}

@article{Newman2003c,
  title = {Why Social Networks Are Different from Other Types of Networks},
  author = {Newman, M. E. J. and Park, Juyong},
  year = {2003},
  month = sep,
  journal = {Physical Review E},
  volume = {68},
  number = {3},
  pages = {036122},
  issn = {1063-651X, 1095-3787},
  doi = {10.1103/PhysRevE.68.036122},
  urldate = {2021-12-09},
  langid = {english},
  file = {/home/marnix/Zotero/storage/A8L3MMFL/Newman and Park - 2003 - Why social networks are different from other types.pdf}
}

@article{Newman2005,
  title = {Power Laws, {{Pareto}} Distributions and {{Zipf}}'s Law},
  author = {Newman, M. E. J.},
  year = {2005},
  month = sep,
  journal = {Contemporary Physics},
  volume = {46},
  number = {5},
  eprintclass = {cond-mat},
  pages = {323--351},
  publisher = {Taylor & Francis},
  issn = {0010-7514},
  doi = {10.1080/00107510500052444},
  abstract = {When the probability of measuring a particular value of some quantity varies inversely as a power of that value, the quantity is said to follow a power law, also known variously as Zipf's law or the Pareto distribution. Power laws appear widely in physics, biology, earth and planetary sciences, economics and finance, computer science, demography and the social sciences. For instance, the distributions of the sizes of cities, earthquakes, forest fires, solar flares, moon craters and people's personal fortunes all appear to follow power laws. The origin of power-law behaviour has been a topic of debate in the scientific community for more than a century. Here we review some of the empirical evidence for the existence of power-law forms and the theories proposed to explain them.},
  arxiv = {0412004},
  arxivid = {cond-mat/0412004},
  keywords = {distributions,law,pareto,power},
  file = {/home/marnix/Zotero/storage/R7E2CIFT/Newman - 2005 - Power laws, Pareto distributions and Zipf's law.pdf}
}

@article{Newman2006,
  title = {Modularity and Community Structure in Networks},
  author = {Newman, M. E. J.},
  year = {2006},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {103},
  number = {23},
  pages = {8577--8582},
  issn = {0027-8424},
  doi = {10.1073/pnas.0601602103},
  abstract = {Many networks of interest in the sciences, including social networks, computer networks, and metabolic and regulatory networks, are found to divide naturally into communities or modules. The problem of detecting and characterizing this community structure is one of the outstanding issues in the study of networked systems. One highly effective approach is the optimization of the quality function known as '' modularity'' over the possible divisions of a network. Here I show that the modularity can be expressed in terms of the eigenvectors of a characteristic matrix for the network, which I call the modularity matrix, and that this expression leads to a spectral algorithm for community detection that returns results of demonstrably higher quality than competing methods in shorter running times. I illustrate the method with applications to several published network data sets.},
  pmid = {16723398},
  keywords = {community,graph,modularity,networks},
  file = {/home/marnix/Zotero/storage/QULCL6ZC/Newman - 2006 - Modularity and community structure in networks.pdf}
}

@article{Newman2011,
  title = {Complex {{Systems}}: {{A Survey}}},
  author = {Newman, M. E. J.},
  year = {2011},
  number = {I},
  eprint = {1112.1440},
  issn = {00029505},
  doi = {10.1119/1.3590372},
  abstract = {A complex system is a system composed of many interacting parts, often called agents, which displays collective behavior that does not follow trivially from the behaviors of the individual parts. Examples include condensed matter systems, ecosystems, stock markets and economies, biological evolution, and indeed the whole of human society. Substantial progress has been made in the quantitative understanding of complex systems, particularly since the 1980s, using a combination of basic theory, much of it derived from physics, and computer simulation. The subject is a broad one, drawing on techniques and ideas from a wide range of areas. Here I give a survey of the main themes and methods of complex systems science and an annotated bibliography of resources, ranging from classic papers to recent books and reviews.},
  archiveprefix = {arXiv},
  arxivid = {1112.1440},
  isbn = {0780340531},
  file = {/home/marnix/Zotero/storage/Z23SHVZX/Newman - 2011 - Complex Systems A Survey.pdf}
}

@article{Newman2015,
  title = {Generalized {{Communities}} in {{Networks}}},
  author = {Newman, M. E. J. and Peixoto, Tiago P.},
  year = {2015},
  journal = {Physical Review Letters},
  volume = {115},
  number = {8},
  eprint = {1505.07478},
  pages = {1--5},
  issn = {10797114},
  doi = {10.1103/PhysRevLett.115.088701},
  abstract = {A substantial volume of research has been devoted to studies of community structure in networks, but communities are not the only possible form of large-scale network structure. Here we describe a broad extension of community structure that encompasses traditional communities but includes a wide range of generalized structural patterns as well. We describe a principled method for detecting this generalized structure in empirical network data and demonstrate with real-world examples how it can be used to learn new things about the shape and meaning of networks.},
  archiveprefix = {arXiv},
  arxivid = {1505.07478},
  isbn = {0031-9007\r1079-7114},
  pmid = {26340218},
  file = {/home/marnix/Zotero/storage/KFEJ55TM/Newman, Peixoto - 2015 - Generalized Communities in Networks.pdf}
}

@article{Ng2008,
  title = {Could Formant Frequencies of Snore Signals Be an Alternative Means for the Diagnosis of Obstructive Sleep Apnea?},
  author = {Ng, Andrew Keong and Koh, Tong San and Baey, Eugene and Lee, Teck Hock and Abeyratne, Udantha Ranjith and Puvanendran, Kathiravelu},
  year = {2008},
  journal = {Sleep Medicine},
  volume = {9},
  number = {8},
  pages = {894--898},
  issn = {1389-9457},
  doi = {10.1016/j.sleep.2007.07.010},
  abstract = {Objective To study the feasibility of using acoustic signatures in snore signals for the diagnosis of obstructive sleep apnea (OSA). Methods Snoring sounds of 30 apneic snorers (24 males; 6 females; apnea--hypopnea index, AHI=46.9{\textpm}25.7events/h) and 10 benign snorers (6 males; 4 females; AHI=4.6{\textpm}3.4events/h) were captured in a sleep laboratory. The recorded snore signals were preprocessed to remove noise, and subsequently, modeled using a linear predictive coding (LPC) technique. Formant frequencies (F1, F2, and F3) were extracted from the LPC spectrum for analysis. The accuracy of this approach was assessed using receiver operating characteristic curves and notched box plots. The relationship between AHI and F1 was further explored via regression analysis. Results Quantitative differences in formant frequencies between apneic and benign snores are found in same- or both-gender snorers. Apneic snores exhibit higher formant frequencies than benign snores, especially F1, which can be related to the pathology of OSA. This study yields a sensitivity of 88\%, a specificity of 82\%, and a threshold value of F1=470Hz that best differentiate apneic snorers from benign snorers (both gender combined). Conclusion Acoustic signatures in snore signals carry information for OSA diagnosis, and snore-based analysis might potentially be a non-invasive and inexpensive diagnostic approach for mass screening of OSA.},
  keywords = {Acoustic analysis,Coding,Formant frequencies,Linear predictive coding,Obstructive sleep apnea,Polysomnography,Snore signals,Snoring}
}

@article{Nielsen2012,
  title = {An Approximate {{Bayesian}} Fundamental Frequency Estimator},
  author = {Nielsen, Jesper Kj{\ae}r and Christensen, Mads Gr{\ae}sb{\o}ll and Jensen, S{\o}ren Holdt},
  year = {2012},
  journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
  pages = {4617--4620},
  issn = {15206149},
  doi = {10.1109/ICASSP.2012.6288947},
  abstract = {Joint fundamental frequency and model order estimation is an important problem in several applications such as speech and music processing. In this paper, we develop an approximate estimation algorithm of these quantities using Bayesian inference. The inference about the fundamental frequency and the model order is based on a probability model which corresponds to a minimum of prior information. From this probability model, we give the exact posterior distributions on the fundamental frequency and the model order, and we also present analytical approximations of these distributions which lower the computational load of the algorithm. By use of simulations on both a synthetic signal and a speech signal, the algorithm is demonstrated to be more accurate than a state-of-the-art maximum likelihood-based method.},
  isbn = {9781467300469},
  keywords = {Bayesian inference and model comparison,Fundamental frequency,Zellner's g-prior},
  file = {/home/marnix/Zotero/storage/R4P7VB2M/Nielsen2012 An Approximate Bayesian Fundamental Frequency Estimator.pdf}
}

@article{Nielsen2013,
  title = {Default Bayesian Estimation of the Fundamental Frequency},
  author = {Nielsen, Jesper Kj{\ae}r and Christensen, Mads Gr{\ae}sboll and Jensen, Soren Holdt},
  year = {2013},
  journal = {IEEE Transactions on Audio, Speech and Language Processing},
  volume = {21},
  number = {3},
  pages = {598--610},
  issn = {15587916},
  doi = {10.1109/TASL.2012.2229979},
  abstract = {Joint fundamental frequency and model order estimation is an important problem in several applications. In this paper, a default estimation algorithm based on a minimum of prior information is presented. The algorithm is developed in a Bayesian framework, and it can be applied to both real- and complex-valued discrete-time signals which may have missing samples or may have been sampled at a non-uniform sampling frequency. The observation model and prior distributions corresponding to the prior information are derived in a consistent fashion using maximum entropy and invariance arguments. Moreover, several approximations of the posterior distributions on the fundamental frequency and the model order are derived, and one of the state-of-the-art joint fundamental frequency and model order estimators is demonstrated to be a special case of one of these approximations. The performance of the approximations are evaluated in a small-scale simulation study on both synthetic and real world signals. The simulations indicate that the proposed algorithm yields more accurate results than previous algorithms. The simulation code is available online.},
  keywords = {Bayesian model comparison,fundamental frequency estimation,Zellner's g-prior},
  file = {/home/marnix/Zotero/storage/BCNUJZ64/Nielsen2013 Default Bayesian Estimation of the Fundamental Frequency.pdf}
}

@article{Nier2007,
  title = {Network Models and Financial Stability},
  author = {Nier, Erlend and Yang, Jing and Yorulmazer, Tanju and Alentorn, Amadeo},
  year = {2007},
  journal = {Journal of Economic Dynamics and Control},
  volume = {31},
  number = {6},
  pages = {2033--2060}
}

@misc{Nishihara2014,
  title = {Parallel {{MCMC}} with {{Generalized Elliptical Slice Sampling}}},
  author = {Nishihara, Robert and Murray, Iain and Adams, Ryan P.},
  year = {2014},
  month = jul,
  number = {arXiv:1210.7477},
  eprint = {1210.7477},
  primaryclass = {stat},
  publisher = {arXiv},
  urldate = {2023-03-02},
  abstract = {Probabilistic models are conceptually powerful tools for finding structure in data, but their practical effectiveness is often limited by our ability to perform inference in them. Exact inference is frequently intractable, so approximate inference is often performed using Markov chain Monte Carlo (MCMC). To achieve the best possible results from MCMC, we want to efficiently simulate many steps of a rapidly mixing Markov chain which leaves the target distribution invariant. Of particular interest in this regard is how to take advantage of multi-core computing to speed up MCMC-based inference, both to improve mixing and to distribute the computational load. In this paper, we present a parallelizable Markov chain Monte Carlo algorithm for efficiently sampling from continuous probability distributions that can take advantage of hundreds of cores. This method shares information between parallel Markov chains to build a scale-mixture of Gaussians approximation to the density function of the target distribution. We combine this approximation with a recent method known as elliptical slice sampling to create a Markov chain with no step-size parameters that can mix rapidly without requiring gradient or curvature computations.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/4E7FTN3Y/Nishihara et al. - 2014 - Parallel MCMC with Generalized Elliptical Slice Sa.pdf;/home/marnix/Zotero/storage/H5WFUNFN/1210.html}
}

@article{Nix1999,
  title = {Maximum-Likelihood Continuity Mapping ({{MALCOM}}): {{An}} Alternative to {{HMMs}}},
  author = {Nix, D A and Hogden, J E},
  year = {1999},
  journal = {In Kearns},
  pages = {744--750},
  abstract = {(MALCOM), an alternative to hidden Markov models (HMMs) for processing{\textbackslash}nsequence data such as speech. While HMMs have a discrete ``hidden''{\textbackslash}nspace constrained by a fixed finite-automaton architecture, MALCOM{\textbackslash}nhas a continuous hidden space---a \{{\textbackslash}it continuity map\}---that is{\textbackslash}nconstrained only by a smoothness requirement on paths through the{\textbackslash}nspace. MALCOM fits into the same probabilistic framework for speech{\textbackslash}nrecognition as HMMs, but it represents a more realistic model of{\textbackslash}nthe speech production process. To evaluate the extent to which MALCOM{\textbackslash}ncaptures speech production information, we generated continuous speech{\textbackslash}ncontinuity maps for three speakers and used the paths through them{\textbackslash}nto predict measured speech articulator data. The median correlation{\textbackslash}nbetween the MALCOM paths \{{\textbackslash}it obtained from only the speech acoustics\}{\textbackslash}nand articulator measurements was 0.77 on an independent test set{\textbackslash}nnot used to train MALCOM or the predictor. This unsupervised model{\textbackslash}nachieved correlations over speakers and articulators only 0.02 to{\textbackslash}n0.15 lower than those obtained using an analogous supervised method{\textbackslash}nwhich \{{\textbackslash}it used articulatory measurements as well as acoustics.\}}
}

@inproceedings{Nolan2001,
  title = {Speaker Identification Evidence: Its Forms, Limitations, and Roles},
  booktitle = {Proceedings of the {{Conference}} ``{{Law}} and {{Language}}: {{Prospect}} and {{Retrospect}}'', {{Levi}} ({{Finnish Lapland}})},
  author = {Nolan, Francis},
  year = {2001},
  pages = {1--19},
  file = {/home/marnix/Zotero/storage/Y54ND3KG/Nolan - 2001 - Speaker identification evidence its forms, limita.pdf}
}

@article{Nummenmaa2014,
  title = {Bodily Maps of Emotions},
  author = {Nummenmaa, L. and Glerean, E. and Hari, R. and Hietanen, J. K.},
  year = {2014},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {111},
  number = {2},
  pages = {646--651},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1321664111},
  urldate = {2021-10-09},
  langid = {english},
  file = {/home/marnix/Zotero/storage/BHPWT8SM/Nummenmaa et al. - 2014 - Bodily maps of emotions.pdf}
}

@article{Ochiai2014,
  title = {Three-{{Dimensional Mid-Air Acoustic Manipulation}} by {{Ultrasonic Phased Arrays}}},
  author = {Ochiai, Yoichi and Hoshi, Takayuki and Rekimoto, Jun},
  year = {2014},
  month = may,
  journal = {PLOS ONE},
  volume = {9},
  number = {5},
  pages = {e97590},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0097590},
  urldate = {2023-09-11},
  abstract = {The essence of levitation technology is the countervailing of gravity. It is known that an ultrasound standing wave is capable of suspending small particles at its sound pressure nodes. The acoustic axis of the ultrasound beam in conventional studies was parallel to the gravitational force, and the levitated objects were manipulated along the fixed axis (i.e. one-dimensionally) by controlling the phases or frequencies of bolted Langevin-type transducers. In the present study, we considered extended acoustic manipulation whereby millimetre-sized particles were levitated and moved three-dimensionally by localised ultrasonic standing waves, which were generated by ultrasonic phased arrays. Our manipulation system has two original features. One is the direction of the ultrasound beam, which is arbitrary because the force acting toward its centre is also utilised. The other is the manipulation principle by which a localised standing wave is generated at an arbitrary position and moved three-dimensionally by opposed and ultrasonic phased arrays. We experimentally confirmed that expanded-polystyrene particles of 0.6 mm, 1 mm, and 2 mm in diameter could be manipulated by our proposed method.},
  langid = {english},
  keywords = {Acoustic signals,Acoustics,Gravitation,Motion,Polystyrene,Potential energy,Sound pressure,Specific gravity},
  file = {/home/marnix/Zotero/storage/ARBS8X4M/Ochiai et al. - 2014 - Three-Dimensional Mid-Air Acoustic Manipulation by.pdf}
}

@article{OCinneide2012,
  title = {Phase-{{Distortion-Robust Voice-Source Analysis}}},
  author = {O Cinneide, Alan},
  year = {2012},
  publisher = {Dublin Institute of Technology},
  doi = {10.21427/D7FW3B},
  urldate = {2025-10-09},
  copyright = {Creative Commons Attribution- Noncommercial-Share Alike 3.0 License},
  langid = {english},
  file = {/home/marnix/Zotero/storage/SP7PR35F/O Cinneide - 2012 - Phase-Distortion-Robust Voice-Source Analysis.pdf}
}

@article{OHagan1987,
  title = {Monte {{Carlo}} Is {{Fundamentally Unsound}}},
  author = {O'Hagan, A.},
  year = {1987},
  journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
  volume = {36},
  number = {2/3},
  eprint = {2348519},
  eprinttype = {jstor},
  pages = {247--249},
  issn = {0039-0526},
  doi = {10.2307/2348519},
  urldate = {2019-10-30},
  abstract = {We present some fundamental objections to the Monte Carlo method of numerical integration.},
  file = {/home/marnix/Zotero/storage/7S2VMQBC/O'Hagan - 1987 - Monte Carlo is Fundamentally Unsound.pdf}
}

@article{Ohman2001,
  title = {Why Current Speech Technology Is False Phonetics},
  author = {{\"O}hman, Sven},
  year = {2001},
  journal = {Working Papers in Linguistics},
  volume = {49},
  pages = {180--183},
  publisher = {Citeseer},
  file = {/home/marnix/Zotero/storage/5TZBJPWU/Öhman - 2001 - Why current speech technology is false phonetics.pdf}
}

@article{Olhede2013,
  title = {Network Histograms and Universality of Blockmodel Approximation},
  author = {Olhede, Sofia C. and Wolfe, Patrick J.},
  year = {2013},
  eprint = {1312.5306},
  pages = {1--27},
  issn = {0027-8424},
  doi = {10.1073/pnas.1400374111},
  abstract = {In this article we introduce the network histogram: a statistical summary of network interactions, to be used as a tool for exploratory data analysis. A network histogram is obtained by fitting a stochastic blockmodel to a single observation of a network dataset. Blocks of edges play the role of histogram bins, and community sizes that of histogram bandwidths or bin sizes. Just as standard histograms allow for varying bandwidths, different blockmodel estimates can all be considered valid representations of an underlying probability model, subject to bandwidth constraints. Here we provide methods for automatic bandwidth selection, by which the network histogram approximates the generating mechanism that gives rise to exchangeable random graphs. This makes the blockmodel a universal network representation for unlabeled graphs. With this insight, we discuss the interpretation of network communities in light of the fact that many different community assignments can all give an equally valid representation of such a network. To demonstrate the fidelity-versus-interpretability tradeoff inherent in considering different numbers and sizes of communities, we analyze two publicly available networks - political weblogs and student friendships - and discuss how to interpret the network histogram when additional information related to node and edge labeling is present.},
  archiveprefix = {arXiv},
  arxivid = {1312.5306},
  isbn = {1400374111},
  pmid = {25275010},
  keywords = {community detection,exchangeable random graphs,graphons,nonparametric statistics,statistical network analysis,stochastic block-},
  file = {/home/marnix/Zotero/storage/6QPBU7XD/Olhede2014 Network histograms and universality of blockmodel approximation.pdf}
}

@article{Ondel2016,
  title = {Variational {{Inference}} for {{Acoustic Unit Discovery}}},
  author = {Ondel, Lucas and Burget, Luka{\v s} and {\v C}ernock{\'y}, Jan},
  year = {2016},
  month = jan,
  journal = {Procedia Computer Science},
  series = {{{SLTU-2016}} 5th {{Workshop}} on {{Spoken Language Technologies}} for {{Under-resourced}} Languages 09-12 {{May}} 2016 {{Yogyakarta}}, {{Indonesia}}},
  volume = {81},
  pages = {80--86},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2016.04.033},
  urldate = {2020-06-09},
  abstract = {Recently, several nonparametric Bayesian models have been proposed to automatically discover acoustic units in unlabeled data. Most of them are trained using various versions of the Gibbs Sampling (GS) method. In this work, we consider Variational Bayes (VB) as alternative inference process. Even though VB yields an approximate solution of the posterior distribution it can be easily parallelized which makes it more suitable for large database. Results show that, notwithstanding VB inference is an order of magnitude faster, it outperforms GS in terms of accuracy.},
  langid = {english},
  keywords = {acoustic unit discovery,Bayesian non-parametric,Variational Bayes},
  file = {/home/marnix/Zotero/storage/6KQFXUKJ/Ondel et al. - 2016 - Variational Inference for Acoustic Unit Discovery.pdf;/home/marnix/Zotero/storage/923C4EXE/S1877050916300473.html}
}

@inproceedings{Ondel2018,
  title = {Bayesian {{Models}} for {{Unit Discovery}} on a {{Very Low Resource Language}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ondel, Lucas and Godard, Pierre and Besacier, Laurent and Larsen, Elin and {Hasegawa-Johnson}, Mark and Scharenborg, Odette and Dupoux, Emmanuel and Burget, Lukas and Yvon, Fran{\c c}ois and Khudanpur, Sanjeev},
  year = {2018},
  month = apr,
  pages = {5939--5943},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2018.8461545},
  abstract = {Developing speech technologies for low-resource languages has become a very active research field over the last decade. Among others, Bayesian models have shown some promising results on artificial examples but still lack of in situ experiments. Our work applies state-of-the-art Bayesian models to unsupervised Acoustic Unit Discovery (AUD) in a real low-resource language scenario. We also show that Bayesian models can naturally integrate information from other resourceful languages by means of informative prior leading to more consistent discovered units. Finally, discovered acoustic units are used, either as the I-best sequence or as a lattice, to perform word segmentation. Word segmentation results show that this Bayesian approach clearly outperforms a Segmental-DTW baseline on the same corpus.},
  keywords = {acoustic signal processing,Acoustic Unit Discovery,active research field,AUD,Bayes methods,Bayesian approach,Bayesian Model,Bayesian Models,Data models,discovered acoustic units,Hidden Markov models,Informative Prior,Lattices,Low-Resource ASR,low-resource language scenario,low-resource languages,Measurement,Mel frequency cepstral coefficient,natural language processing,resourceful languages,Segmental-DTW baseline,speech processing,speech technologies,unsupervised acoustic unit discovery,Very Low Resource Language,word segmentation},
  file = {/home/marnix/Zotero/storage/AHNUTIL8/Ondel et al. - 2018 - Bayesian Models for Unit Discovery on a Very Low R.pdf;/home/marnix/Zotero/storage/VK42BQPP/8461545.html}
}

@article{Ondel2019,
  title = {Bayesian {{Subspace Hidden Markov Model}} for {{Acoustic Unit Discovery}}},
  author = {Ondel, Lucas and Vydana, Hari Krishna and Burget, Luk{\'a}{\v s} and {\v C}ernock{\'y}, Jan},
  year = {2019},
  month = jul,
  journal = {arXiv:1904.03876 [cs, eess, stat]},
  eprint = {1904.03876},
  primaryclass = {cs, eess, stat},
  urldate = {2020-09-15},
  abstract = {This work tackles the problem of learning a set of language specific acoustic units from unlabeled speech recordings given a set of labeled recordings from other languages. Our approach may be described by the following two steps procedure: first the model learns the notion of acoustic units from the labelled data and then the model uses its knowledge to find new acoustic units on the target language. We implement this process with the Bayesian Subspace Hidden Markov Model (SHMM), a model akin to the Subspace Gaussian Mixture Model (SGMM) where each low dimensional embedding represents an acoustic unit rather than just a HMM's state. The subspace is trained on 3 languages from the GlobalPhone corpus (German, Polish and Spanish) and the AUs are discovered on the TIMIT corpus. Results, measured in equivalent Phone Error Rate, show that this approach significantly outperforms previous HMM based acoustic units discovery systems and compares favorably with the Variational Auto Encoder-HMM.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/YXP5RZGB/Ondel et al. - 2019 - Bayesian Subspace Hidden Markov Model for Acoustic.pdf;/home/marnix/Zotero/storage/D5EDP2LH/1904.html}
}

@article{Oord2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  month = sep,
  journal = {arXiv:1609.03499 [cs]},
  eprint = {1609.03499},
  primaryclass = {cs},
  urldate = {2020-06-03},
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-ofthe-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound},
  file = {/home/marnix/Zotero/storage/YHE64UJM/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf}
}

@inproceedings{Oppermann2013,
  title = {Reconstructing Signals from Noisy Data with Unknown Signal and Noise Covariance},
  booktitle = {{{BAYESIAN INFERENCE AND MAXIMUM ENTROPY METHODS IN SCIENCE AND ENGINEERING}}: 32nd {{International Workshop}} on {{Bayesian Inference}} and {{Maximum Entropy Methods}} in {{Science}} and {{Engineering}}},
  author = {Oppermann, N. and Robbers, G. and En{\ss}lin, T. A.},
  year = {2013},
  pages = {122--129},
  address = {Garching, Germany},
  doi = {10.1063/1.4819991},
  urldate = {2019-04-23},
  file = {/home/marnix/Zotero/storage/5L9JMFKE/Oppermann et al. - 2013 - Reconstructing signals from noisy data with unknow.pdf}
}

@article{Orbanz2010,
  title = {Bayesian Nonparametric Models},
  author = {Orbanz, Peter and Teh, Yee Whye},
  year = {2010},
  journal = {Encyclopedia of machine learning},
  volume = {1},
  file = {/home/marnix/Zotero/storage/L6BVCVMM/Orbanz and Teh - 2010 - Bayesian nonparametric models..pdf}
}

@book{ORuanaidh1996,
  title = {Numerical {{Bayesian Methods Applied}} to {{Signal Processing}}},
  author = {{\'O} Ruanaidh, Joseph J. K. and Fitzgerald, William J.},
  editor = {Chambers, J. and Eddy, W. and H{\"a}rdle, W. and Sheather, S. and Tierney, L.},
  year = {1996},
  series = {Statistics and {{Computing}}},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4612-0717-7},
  urldate = {2019-04-24},
  isbn = {978-1-4612-6880-2 978-1-4612-0717-7},
  file = {/home/marnix/Zotero/storage/YA39TLVC/Ó Ruanaidh and Fitzgerald - 1996 - Numerical Bayesian Methods Applied to Signal Proce.pdf}
}

@article{OShaughnessy2008,
  title = {Invited Paper: {{Automatic}} Speech Recognition: {{History}}, Methods and Challenges},
  author = {O'Shaughnessy, Douglas},
  year = {2008},
  journal = {Pattern Recognition},
  volume = {41},
  number = {10},
  pages = {2965--2979},
  issn = {00313203},
  doi = {10.1016/j.patcog.2008.05.008},
  abstract = {The field of automatic speech recognition (ASR) is discussed from the viewpoint of pattern recognition (PR). This tutorial examines the problem area, its methods, successes and failures, focusing on the nature of the speech signal and techniques to accomplish useful data reduction. Comparison is made with other areas of PR. Suggestions are given for areas of future progress. {\copyright} 2008 Elsevier Ltd. All rights reserved.},
  isbn = {0031-3203},
  keywords = {Adaptation,Automatic speech recognition,Compensation,Hidden Markov models,Pattern recognition,Spectral representation}
}

@inproceedings{OstvoldEk2024,
  title = {Speculative Design through the Lens of {{AI}}},
  booktitle = {{{DS}} 131: {{Proceedings}} of the {{International Conference}} on {{Engineering}} and {{Product Design Education}} ({{E}}\&{{PDE}} 2024)},
  author = {{\O}stvold Ek, Mathias and Paulsen, Magnus and Trondsen, June Kyong},
  year = {2024},
  pages = {627--632},
  issn = {3005-4753},
  doi = {10.35199/EPDE.2024.106},
  urldate = {2025-03-05},
  abstract = {This review paper explores the evolving relationship between artificial intelligence (AI) and speculative design. Through a literature review analyzing ten case studies, this paper highlights the nuances of using AI-driven tools in envisioning speculative futures. The paper investigates how AI influences speculative design approaches to identify the benefits, limitations, and potential challenges when combining the two. The results are presented according to the three most prominent themes identified across the ten articles: using AI for image generation, perceived accessibility of AI, and using AI to envision future scenarios. While using AI to automate parts of the design process is perceived as a significant benefit, some also regard the results produced by AI as uninspiring and needing more contextual factors. This is addressed in the discussion section, focusing on the challenges of AI-generated images. Lastly, this paper discusses the ambiguous terminology used about speculative design and what implications this could have for future research.},
  isbn = {978-1-912254-20-0},
  langid = {english},
  file = {/home/marnix/Zotero/storage/46JX9N9B/Østvold Ek et al. - 2024 - SPECULATIVE DESIGN THROUGH THE LENS OF AI.pdf}
}

@article{Ouni2014,
  title = {Multimodal {{Speech}} : From Articulatory Speech to Audiovisual Speech},
  author = {Ouni, Slim},
  year = {2014},
  file = {/home/marnix/Zotero/storage/3G9A9SDZ/Ouni2014 multimodal speech.pdf}
}

@incollection{Owren1998,
  title = {Applying {{Linear Predictive Coding}} ({{LPC}}) to {{Frequency-spectrum Analysis}} of {{Animal Acoustic Signals}}},
  booktitle = {Animal {{Acoustic Communication}}},
  author = {Owren, M. J. and Bernacki, R. H.},
  editor = {Hopp, Steven L. and Owren, Michael J. and Evans, Christopher S.},
  year = {1998},
  pages = {129--162},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-76220-8_5},
  urldate = {2020-02-05},
  isbn = {978-3-642-76222-2 978-3-642-76220-8},
  langid = {english},
  file = {/home/marnix/Zotero/storage/ZUWQTTQU/Owren and Bernacki - 1998 - Applying Linear Predictive Coding (LPC) to Frequen.pdf}
}

@techreport{Page1998,
  title = {The {{PageRank Citation Ranking}}: {{Bringing Order}} to the {{Web}}},
  author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
  year = {1998},
  institution = {Stanford Digital Library Technologies Project},
  abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes \{PageRank\}, a method for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare \{PageRank\} to an idealized random Web surfer. We show how to efficiently compute \{PageRank\} for large...},
  keywords = {google,pagerank}
}

@article{Paisley2012,
  title = {Variational {{Bayesian Inference}} with {{Stochastic Search}}},
  author = {Paisley, John and Blei, David M and Jordan, Michael I},
  year = {2012},
  abstract = {Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/SEQC4FVE/Paisley et al. - Variational Bayesian Inference with Stochastic Search.pdf}
}

@inproceedings{Palaz2015,
  title = {Convolutional {{Neural Networks-based}} Continuous Speech Recognition Using Raw Speech Signal},
  booktitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Palaz, D. and {Magimai.-Doss}, M. and Collobert, R.},
  year = {2015},
  month = apr,
  pages = {4295--4299},
  doi = {10.1109/ICASSP.2015.7178781},
  abstract = {State-of-the-art automatic speech recognition systems model the relationship between acoustic speech signal and phone classes in two stages, namely, extraction of spectral-based features based on prior knowledge followed by training of acoustic model, typically an artificial neural network (ANN). In our recent work, it was shown that Convolutional Neural Networks (CNNs) can model phone classes from raw acoustic speech signal, reaching performance on par with other existing feature-based approaches. This paper extends the CNN-based approach to large vocabulary speech recognition task. More precisely, we compare the CNN-based approach against the conventional ANN-based approach on Wall Street Journal corpus. Our studies show that the CNN-based approach achieves better performance than the conventional ANN-based approach with as many parameters. We also show that the features learned from raw speech by the CNN-based approach could generalize across different databases.},
  keywords = {acoustic model training,acoustic signal processing,Acoustics,ANN approach,artificial neural network,automatic speech recognition,CNN approach,continuous speech recognition,Convolution,convolutional neural network,convolutional neural networks,feature extraction,Feature extraction,feature learning,Hidden Markov models,large vocabulary speech recognition task,learning (artificial intelligence),neural nets,Neural networks,phone classes,prior knowledge,raw acoustic speech signal,raw signal,spectral-based feature extraction,Speech,speech recognition,Speech recognition,state-of-the-art automatic speech recognition system model,Wall Street Journal corpus},
  file = {/home/marnix/Zotero/storage/NI8RHIKP/Palaz et al. - 2015 - Convolutional Neural Networks-based continuous spe.pdf;/home/marnix/Zotero/storage/IRV5UGJ9/7178781.html}
}

@article{Paliwal1981,
  title = {On the Performance of {{Burg}}'s Method of Maximum Entropy Spectral Analysis When Applied to Voiced Speech},
  author = {Paliwal, K. K. and Rao, R. V. S.},
  year = {1981},
  volume = {4},
  pages = {59--63},
  keywords = {autocorrelation method,burg,covariance method,formants,linear prediction,maximum entropy spectral analysis,s method,voiced speech},
  file = {/home/marnix/Zotero/storage/DGY448KC/Paliwal1981 On the performance of Burg's method of maximum entropy spectral analysis when applied to voiced speech.pdf}
}

@article{Pandey2014,
  title = {To Go Deep or Wide in Learning?},
  author = {Pandey, Gaurav and Dukkipati, Ambedkar},
  year = {2014},
  abstract = {To achieve acceptable performance for AI tasks, one can either use sophisticated feature extraction methods as the first layer in a twolayered supervised learning model, or learn the features directly using a deep (multilayered) model. While the first approach is very problem-specific, the second approach has computational overheads in learning multiple layers and fine-tuning of the model. In this paper, we propose an approach called wide learning based on arc-cosine kernels, that learns a single layer of infinite width. We propose exact and inexact learning strategies for wide learning and show that wide learning with single layer outperforms single layer as well as deep architectures of finite width for some benchmark datasets.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/7D3B2EYU/Pandey and Dukkipati - To go deep or wide in learning.pdf}
}

@article{Pantazis2009,
  title = {Chirp Rate Estimation of Speech Based on a Time-Varying Quasi-Harmonic Model},
  author = {Pantazis, Yannis and Rosec, Olivier and Stylianou, Yannis},
  year = {2009},
  pages = {3985--3988},
  isbn = {9781424423545},
  file = {/home/marnix/Zotero/storage/UEABJ2GR/Pantazis2009 Chirp rate estimation of speech.pdf}
}

@article{Park2004,
  title = {The Statistical Mechanics of Networks},
  author = {Park, Juyong and Newman, M. E. J.},
  year = {2004},
  month = dec,
  journal = {Physical Review E},
  volume = {70},
  number = {6},
  eprint = {cond-mat/0405566},
  pages = {066117},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.70.066117},
  urldate = {2022-05-25},
  abstract = {We study the family of network models derived by requiring the expected properties of a graph ensemble to match a given set of measurements of a real-world network, while maximizing the entropy of the ensemble. Models of this type play the same role in the study of networks as is played by the Boltzmann distribution in classical statistical mechanics; they offer the best prediction of network properties subject to the constraints imposed by a given set of observations. We give exact solutions of models within this class that incorporate arbitrary degree distributions and arbitrary but independent edge probabilities. We also discuss some more complex examples with correlated edges that can be solved approximately or exactly by adapting various familiar methods, including mean-field theory, perturbation theory, and saddle-point expansions.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics},
  file = {/home/marnix/Zotero/storage/7H8ZEQ3F/Park and Newman - 2004 - The statistical mechanics of networks.pdf;/home/marnix/Zotero/storage/UD22KXVL/0405566.html}
}

@inproceedings{Park2008,
  title = {Gaussian Process Regression for Voice Activity Detection and Speech Enhancement},
  booktitle = {2008 {{IEEE International Joint Conference}} on {{Neural Networks}} ({{IEEE World Congress}} on {{Computational Intelligence}})},
  author = {Park, Sunho and Seungjin Choi},
  year = {2008},
  month = jun,
  pages = {2879--2882},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2008.4634203},
  abstract = {Gaussian process (GP) model is a flexible nonparametric Bayesian method that is widely used in regression and classification. In this paper we present a probabilistic method where we solve voice activity detection (VAD) and speech enhancement in a single framework of GP regression, modeling clean speech by a GP smoother. Optimized hyperparameters in GP models lead us to a novel VAD method since learned length-scale parameters in covariance functions are much different between voiced and unvoiced frames. Clean speech is estimated by posterior means in GP models. Numerical experiments confirm the validity of our method.},
  keywords = {Bayes methods,covariance functions,Gaussian process regression,Gaussian processes,Hidden Markov models,Kalman filters,nonparametric Bayesian method,nonparametric statistics,Numerical models,optimized hyperparameters,probabilistic method,probability,regression analysis,signal classification,signal detection,Signal to noise ratio,Speech,speech enhancement,Speech enhancement,Speech processing,voice activity detection},
  file = {/home/marnix/Zotero/storage/P6M9D38M/Park and Seungjin Choi - 2008 - Gaussian process regression for voice activity det.pdf;/home/marnix/Zotero/storage/FZZD9J7S/4634203.html}
}

@misc{Park2023,
  title = {Generative {{Agents}}: {{Interactive Simulacra}} of {{Human Behavior}}},
  shorttitle = {Generative {{Agents}}},
  author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  year = {2023},
  month = apr,
  number = {arXiv:2304.03442},
  eprint = {2304.03442},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-04-25},
  abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/home/marnix/Zotero/storage/6JDM4W52/Park et al. - 2023 - Generative Agents Interactive Simulacra of Human .pdf;/home/marnix/Zotero/storage/HHQV9VYM/2304.html}
}

@incollection{Parra2017,
  title = {Spectral {{Mixture Kernels}} for {{Multi-Output Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Parra, Gabriel and Tobar, Felipe},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {6681--6690},
  publisher = {Curran Associates, Inc.},
  urldate = {2020-10-05},
  file = {/home/marnix/Zotero/storage/8EFGKHKH/Parra and Tobar - 2017 - Spectral Mixture Kernels for Multi-Output Gaussian.pdf;/home/marnix/Zotero/storage/I2LVJMVV/7245-spectral-mixture-kernels-for-multi-output-gaussian-processes.html}
}

@article{Passerini2008,
  title = {The von {{Neumann Entropy}} of {{Networks}}},
  author = {Passerini, Filippo and Severini, Simone},
  year = {2008},
  month = apr,
  journal = {SSRN Electronic Journal},
  eprint = {0812.2597},
  pages = {1--6},
  issn = {1556-5068},
  doi = {10.2139/ssrn.1382662},
  abstract = {We normalize the combinatorial Laplacian of a graph by the degree sum, look at its eigenvalues as a probability distribution and then study its Shannon entropy. Equivalently, we represent a graph with a quantum mechanical state and study its von Neumann entropy. At the graph-theoretic level, this quantity may be interpreted as a measure of regularity; it tends to be larger in relation to the number of connected components, long paths and nontrivial symmetries. When the set of vertices is asymptotically large, we prove that regular graphs and the complete graph have equal entropy, and specifically it turns out to be maximum. On the other hand, when the number of edges is fixed, graphs with large cliques appear to minimize the entropy.},
  archiveprefix = {arXiv},
  arxivid = {0812.2597},
  isbn = {9781609601713},
  keywords = {entropy},
  file = {/home/marnix/Zotero/storage/4G4U8WQI/Passerini, Severini - 2008 - The von Neumann Entropy of Networks.pdf}
}

@misc{Patel,
  title = {Google "{{We Have No Moat}}, {{And Neither Does OpenAI}}"},
  author = {Patel, Dylan},
  urldate = {2024-04-14},
  abstract = {Leaked Internal Google Document Claims Open Source AI Will Outcompete Google and OpenAI},
  howpublished = {https://www.semianalysis.com/p/google-we-have-no-moat-and-neither},
  langid = {english},
  file = {/home/marnix/Zotero/storage/4D8ZVFMR/google-we-have-no-moat-and-neither.html}
}

@article{Paul2016,
  title = {Consistent Community Detection in Multi-Relational Data through Restricted Multi-Layer Stochastic Blockmodel},
  author = {Paul, Subhadeep and Chen, Yuguo},
  year = {2016},
  journal = {Electronic Journal of Statistics},
  volume = {10},
  number = {2},
  eprint = {1506.02699},
  pages = {3807--3870},
  issn = {19357524},
  doi = {10.1214/16-EJS1211},
  abstract = {In recent years there has been an increased interest in statistical analysis of data with multiple types of relations among a set of entities. Such multi-relational data can be represented as multi-layer graphs where the set of vertices represents the entities and multiple types of edges represent the different relations among them. For community detection in multi-layer graphs, we consider two random graph models, the multi-layer stochastic blockmodel (MLSBM) and a model with a restricted parameter space, the restricted multi-layer stochastic blockmodel (RMLSBM). We derive consistency results for community assignments of the maximum likelihood estimators (MLEs) in both models where MLSBM is assumed to be the true model, and either the number of nodes or the number of types of edges or both grow. We compare MLEs in the two models with other baseline approaches, such as separate modeling of layers, aggregating the layers and majority voting. RMLSBM is shown to have advantage over MLSBM when either the growth rate of the number of communities is high or the growth rate of the average degree of the component graphs in the multi-graph is low. We also derive minimax rates of error and sharp thresholds for achieving consistency of community detection in both models, which are then used to compare the multi-layer models with a baseline model, the aggregate stochastic block model. The simulation studies and real data applications confirm the superior performance of the multi-layer approaches in comparison to the baseline procedures.},
  archiveprefix = {arXiv},
  arxivid = {1506.02699},
  keywords = {Community detection,Consistency,Consistency thresholds,Minimax rates,Multi-layer networks,Stochastic blockmodel},
  file = {/home/marnix/Zotero/storage/QWSKINCV/Paul2016 Community Detection in Multi-Relational Data Through Restricted Multi-Layer Stochastic Blockmodel.pdf}
}

@article{Peixoto2012,
  title = {No Need for Conspiracy: {{Self-organized}} Cartel Formation in a Modified Trust Game},
  author = {Peixoto, Tiago P and Bornholdt, Stefan},
  year = {2012},
  journal = {Physical review letters},
  volume = {108},
  number = {21},
  pages = {218702},
  publisher = {APS}
}

@article{Peixoto2014,
  title = {Hierarchical Block Structures and High-Resolution Model Selection in Large Networks},
  author = {Peixoto, Tiago P},
  year = {2014},
  journal = {Physical Review X},
  volume = {4},
  number = {1},
  pages = {11047},
  publisher = {APS},
  file = {/home/marnix/Zotero/storage/KCEBZWVE/Peixoto - 2014 - Hierarchical block structures and high-resolution model selection in large networks.pdf}
}

@article{Peixoto2014a,
  title = {Efficient {{Monte Carlo}} and Greedy Heuristic for the Inference of Stochastic Block Models},
  author = {Peixoto, Tiago P},
  year = {2014},
  journal = {Physical Review E},
  volume = {89},
  number = {1},
  pages = {12804},
  publisher = {APS},
  file = {/home/marnix/Zotero/storage/QWRFVL4F/Peixoto - 2014 - Efficient Monte Carlo and greedy heuristic for the inference of stochastic block models.pdf}
}

@article{Peixoto2014b,
  title = {The Graph-Tool {{Python}} Library},
  author = {Peixoto, Tiago P},
  year = {2014},
  journal = {figshare},
  doi = {10.6084/m9.figshare.1164194},
  keywords = {all,complex networks,graph,network,other}
}

@article{Peixoto2015a,
  title = {Inferring the Mesoscale Structure of Layered, Edge-Valued, and Time-Varying Networks},
  author = {Peixoto, Tiago P},
  year = {2015},
  journal = {Physical Review E},
  volume = {92},
  number = {4},
  pages = {42807},
  publisher = {APS},
  file = {/home/marnix/Zotero/storage/4QXQR5Z6/Peixoto - 2015 - Inferring the mesoscale structure of layered, edge-valued, and time-varying networks(4).pdf}
}

@article{Peixoto2016,
  title = {Nonparametric {{Bayesian}} Inference of the Microcanonical Stochastic Block Model},
  author = {Peixoto, Tiago P.},
  year = {2016},
  journal = {Physical Review E},
  volume = {95},
  number = {1},
  eprint = {1610.02703},
  issn = {2470-0045},
  doi = {10.1103/PhysRevE.95.012317},
  abstract = {A principled approach to characterize the hidden structure of networks is to formulate generative models, and then infer their parameters from data. When the desired structure is composed of modules or "communities", a suitable choice for this task is the stochastic block model (SBM), where nodes are divided into groups, and the placement of edges is conditioned on the group memberships. Here, we present a nonparametric Bayesian method to infer the modular structure of empirical networks, including the number of modules and their hierarchical organization. We focus on a microcanonical variant of the SBM, where the structure is imposed via hard constraints, i.e. the generated networks are not allowed to violate the patterns imposed by the model. We show how this simple model variation allows simultaneously for two important improvements over more traditional inference approaches: 1. Deeper Bayesian hierarchies, with noninformative priors replaced by sequences of priors and hyperpriors, that not only remove limitations that seriously degrade the inference on large networks, but also reveal structures at multiple scales; 2. A very efficient inference algorithm that scales well not only for networks with a large number of nodes and edges, but also with an unlimited number of modules. We show also how this approach can be used to sample modular hierarchies from the posterior distribution, as well as to perform model selection. We discuss and analyze the differences between sampling from the posterior and simply finding the single parameter estimate that maximizes it. Furthermore, we expose a direct equivalence between our microcanonical approach and alternative derivations based on the canonical SBM.},
  archiveprefix = {arXiv},
  arxivid = {1610.02703},
  file = {/home/marnix/Zotero/storage/JQ5HKGUA/Peixoto - 2016 - Nonparametric Bayesian inference of the microcanon.pdf}
}

@article{Peixoto2017,
  title = {Bayesian Stochastic Blockmodeling},
  author = {Peixoto, Tiago P.},
  year = {2017},
  eprint = {1705.10225},
  issn = {09420940},
  doi = {10.1007/9783642532580},
  abstract = {This chapter provides a self-contained introduction to the use of Bayesian inference to extract large-scale modular structures from network data, based on the stochastic block model (SBM), as well as its degree-corrected and overlapping generalizations. We focus on nonparametric formulations that allow their inference in a manner that prevents overfitting, and enables model selection. We discuss aspects on the choice of priors, in particular how to avoid underfitting via increased Bayesian hierarchies, and we contrast the task of sampling network partitions from the posterior distribution with finding the single point estimate that maximizes it, while describing efficient algorithms to perform either one. We also show how inferring the SBM can be used to predict missing and spurious links, and shed light on the fundamental limitations of the detectability of modular structures in networks.},
  archiveprefix = {arXiv},
  arxivid = {1705.10225},
  isbn = {0671631985},
  pmid = {27489955},
  file = {/home/marnix/Zotero/storage/NXNMKMBF/Peixoto - 2017 - Bayesian stochastic blockmodeling.pdf}
}

@article{Peixoto2017a,
  ids = {Peixoto2017b},
  title = {Modelling Sequences and Temporal Networks with Dynamic Community Structures},
  author = {Peixoto, Tiago P. and Rosvall, Martin},
  year = {2017},
  month = sep,
  journal = {Nature Communications},
  volume = {8},
  number = {1},
  eprint = {1509.04740},
  pages = {582},
  issn = {2041-1723},
  doi = {10.1038/s41467-017-00148-9},
  urldate = {2019-01-28},
  abstract = {The description of temporal networks is usually simplified in terms of their dynamic community structures, whose identification however relies on a priori assumptions. Here the authors present a data-driven method that determines relevant timescales for the dynamics and uses it to identify communities.},
  archiveprefix = {arXiv},
  copyright = {2017 The Author(s)},
  langid = {english},
  keywords = {Computer Science - Social and Information Networks,Condensed Matter - Statistical Mechanics,Physics - Physics and Society,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/IB536QUM/Peixoto and Rosvall - 2017 - Modelling sequences and temporal networks with dyn.pdf;/home/marnix/Zotero/storage/ZCKKR2UA/Peixoto and Rosvall - 2017 - Modelling sequences and temporal networks with dyn.pdf;/home/marnix/Zotero/storage/95CPISNK/s41467-017-00148-9.html}
}

@article{Peixoto2018,
  title = {Nonparametric Weighted Stochastic Block Models},
  author = {Peixoto, Tiago P.},
  year = {2018},
  journal = {Physical Review E},
  volume = {97},
  number = {1},
  eprint = {1708.01432},
  issn = {24700053},
  doi = {10.1103/PhysRevE.97.012306},
  abstract = {We present a Bayesian formulation of weighted stochastic block models that can be used to infer the large-scale modular structure of weighted networks, including their hierarchical organization. Our method is nonparametric, and thus does not require the prior knowledge of the number of groups or other dimensions of the model, which are instead inferred from data. We give a comprehensive treatment of different kinds of edge weights (i.e. continuous or discrete, signed or unsigned, bounded or unbounded), as well as arbitrary weight transformations, and describe an unsupervised model selection approach to choose the best network description. We illustrate the application of our method to a variety of empirical weighted networks, such as global migrations, voting patterns in congress, and neural connections in the human brain.},
  archiveprefix = {arXiv},
  arxivid = {1708.01432},
  file = {/home/marnix/Zotero/storage/BVCVFTRL/Peixoto 2017 Nonparametric weighted stochastic block models.pdf}
}

@article{Peixoto2020,
  title = {Latent {{Poisson}} Models for Networks with Heterogeneous Density},
  author = {Peixoto, Tiago P.},
  year = {2020},
  month = jul,
  journal = {Physical Review E},
  volume = {102},
  number = {1},
  eprint = {2002.07803},
  pages = {012309},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.102.012309},
  urldate = {2021-12-16},
  abstract = {Empirical networks are often globally sparse, with a small average number of connections per node, when compared to the total size of the network. However, this sparsity tends not to be homogeneous, and networks can also be locally dense, for example with a few nodes connecting to a large fraction of the rest of the network, or with small groups of nodes with a large probability of connections between them. Here we show how latent Poisson models which generate hidden multigraphs can be effective at capturing this density heterogeneity, while being more tractable mathematically than some of the alternatives that model simple graphs directly. We show how these latent multigraphs can be reconstructed from data on simple graphs, and how this allows us to disentangle disassortative degree-degree correlations from the constraints of imposed degree sequences, and to improve the identification of community structure in empirically relevant scenarios.},
  archiveprefix = {arXiv},
  keywords = {{Physics - Data Analysis, Statistics and Probability},Computer Science - Machine Learning,Computer Science - Social and Information Networks,Physics - Physics and Society,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/MBR6KR7K/Peixoto - 2020 - Latent Poisson models for networks with heterogene.pdf;/home/marnix/Zotero/storage/8QLMLLET/2002.html}
}

@article{Peixoto2020a,
  title = {Merge-Split {{Markov}} Chain {{Monte Carlo}} for Community Detection},
  author = {Peixoto, Tiago P.},
  year = {2020},
  month = jul,
  journal = {Physical Review E},
  volume = {102},
  number = {1},
  eprint = {2003.07070},
  primaryclass = {physics, stat},
  pages = {012305},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.102.012305},
  urldate = {2024-10-06},
  abstract = {We present a Markov chain Monte Carlo scheme based on merges and splits of groups that is capable of efficiently sampling from the posterior distribution of network partitions, defined according to the stochastic block model (SBM). We demonstrate how schemes based on the move of single nodes between groups systematically fail at correctly sampling from the posterior distribution even on small networks, and how our merge-split approach behaves significantly better, and improves the mixing time of the Markov chain by several orders of magnitude in typical cases. We also show how the scheme can be straightforwardly extended to nested versions of the SBM, yielding asymptotically exact samples of hierarchical network partitions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {{Physics - Data Analysis, Statistics and Probability},Computer Science - Machine Learning,Computer Science - Social and Information Networks,Physics - Physics and Society,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/P62LCFMB/Peixoto - 2020 - Merge-split Markov chain Monte Carlo for community detection.pdf}
}

@misc{Peixoto2022,
  title = {Descriptive vs. Inferential Community Detection},
  author = {Peixoto, Tiago P.},
  year = {2022},
  month = aug,
  number = {arXiv:2112.00183},
  eprint = {2112.00183},
  primaryclass = {physics, stat},
  publisher = {arXiv},
  urldate = {2022-09-15},
  abstract = {Community detection is one of the most important methodological fields of network science, and one which has attracted a significant amount of attention over the past decades. This area deals with the automated division of a network into fundamental building blocks, with the objective of providing a summary of its large-scale structure. Despite its importance and widespread adoption, there is a noticeable gap between what is arguably the state-of-the-art and the methods that are actually used in practice in a variety of fields. Here we attempt to address this discrepancy by dividing existing methods according to whether they have a "descriptive" or an "inferential" goal. While descriptive methods find patterns in networks based on context-dependent notions of community structure, inferential methods articulate generative models, and attempt to fit them to data. In this way, they are able to provide insights into the mechanisms of network formation, and separate structure from randomness in a manner supported by statistical evidence. We review how employing descriptive methods with inferential aims is riddled with pitfalls and misleading answers, and thus should be in general avoided. We argue that inferential methods are more typically aligned with clearer scientific questions, yield more robust results, and should be in many cases preferred. We attempt to dispel some myths and half-truths often believed when community detection is employed in practice, in an effort to improve both the use of such methods as well as the interpretation of their results.},
  archiveprefix = {arXiv},
  keywords = {{Physics - Data Analysis, Statistics and Probability},Computer Science - Social and Information Networks,Physics - Physics and Society,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/marnix/Zotero/storage/KD383Q5B/Peixoto - 2022 - Descriptive vs. inferential community detection.pdf;/home/marnix/Zotero/storage/JS9IIGQF/2112.html}
}

@article{Peixoto2022a,
  title = {Ordered Community Detection in Directed Networks},
  author = {Peixoto, Tiago P.},
  year = {2022},
  month = aug,
  journal = {Physical Review E},
  volume = {106},
  number = {2},
  pages = {024305},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.106.024305},
  urldate = {2024-09-19},
  abstract = {We develop a method to infer community structure in directed networks where the groups are ordered in a latent one-dimensional hierarchy that determines the preferred edge direction. Our nonparametric Bayesian approach is based on a modification of the stochastic block model (SBM), which can take advantage of rank alignment and coherence to produce parsimonious descriptions of networks that combine ordered hierarchies with arbitrary mixing patterns between groups. Since our model also includes directed degree correction, we can use it to distinguish nonlocal hierarchical structure from local in- and out-degree imbalance---thus, removing a source of conflation present in most ranking methods. We also demonstrate how we can reliably compare with the results obtained with the unordered SBM variant to determine whether a hierarchical ordering is statistically warranted in the first place. We illustrate the application of our method on a wide variety of empirical networks across several domains.},
  file = {/home/marnix/Zotero/storage/WCTZ9GDW/Peixoto - 2022 - Ordered community detection in directed networks.pdf;/home/marnix/Zotero/storage/UYF56ZCJ/PhysRevE.106.html}
}

@misc{Peixoto2024,
  title = {Network Reconstruction via the Minimum Description Length Principle},
  author = {Peixoto, Tiago P.},
  year = {2024},
  month = may,
  number = {arXiv:2405.01015},
  eprint = {2405.01015},
  primaryclass = {physics, q-bio, stat},
  publisher = {arXiv},
  urldate = {2024-10-03},
  abstract = {A fundamental problem associated with the task of network reconstruction from dynamical or behavioral data consists in determining the most appropriate model complexity in a manner that prevents overfitting, and produces an inferred network with a statistically justifiable number of edges. The status quo in this context is based on \$L\_\{1\}\$ regularization combined with cross-validation. However, besides its high computational cost, this commonplace approach unnecessarily ties the promotion of sparsity with weight "shrinkage". This combination forces a trade-off between the bias introduced by shrinkage and the network sparsity, which often results in substantial overfitting even after cross-validation. In this work, we propose an alternative nonparametric regularization scheme based on hierarchical Bayesian inference and weight quantization, which does not rely on weight shrinkage to promote sparsity. Our approach follows the minimum description length (MDL) principle, and uncovers the weight distribution that allows for the most compression of the data, thus avoiding overfitting without requiring cross-validation. The latter property renders our approach substantially faster to employ, as it requires a single fit to the complete data. As a result, we have a principled and efficient inference scheme that can be used with a large variety of generative models, without requiring the number of edges to be known in advance. We also demonstrate that our scheme yields systematically increased accuracy in the reconstruction of both artificial and empirical networks. We highlight the use of our method with the reconstruction of interaction networks between microbial communities from large-scale abundance samples involving in the order of \$10{\textasciicircum}\{4\}\$ to \$10{\textasciicircum}\{5\}\$ species, and demonstrate how the inferred model can be used to predict the outcome of interventions in the system.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {{Physics - Data Analysis, Statistics and Probability},Computer Science - Machine Learning,Computer Science - Social and Information Networks,Quantitative Biology - Populations and Evolution,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/BHCV6HC8/Peixoto - 2024 - Network reconstruction via the minimum description length principle.pdf}
}

@article{Pellegrino2011,
  title = {Across-{{Language Perspective}} on {{Speech Information Rate}}},
  author = {Pellegrino, Fran{\c c}ois and Coup{\'e}, Christophe and Marsico, Egidio},
  year = {2011},
  journal = {Language},
  volume = {87},
  number = {3},
  pages = {539--558},
  issn = {1535-0665},
  doi = {10.1353/lan.2011.0057},
  abstract = {This article is a crosslinguistic investigation of the hypothesis that the average information rate conveyed during speech communication results from a trade-off between average information density and speech rate. The study, based on seven languages, shows a negative correlation between density and rate, indicating the existence of several encoding strategies. However, these strategies do not necessarily lead to a constant information rate. These results are further investigated in relation to the notion of syllabic complexity.},
  isbn = {0097-8507},
  pmid = {21252317},
  keywords = {cross-lan- guage study,information theory,speech communication,speech rate,working memory},
  file = {/home/marnix/Zotero/storage/KB6PPZ8T/Pellegrino2011 A cross-language perspective on speech information rate.pdf}
}

@misc{Pena2025,
  title = {Properties of the Generalized Inverse {{Gaussian}} with Applications to {{Monte Carlo}} Simulation and Distribution Function Evaluation},
  author = {Pe{\~n}a, Victor and Jauch, Michael},
  year = {2025},
  month = jan,
  number = {arXiv:2401.00749},
  eprint = {2401.00749},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.00749},
  urldate = {2025-05-06},
  abstract = {The generalized inverse Gaussian, denoted \${\textbackslash}mathrm\{GIG\}(p, a, b)\$, is a flexible family of distributions that includes the gamma, inverse gamma, and inverse Gaussian distributions as special cases. In addition to its applications in statistical modeling and its theoretical interest, the GIG often arises in computational statistics, especially in Markov chain Monte Carlo (MCMC) algorithms for posterior inference. This article introduces two mixture representations for the GIG: one that expresses the distribution as a continuous mixture of inverse Gaussians and another that reveals a recursive relationship between GIGs with different values of \$p\$. The former representation forms the basis for a data augmentation scheme that leads to a geometrically ergodic Gibbs sampler for the GIG. This simple Gibbs sampler, which alternates between gamma and inverse Gaussian conditional distributions, can be incorporated within an encompassing MCMC algorithm when simulation from a GIG is required. The latter representation leads to algorithms for exact, rejection-free sampling as well as CDF evaluation for the GIG with half-integer \$p.\$ We highlight computational examples from the literature where these new algorithms could be applied.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Probability,Statistics - Computation},
  file = {/home/marnix/Zotero/storage/S8IWBRM8/Peña and Jauch - 2025 - Properties of the generalized inverse Gaussian with applications to Monte Carlo simulation and distr.pdf;/home/marnix/Zotero/storage/CL6YI5KD/2401.html}
}

@article{Peng2024,
  title = {{{VOICECRAFT}}: {{Zero-Shot Speech Editing}} and {{Text-to-Speech}} in the {{Wild}}},
  author = {Peng, Puyuan and Huang, Po-Yao and Li, Shang-Wen and Mohamed, Abdelrahman and Harwath, David},
  year = {2024},
  abstract = {We introduce VOICECRAFT, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts1. VOICECRAFT employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VOICECRAFT produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models and real recordings. In particular, for speech editing evaluation, we introduce a high quality, challenging, and realistic dataset named REALEDIT. We encourage readers to listen to the demos at https: //jasonppy.github.io/VoiceCraft\_web.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/Z2AAUL6B/Peng et al. - VOICECRAFT Zero-Shot Speech Editing and Text-to-S.pdf}
}

@article{Perrotin2017,
  title = {On the {{Use}} of a {{Spectral Glottal Model}} for the {{Source-filter Separation}} of {{Speech}}},
  author = {Perrotin, Olivier and McLoughlin, Ian Vince},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.08034 [cs, eess]},
  eprint = {1712.08034},
  primaryclass = {cs, eess},
  urldate = {2019-03-12},
  abstract = {The estimation of glottal flow from a speech waveform is a key method for speech analysis and parameterization. Significant research effort has been made to dissociate the first vocal tract resonance from the glottal formant (the low-frequency resonance describing the open-phase of the vocal fold vibration). However few methods cope with estimation of high-frequency spectral tilt to describe the return-phase of the vocal fold vibration, which is crucial to the perception of vocal effort. This paper proposes an improved version of the well-known Iterative Adaptive Inverse Filtering (IAIF) called GFM-IAIF. GFM-IAIF includes a full spectral model of the glottis that incorporates both glottal formant and spectral tilt features. Comparisons with the standard IAIF method show that while GFM-IAIF maintains good performance on vocal tract removal, it significantly improves the perceptive timbral variations associated to vocal effort.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/marnix/Zotero/storage/RUS96PYK/Perrotin and McLoughlin - 2017 - On the Use of a Spectral Glottal Model for the Sou.pdf;/home/marnix/Zotero/storage/DGV9SS7B/1712.html}
}

@article{Perrotin2021,
  title = {Perceptual Equivalence of the {{Liljencrants}}--{{Fant}} and Linear-Filter Glottal Flow Models},
  author = {Perrotin, Olivier and Feug{\`e}re, Lionel and {d'Alessandro}, Christophe},
  year = {2021},
  month = aug,
  journal = {The Journal of the Acoustical Society of America},
  volume = {150},
  number = {2},
  pages = {1273--1285},
  publisher = {Acoustical Society of America},
  issn = {0001-4966},
  doi = {10.1121/10.0005879},
  urldate = {2022-04-08},
  abstract = {Speech glottal flow has been predominantly described in the time-domain in past decades, the Liljencrants--Fant (LF) model being the most widely used in speech analysis and synthesis, despite its computational complexity. The causal/anti-causal linear model (LFCALM) was later introduced as a digital filter implementation of LF, a mixed-phase spectral model including both anti-causal and causal filters to model the vocal-fold open and closed phases, respectively. To further simplify computation, a causal linear model (LFLM) describes the glottal flow with a fully causal set of filters. After expressing these three models under a single analytic formulation, we assessed here their perceptual consistency, when driven by a single parameter Rd related to voice quality. All possible paired combinations of signals generated using six\,Rd levels for each model were presented to subjects who were asked whether the two signals in each pair differed. Model pairs LFLM--LFCALM were judged similar when sharing the same Rd value, and LF was considered the same as LFLM and LFCALM given a consistent shift in Rd. Overall, the similarity between these models encourages the use of the simpler and more computationally efficient models LFCALM and LFLM in speech synthesis applications.},
  file = {/home/marnix/Zotero/storage/2GJNTMWP/Perrotin et al. - 2021 - Perceptual equivalence of the Liljencrants–Fant an.pdf}
}

@article{Petersen2008,
  title = {The Matrix Cookbook, Vol. 7},
  author = {Petersen, {\relax KB} and Pedersen, {\relax MS} and others},
  year = {2008},
  journal = {Technical University of Denmark},
  volume = {15},
  file = {/home/marnix/Zotero/storage/K3ADBY2Y/Petersen et al. - 2008 - The matrix cookbook, vol. 7.pdf}
}

@article{Peterson1952,
  title = {Control Methods Used in a Study of the Vowels},
  author = {Peterson, Gordon E and Barney, Harold L},
  year = {1952},
  journal = {The Journal of the acoustical society of America},
  volume = {24},
  number = {2},
  pages = {175--184},
  publisher = {ASA},
  file = {/home/marnix/Zotero/storage/2K55JSM5/Peterson and Barney - Control Methods Used in a Study of the Vowels.pdf}
}

@article{Peterson1966,
  title = {The {{Elements}} of an {{Acoustic Phonetic Theory}}},
  author = {Peterson, Gordon E. and Shoup, June E.},
  year = {1966},
  month = mar,
  journal = {Journal of Speech and Hearing Research},
  volume = {9},
  number = {1},
  pages = {68--99},
  issn = {0022-4685},
  doi = {10.1044/jshr.0901.68},
  urldate = {2019-03-13},
  langid = {english},
  file = {/home/marnix/Zotero/storage/NZ6PB2DW/Peterson and Shoup - 1966 - The Elements of an Acoustic Phonetic Theory.pdf}
}

@article{Petrosyan2023,
  title = {{{SuperNest}}: {{Accelerated Nested Sampling Applied}} to {{Astrophysics}} and {{Cosmology}}},
  shorttitle = {{{SuperNest}}},
  author = {Petrosyan, Aleksandr and Handley, Will},
  year = {2023},
  journal = {Physical Sciences Forum},
  volume = {5},
  number = {1},
  pages = {51},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2673-9984},
  doi = {10.3390/psf2022005051},
  urldate = {2024-01-05},
  abstract = {We present a method for improving the performance of nested sampling as well as its accuracy. Building on previous work we show that posterior repartitioning may be used to reduce the amount of time nested sampling spends in compressing from prior to posterior if a suitable ``proposal'' distribution is supplied. We showcase this on a cosmological example with a Gaussian posterior, and release the code as an LGPL licensed, extensible Python package supernest.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Bayesian inference},
  file = {/home/marnix/Zotero/storage/DYDJC53D/Petrosyan and Handley - 2023 - SuperNest Accelerated Nested Sampling Applied to .pdf}
}

@article{Pfeiffer1954,
  title = {The Maximum Response Ratio of Linear Systems},
  author = {Pfeiffer, P. E.},
  year = {1954},
  month = dec,
  journal = {Electrical Engineering},
  volume = {73},
  number = {12},
  pages = {1093--1093},
  issn = {0095-9197},
  doi = {10.1109/EE.1954.6439137},
  urldate = {2022-03-02},
  file = {/home/marnix/Zotero/storage/PZ5JXNPU/Pfeiffer - 1954 - The maximum response ratio of linear systems.pdf}
}

@article{Pinson1963,
  title = {Pitch-{{Synchronous Time}}-{{Domain Estimation}} of {{Formant Frequencies}} and {{Bandwidths}}},
  author = {Pinson, Elliot N.},
  year = {1963},
  month = aug,
  journal = {The Journal of the Acoustical Society of America},
  volume = {35},
  number = {8},
  pages = {1264--1273},
  issn = {0001-4966},
  doi = {10.1121/1.1918682},
  urldate = {2019-12-22},
  file = {/home/marnix/Zotero/storage/89SQFREJ/Pinson - 1963 - Pitch‐Synchronous Time‐Domain Estimation of Forman.pdf;/home/marnix/Zotero/storage/7DAEIC9S/1.html}
}

@misc{Pleiss2020,
  title = {Fast {{Matrix Square Roots}} with {{Applications}} to {{Gaussian Processes}} and {{Bayesian Optimization}}},
  author = {Pleiss, Geoff and Jankowiak, Martin and Eriksson, David and Damle, Anil and Gardner, Jacob R.},
  year = {2020},
  month = nov,
  number = {arXiv:2006.11267},
  eprint = {2006.11267},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.11267},
  urldate = {2025-08-22},
  abstract = {Matrix square roots and their inverses arise frequently in machine learning, e.g., when sampling from high-dimensional Gaussians \${\textbackslash}mathcal\{N\}({\textbackslash}mathbf 0, {\textbackslash}mathbf K)\$ or whitening a vector \${\textbackslash}mathbf b\$ against covariance matrix \${\textbackslash}mathbf K\$. While existing methods typically require \$O(N{\textasciicircum}3)\$ computation, we introduce a highly-efficient quadratic-time algorithm for computing \${\textbackslash}mathbf K{\textasciicircum}\{1/2\} {\textbackslash}mathbf b\$, \${\textbackslash}mathbf K{\textasciicircum}\{-1/2\} {\textbackslash}mathbf b\$, and their derivatives through matrix-vector multiplication (MVMs). Our method combines Krylov subspace methods with a rational approximation and typically achieves \$4\$ decimal places of accuracy with fewer than \$100\$ MVMs. Moreover, the backward pass requires little additional computation. We demonstrate our method's applicability on matrices as large as \$50,{\textbackslash}!000 {\textbackslash}times 50,{\textbackslash}!000\$ - well beyond traditional methods - with little approximation error. Applying this increased scalability to variational Gaussian processes, Bayesian optimization, and Gibbs sampling results in more powerful models with higher accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/T66J8DWY/Pleiss et al. - 2020 - Fast Matrix Square Roots with Applications to Gaussian Processes and Bayesian Optimization.pdf;/home/marnix/Zotero/storage/WH7VZ7SZ/2006.html}
}

@article{Plumpe1999,
  title = {Modeling of the Glottal Flow Derivative Waveform with Application to Speaker Identification},
  author = {Plumpe, M.D. and Quatieri, T.F. and Reynolds, D.A.},
  year = {1999},
  month = sep,
  journal = {IEEE Transactions on Speech and Audio Processing},
  volume = {7},
  number = {5},
  pages = {569--586},
  issn = {1558-2353},
  doi = {10.1109/89.784109},
  abstract = {An automatic technique for estimating and modeling the glottal flow derivative source waveform from speech, and applying the model parameters to speaker identification, is presented. The estimate of the glottal flow derivative is decomposed into coarse structure, representing the general flow shape, and fine structure, comprising aspiration and other perturbations in the flow, from which model parameters are obtained. The glottal flow derivative is estimated using an inverse filter determined within a time interval of vocal-fold closure that is identified through differences in formant frequency modulation during the open and closed phases of the glottal cycle. This formant motion is predicted by Ananthapadmanabha and Fant (1982) to be a result of time-varying and nonlinear source/vocal tract coupling within a glottal cycle. The glottal flow derivative estimate is modeled using the Liljencrants-Fant (1986) model to capture its coarse structure, while the fine structure of the flow derivative is represented through energy and perturbation measures. The model parameters are used in a Gaussian mixture model speaker identification (SID) system. Both coarse- and fine-structure glottal features are shown to contain significant speaker-dependent information. For a large TIMIT database subset, averaging over male and female SID scores, the coarse-structure parameters achieve about 60\% accuracy, the fine-structure parameters give about 40\% accuracy, and their combination yields about 70\% correct identification. Finally, in preliminary experiments on the counterpart telephone-degraded NTIMIT database, about a 5\% error reduction in SID scores is obtained when source features are combined with traditional mel-cepstral measures.},
  keywords = {Couplings,Energy capture,Energy measurement,Filters,Frequency estimation,Frequency modulation,Phase estimation,Shape,Spatial databases,Speech},
  file = {/home/marnix/Zotero/storage/A8JBDP9Z/Plumpe et al. - 1999 - Modeling of the glottal flow derivative waveform w.pdf;/home/marnix/Zotero/storage/XIIPUUBW/784109.html}
}

@article{Podusenko2021,
  title = {Message {{Passing-Based Inference}} for {{Time-Varying Autoregressive Models}}},
  author = {Podusenko, Albert and Kouw, Wouter M. and {de Vries}, Bert},
  year = {2021},
  month = jun,
  journal = {Entropy},
  volume = {23},
  number = {6},
  pages = {683},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e23060683},
  urldate = {2024-02-23},
  abstract = {Time-varying autoregressive (TVAR) models are widely used for modeling of non-stationary signals. Unfortunately, online joint adaptation of both states and parameters in these models remains a challenge. In this paper, we represent the TVAR model by a factor graph and solve the inference problem by automated message passing-based inference for states and parameters. We derive structured variational update rules for a composite ``AR node'' with probabilistic observations that can be used as a plug-in module in hierarchical models, for example, to model the time-varying behavior of the hyper-parameters of a time-varying AR model. Our method includes tracking of variational free energy (FE) as a Bayesian measure of TVAR model performance. The proposed methods are verified on a synthetic data set and validated on real-world data from temperature modeling and speech enhancement tasks.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Bayesian inference,factor graph,free energy,hybrid message passing,model selection,non-stationary systems,probabilistic graphical models},
  file = {/home/marnix/Zotero/storage/MVQUN8C2/Podusenko et al. - 2021 - Message Passing-Based Inference for Time-Varying A.pdf}
}

@article{Poeppel2008,
  title = {Speech Perception at the Interface of Neurobiology and Linguistics},
  author = {Poeppel, David and Idsardi, William J and {van Wassenhove}, Virginie},
  year = {2008},
  month = mar,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {363},
  number = {1493},
  pages = {1071--1086},
  issn = {0962-8436},
  doi = {10.1098/rstb.2007.2160},
  urldate = {2019-02-13},
  abstract = {Speech perception consists of a set of computations that take continuously varying acoustic waveforms as input and generate discrete representations that make contact with the lexical representations stored in long-term memory as output. Because the perceptual objects that are recognized by the speech perception enter into subsequent linguistic computation, the format that is used for lexical representation and processing fundamentally constrains the speech perceptual processes. Consequently, theories of speech perception must, at some level, be tightly linked to theories of lexical representation. Minimally, speech perception must yield representations that smoothly and rapidly interface with stored lexical items. Adopting the perspective of Marr, we argue and provide neurobiological and psychophysical evidence for the following research programme. First, at the implementational level, speech perception is a multi-time resolution process, with perceptual analyses occurring concurrently on at least two time scales (approx. 20--80{$\mkern1mu$}ms, approx. 150--300{$\mkern1mu$}ms), commensurate with (sub)segmental and syllabic analyses, respectively. Second, at the algorithmic level, we suggest that perception proceeds on the basis of internal forward models, or uses an `analysis-by-synthesis' approach. Third, at the computational level (in the sense of Marr), the theory of lexical representation that we adopt is principally informed by phonological research and assumes that words are represented in the mental lexicon in terms of sequences of discrete segments composed of distinctive features. One important goal of the research programme is to develop linking hypotheses between putative neurobiological primitives (e.g. temporal primitives) and those primitives derived from linguistic inquiry, to arrive ultimately at a biologically sensible and theoretically satisfying model of representation and computation in speech.},
  pmcid = {PMC2606797},
  pmid = {17890189},
  file = {/home/marnix/Zotero/storage/76CC3EMQ/Poeppel et al. - 2008 - Speech perception at the interface of neurobiology.pdf}
}

@article{Poledna2015,
  title = {The Multi-Layer Network Nature of Systemic Risk and Its Implications for the Costs of Financial Crises},
  author = {Poledna, Sebastian and {Molina-Borboa}, Jos{\'e} Luis and {Mart{\textbackslash}'{\textbackslash}inez-Jaramillo}, Seraf{\textbackslash}'{\textbackslash}in and {van der Leij}, Marco and Thurner, Stefan and {Mar{\'t}inez-Jaramillo}, Sera{\'f}in and {van der Leij}, Marco and Thurner, Stefan},
  year = {2015},
  journal = {Journal of Financial Stability},
  volume = {20},
  eprint = {1505.04276v1},
  pages = {70--81},
  publisher = {Elsevier},
  issn = {15723089},
  doi = {10.1016/j.jfs.2015.08.001},
  abstract = {The inability to see and quantify systemic financial risk comes at an immense social cost. Systemic risk in the financial system arises to a large extent as a consequence of the interconnectedness of its institutions, which are linked through networks of different types of financial contracts, such as credit, derivatives, foreign exchange, and securities. The interplay of the various exposure networks can be represented as layers in a financial multi-layer network. In this work we quantify the daily contributions to systemic risk from four layers of the Mexican banking system from 2007 to 2013. We show that focusing on a single layer underestimates the total systemic risk by up to 90\%. By assigning systemic risk levels to individual banks we study the systemic risk profile of the Mexican banking system on all market layers. This profile can be used to quantify systemic risk on a national level in terms of nation-wide expected systemic losses. We show that market-based systemic risk indicators systematically underestimate expected systemic losses. We find that expected systemic losses are up to a factor of four higher now than before the financial crisis of 2007-2008. We find that systemic risk contributions of individual transactions can be up to a factor of one thousand higher than the corresponding credit risk, which creates huge risks for the public. We find an intriguing non-linear effect whereby the sum of systemic risk of all layers underestimates the total risk. The method presented here is the first objective data-driven quantification of systemic risk on national scales that reveal its true levels.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1505.04276v1},
  isbn = {3120524565},
  pmid = {1048250},
  keywords = {Cascading failure,Financial regulation,Multiplex networks,Quantitative social science,Risk propagation,Systemic risk mitigation},
  file = {/home/marnix/Zotero/storage/DJBBU9M8/Poledna et al. - 2015 - The multi-layer network nature of systemic risk and its implications for the costs of financial crises.pdf}
}

@book{Porfiriev2012,
  title = {Crises in {{Russia}}: {{Contemporary Management Policy}} and {{Practice From A Historical Perspective}}},
  author = {Porfiriev, Boris},
  year = {2012},
  publisher = {Routledge},
  address = {New York, NY, USA}
}

@book{Press1992,
  title = {Numerical {{Recipes}} in {{C}} (2nd {{Ed}}.): {{The Art}} of {{Scientific Computing}}},
  author = {Press, William H and Teukolsky, Saul A and Vetterling, William T and Flannery, Brian P},
  year = {1992},
  publisher = {Cambridge University Press},
  address = {New York, NY, USA},
  isbn = {0-521-43108-5},
  file = {/home/marnix/Zotero/storage/BV9P4GUL/1992 Numerical Recipes in C.pdf}
}

@article{Press2009,
  title = {Strong Profiling Is Not Mathematically Optimal for Discovering Rare Malfeasors},
  author = {Press, William H.},
  year = {2009},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {106},
  number = {6},
  pages = {1716--1719},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0813202106},
  urldate = {2020-10-11},
  langid = {english},
  file = {/home/marnix/Zotero/storage/8PVXH8PG/Press - 2009 - Strong profiling is not mathematically optimal for.pdf}
}

@article{Price1989,
  title = {Male and Female Voice Source Characteristics: {{Inverse}} Filtering Results},
  shorttitle = {Male and Female Voice Source Characteristics},
  author = {Price, P. J},
  year = {1989},
  month = sep,
  journal = {Speech Communication},
  volume = {8},
  number = {3},
  pages = {261--277},
  issn = {0167-6393},
  doi = {10.1016/0167-6393(89)90005-8},
  urldate = {2020-01-27},
  abstract = {Detailed data on voicing source characteristics are of interest in both analysis and synthesis of speech. In this study, a time-domain based method of inverse filtering was used to analyze male and female utterances produced with several voice qualities. Fourteen mono-syllabic utterances (ten in normal voicing style, and 2 each in creaky and breathy voice) by each of 8 speakers (4 men, 4 women) were filtered by a set of zero pairs corresponding to the measured formants. The resulting differentiated flow waveforms were analyzed in the time domain and in the frequency domain at voicing onset, near the middle of the vowel, and near the end of voicing. Though there was variability among subjects of the same sex, the women tended to have shorter closed quotients and longer return quotients (the period between maximal rate of change of flow and minimal flow). In the spectral domain, there tended to be less energy at higher frequencies in the middle of the vowel for the women compared to the men. The magnitudes of the male-female differences are similar to those observed for the creaky-normal voicing differences and breathy-normal differences. These differences may arise from a combination of biological, sociological and acoustical effects. Zusammenfassung Ausf{\"u}hrliche Daten {\"u}ber die Kennzeichen der Stimmquelle sind von Interesse sowohl f{\"u}r die Sprachanalyse als f{\"u}r die Sprachsynthese. In dieser Studie wurde eine zeitliche Inversfiltermethode ben{\"u}tzt, um m{\"a}nnliche und weibliche S{\"a}tze mit verschiedenen Stimmqualit{\"a}ten zu analysieren. Vierzehn einsilbige Ausdr{\"u}cke (darin 10 mit normaler Stimme, 2 mit gebrochener, und 2 mit fl{\"u}sternder Stimme) von 8 Sprechern (4 M{\"a}nner und 4 Frauen) wurden durch eine Serie von Zero-paaren gefiltriert, die den gemessen Formanten korrespondierten. Die verschiedenen, differentialen Flutwellenformen die daraus entstanden wurden, im Zeit und Frequenzgebiet, am Stimmen Anfang, in der N{\"a}he der Vokalemitte und am Stimme-ende analysieren. Obwohl Schwankungen zwischen Sprecher vom gleichen Geschlecht festgestellt wurden, waren die weiblichen Stimmen auf k{\"u}rzere geschlossene Quotienten und l{\"a}ngere Perioden zwischen maximaler Erregung und minimalen Flu{$\beta$} gerichtet. Im spektralen Gebiet wurde bei den weiblichen Stimmen weniger Energie in den h{\"o}heren Frequenzen an der Vokalemitte gegen{\"u}ber den m{\"a}nnlichen Stimmen festgestellt. Die Amplituden der Unterschieden zwischen der m{\"a}nnlichen und weiblichen Stimmen sind gleich diesen die in der gebrochenen und fl{\"u}sternden Stimme beobachtet wurden. Die Schwankungen k{\"o}nnen eine Kombination von biologischen, soziologischen und akustischen Gr{\"u}nden haben. R{\'e}sum{\'e} Des donn{\'e}es pr{\'e}cises sur les caract{\'e}ristiques de la source glottique sont importantes pour analyser et synth{\'e}tiser la parole. On a utilis{\'e} ici une m{\'e}thode temporelle de filtrage inverse pour analyser les voix de 4 hommes et de 4 femmes. 14 mono-syllabes ont {\'e}t{\'e} prononc{\'e}es par chacun des locuteurs, (10 avec une voix normale, 2 avec une voix souffl{\'e}e, 2 avec une voix laryngalis{\'e}e) puis filtr{\'e}es par un ensemble de z{\'e}ros correspondant aux formants. On a effectu{\'e} des mesures temporelles et fr{\'e}quentielles de l'onde de d{\'e}bit diff{\'e}renci{\'e}e ainsi obtenue: pr{\`e}s du d{\'e}but du voisement, du centre de la voyelle et de la fin du voisement. Bien qu'on ait constat{\'e} de la variabilit{\'e} parmi les sujets du m{\^e}me sexe, les femmes ont des quotients de fermeture plus courts et des p{\'e}riodes plus longues entre le maximum de l'excitation et le minimum du d{\'e}bit. Dans le domaine spectral, on a trouv{\'e} moins d'{\'e}nergie pour les femmes que pour les hommes dans les hautes fr{\'e}quences pr{\`e}s du centre de la voyelle. Ces diff{\'e}rences hommes-femmes sont comparables {\`a} celle s observ{\'e}es pour les diff{\'e}rences entre voix laryngalis{\'e}es et normales et entre voix souffl{\'e}es et normales. Ces diff{\'e}rences qualitatives peuvent {\^e}tre li{\'e}es {\`a} une combinaison de facteurs biologiques, sociaux et acoustiques.},
  langid = {english},
  keywords = {Glottal source,inverse filtering,male-female differences},
  file = {/home/marnix/Zotero/storage/927GH853/0167639389900058.html}
}

@book{Proakis2001,
  title = {Digital Signal Processing: Principles Algorithms and Applications},
  author = {Proakis, John G},
  year = {2001},
  publisher = {Pearson Education India},
  file = {/home/marnix/Zotero/storage/U2XGT9EY/Proakis - 2001 - Digital signal processing principles algorithms a.pdf}
}

@phdthesis{Pulakka2005,
  title = {Analysis of {{Human Voice Production Using Inverse Filtering}}, {{High-Speed Imaging}}, and {{Electroglottography}}},
  author = {Pulakka, Hannu},
  year = {2005},
  abstract = {The evaluated inverse filtering method was found to produce mostly reasonable estimates of glottal flow. However, the parameters of the system have to be set appropriately, which requires experience on inverse filtering and speech production. The flow estimates often showed a two-stage opening phase with two instants of rapid increase in the flow derivative. The instant of glottal opening detected in the electroglottogram was often found to coincide with an increase in the flow derivative. The instant of minimum flow derivative was found to occur mostly during the last quarter of the closing phase and it was shown to precede the closing peak of the differentiated electroglottogram.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/M2NWW5P5/Pulakka - Analysis of Human Voice Production Using Inverse F.pdf}
}

@misc{Qiu2023,
  title = {Combating the "{{Sameness}}" in {{AI Art}}: {{Reflections}} on the {{Interactive AI Installation Fencing Hallucination}}},
  shorttitle = {Combating the "{{Sameness}}" in {{AI Art}}},
  author = {Qiu, Weihao and Legrady, George},
  year = {2023},
  month = nov,
  number = {arXiv:2311.17080},
  eprint = {2311.17080},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.17080},
  urldate = {2024-03-15},
  abstract = {The article summarizes three types of "sameness" issues in Artificial Intelligence(AI) art, each occurring at different stages of development in AI image creation tools. Through the Fencing Hallucination project, the article reflects on the design of AI art production in alleviating the sense of uniformity, maintaining the uniqueness of images from an AI image synthesizer, and enhancing the connection between the artworks and the audience. This paper endeavors to stimulate the creation of distinctive AI art by recounting the efforts and insights derived from the Fencing Hallucination project, all dedicated to addressing the issue of "sameness".},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/marnix/Zotero/storage/3FTLW6XN/Qiu and Legrady - 2023 - Combating the Sameness in AI Art Reflections on.pdf;/home/marnix/Zotero/storage/6DQ7DWD7/2311.html}
}

@article{Quinn2021,
  title = {Fast Algorithms for Fundamental Frequency Estimation in Autoregressive Noise},
  author = {Quinn, Barry Gerard and Nielsen, Jesper Kj{\ae}r and Christensen, Mads Gr{\ae}sb{\o}ll},
  year = {2021},
  month = mar,
  journal = {Signal Processing},
  volume = {180},
  pages = {107860},
  issn = {01651684},
  doi = {10.1016/j.sigpro.2020.107860},
  urldate = {2025-08-04},
  langid = {english},
  file = {/home/marnix/Zotero/storage/GY8Z233E/Quinn et al. - 2021 - Fast algorithms for fundamental frequency estimation in autoregressive noise.pdf}
}

@book{Rabiner1993,
  title = {Fundamentals of Speech Recognition},
  author = {Rabiner, Lawrence R and Juang, Biing-Hwang and Rutledge, Janet C},
  year = {1993},
  volume = {14},
  publisher = {PTR Prentice Hall Englewood Cliffs},
  file = {/home/marnix/Zotero/storage/LX6HCRXX/Rabiner et al. - 1993 - Fundamentals of speech recognition.pdf}
}

@book{Rabiner2007,
  title = {Introduction to Digital Speech Processing},
  booktitle = {Foundations and Trends in Signal Processing},
  author = {Rabiner, L R and Schafer, R W},
  year = {2007},
  volume = {1},
  eprint = {1011.1669v3},
  issn = {1932-8346},
  doi = {10.1561/2000000001},
  abstract = {Since even before the time of Alexander Graham Bell{\dbend}s revolution- ary invention, engineers and scientists have studied the phenomenon of speech communication with an eye on creating more e?cient and e?ective systems of human-to-human and human-to-machine communi- cation. Starting in the 1960s, digital signal processing (DSP), assumed a central role in speech studies, and today DSP is the key to realizing the fruits of the knowledge that has been gained through decades of research. Concomitant advances in integrated circuit technology and computer architecture have aligned to create a technological environ- ment with virtually limitless opportunities for innovation in speech communication applications. In this text, we highlight the central role of DSP techniques in modern speech communication research and appli- cations. We present a comprehensive overview of digital speech process- ing that ranges from the basic nature of the speech signal, through a variety of methods of representing speech in digital form, to applica- tions in voice communication and automatic synthesis and recognition of speech. The breadth of this subject does not allow us to discuss anyaspect of speech processing to great depth; hence our goal is to pro- vide a useful introduction to the wide range of important concepts that comprise the ?eld of digital speech processing. A more comprehensive treatment will appear in the forthcoming book, Theory and Application of Digital Speech Processing [101].},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1011.1669v3},
  isbn = {978-1-60198-070-0},
  pmid = {24439530},
  file = {/home/marnix/Zotero/storage/JC8CX4WN/Rabiner2007 Introduction to Digital Speech Processing.pdf}
}

@inproceedings{Rakitsch2013,
  title = {It Is All in the Noise: {{Efficient}} Multi-Task {{Gaussian}} Process Inference with Structured Residuals},
  shorttitle = {It Is All in the Noise},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rakitsch, Barbara and Lippert, Christoph and Borgwardt, Karsten and Stegle, Oliver},
  year = {2013},
  volume = {26},
  publisher = {Curran Associates, Inc.},
  urldate = {2022-10-18},
  abstract = {Multi-task prediction models are widely being used to couple regressors or classification models by sharing information across related tasks. A common pitfall of these models is that they assume that the output tasks are independent conditioned on the inputs. Here, we propose a multi-task Gaussian process approach to model both the relatedness between regressors as well as the task correlations in the residuals, in order to more accurately identify true sharing between regressors. The resulting Gaussian model has a covariance term that is the sum of Kronecker products, for which efficient parameter inference and out of sample prediction are feasible. On both synthetic examples and applications to phenotype prediction in genetics, we find substantial benefits of modeling structured noise compared to established alternatives.},
  file = {/home/marnix/Zotero/storage/4TYLA7BT/Rakitsch et al. - 2013 - It is all in the noise Efficient multi-task Gauss.pdf}
}

@article{Ramirez2018,
  title = {Hybrid {{Autoregressive Resonance Estimation}} and {{Density Mixture Formant Tracking Model}}},
  author = {Ram{\'i}rez, M. A.},
  year = {2018},
  journal = {IEEE Access},
  volume = {6},
  pages = {30217--30224},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2841802},
  abstract = {A novel formant tracker is proposed using the mixture models oft densities (tMMs) for vocal tract resonance frequencies estimated with a hybrid linear prediction (HLP) method. The hybrid integercycle pitch-synchronous linear prediction (LP) analysis improves the frequency resolution over voiced segments, leading to closer formant estimates than those provided by other LP methods. In conjunction with HLP, formant trajectories are shown to be more nearly tracked by tMMs than by Gaussian density models. Tests with synthetic voiced and whispered speech as well as with an annotated database confirm better performance than either tMM clustering after formant estimation based on different time-frequency representations or tracking after different LP methods.},
  keywords = {autoregressive models,autoregressive processes,Correlation,density mixture formant tracking model,Estimation,formant estimation,Formant estimation,formant tracking,frequency estimation,Frequency estimation,frequency resolution,Gaussian density models,Gaussian processes,HLP,hybrid autoregressive resonance estimation,hybrid integercycle pitch-synchronous linear prediction,hybrid linear prediction method,Probability density function,Resonant frequency,speech processing,t mixture models,time-frequency representations,tMM,Trajectory,Video recording,vocal tract resonance,vocal tract resonance frequencies,voiced segments,whispered speech},
  file = {/home/marnix/Zotero/storage/G2WR7ZEP/Ramírez - 2018 - Hybrid Autoregressive Resonance Estimation and Den.pdf;/home/marnix/Zotero/storage/UMGZSYMY/8368106.html}
}

@article{Ramus1999,
  title = {Correlates of Linguistic Rhythm in the Speech Signal},
  author = {Ramus, Franck and Nespor, Marina and Mehler, Jacques},
  year = {1999},
  journal = {Cognition},
  volume = {73},
  number = {3},
  pages = {265--292},
  issn = {00100277},
  doi = {10.1016/S0010-0277(99)00058-X},
  abstract = {Spoken languages have been classified by linguists according to their rhythmic properties, and psycholinguists have relied on this classification to account for infants' capacity to discriminate languages. Although researchers have measured many speech signal properties, they have failed to identify reliable acoustic characteristics for language classes. This paper presents instrumental measurements based on a consonant/vowel segmentation for eight languages. The measurements suggest that intuitive rhythm types reflect specific phonological properties, which in turn are signaled by the acoustic/phonetic properties of speech. The data support the notion of rhythm classes and also allow the simulation of infant language discrimination, consistent with the hypothesis that newborns rely on a coarse segmentation of speech. A hypothesis is proposed regarding the role of rhythm perception in language acquisition. Copyright (C) 1999 Elsevier Science B.V.},
  keywords = {Language acquisition,Language discrimination,Phonological bootstrapping,Prosody,Speech rhythm,Syllable structure},
  file = {/home/marnix/Zotero/storage/2HW8YWXK/Ramus1999 Correlates of linguistic rhythm in the speech signal.pdf}
}

@article{Rao2006,
  title = {Prosody Modification Using Instants of Significant Excitation},
  author = {Rao, K.S. and Yegnanarayana, B.},
  year = {2006},
  month = may,
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {14},
  number = {3},
  pages = {972--980},
  issn = {1558-7924},
  doi = {10.1109/TSA.2005.858051},
  abstract = {Prosody modification involves changing the pitch and duration of speech without affecting the message and naturalness. This paper proposes a method for prosody (pitch and duration) modification using the instants of significant excitation of the vocal tract system during the production of speech. The instants of significant excitation correspond to the instants of glottal closure (epochs) in the case of voiced speech, and to some random excitations like onset of burst in the case of nonvoiced speech. Instants of significant excitation are computed from the linear prediction (LP) residual of speech signals by using the property of average group-delay of minimum phase signals. The modification of pitch and duration is achieved by manipulating the LP residual with the help of the knowledge of the instants of significant excitation. The modified residual is used to excite the time-varying filter, whose parameters are derived from the original speech signal. Perceptual quality of the synthesized speech is good and is without any significant distortion. The proposed method is evaluated using waveforms, spectrograms, and listening tests. The performance of the method is compared with linear prediction pitch synchronous overlap and add (LP-PSOLA) method, which is another method for prosody manipulation based on the modification of the LP residual. The original and the synthesized speech signals obtained by the proposed method and by the LP-PSOLA method are available for listening at http://speech.cs.iitm.ernet.in/Main/result/prosody.html.},
  keywords = {Bandwidth,Duration,excitation source,Filters,Frequency estimation,instants of significant excitation (epochs),linear prediction pitch synchronous overlap and add (LP-PSOLA),LP residual,pitch period,Production systems,prosody modification,Shape,Signal synthesis,Spectrogram,Speech synthesis,Testing},
  file = {/home/marnix/Zotero/storage/QF7873U8/Rao and Yegnanarayana - 2006 - Prosody modification using instants of significant.pdf;/home/marnix/Zotero/storage/UGZAN9R2/stamp.html}
}

@article{Rao2018,
  title = {Glottal Inverse Filtering Using Probabilistic Weighted Linear Prediction},
  author = {Rao, Achuth and Ghosh, Prasanta Kumar},
  year = {2018},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {27},
  number = {1},
  pages = {114--124},
  publisher = {IEEE},
  file = {/home/marnix/Zotero/storage/ZA2BYX6X/Rao and Ghosh - 2018 - Glottal inverse filtering using probabilistic weig.pdf}
}

@article{Rasilo2011,
  title = {Method for Speech Inversion with Large Scale Statistical Evaluation},
  author = {Rasilo, Heikki and Laine, Unto K. and R{\"a}s{\"a}nen, Okko and Altosaar, Toomas},
  year = {2011},
  journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
  number = {August},
  pages = {2693--2696},
  issn = {19909772},
  abstract = {An articulatory model of speech production is created for the purpose of studying the links between speech production and perception. A computationally effective method for speech inversion in proposed, using a two-pole predictor structure in order to maintain better articulatory dynamics when compared to conventional dynamic programming methods. Preliminary tests for the effect of inversion are performed for 2500 Finnish syllables extracted from continuous speech, consisting of 125 different syllable classes. A cluster selectivity test shows that the syllables are more reliably clustered using the automatically obtained parametric representation of articulatory gestures rather than the original formant representation that is used as a starting point for the inversion. Copyright {\copyright} 2011 ISCA.},
  isbn = {19909772 (ISSN)},
  keywords = {Articulatory model,Motor theory,Speech inversion,Vocal tract},
  file = {/home/marnix/Zotero/storage/W9G8YMQ7/Rasilo2011 Method for Speech Inversion with Large Scale Statistical Evaluation.pdf}
}

@book{Rasmussen2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  series = {Adaptive Computation and Machine Learning},
  publisher = {MIT Press},
  address = {Cambridge, Mass},
  isbn = {978-0-262-18253-9},
  langid = {english},
  lccn = {QA274.4 .R37 2006},
  keywords = {Data processing,Gaussian processes,Machine learning,Mathematical models},
  annotation = {OCLC: ocm61285753},
  file = {/home/marnix/Zotero/storage/FALYZKTQ/Rasmussen and Williams - 2006 - Gaussian processes for machine learning.pdf}
}

@article{Ray1991,
  title = {An {{Approach}} to the {{Synthesis}} of {{Life}}},
  author = {Ray, Thomas S.},
  year = {1991},
  volume = {X},
  number = {c},
  pages = {2001--2003},
  file = {/home/marnix/Zotero/storage/B3YCLM2J/Ray1991 An Approach To The Synthesis Of Life.pdf}
}

@book{RCoreTeam2016,
  title = {R: {{A Language}} and {{Environment}} for {{Statistical Computing}}},
  author = {{R Core Team}},
  year = {2016},
  address = {Vienna, Austria},
  organization = {R Foundation for Statistical Computing}
}

@inproceedings{Reece2010,
  title = {An Introduction to {{Gaussian}} Processes for the {{Kalman}} Filter Expert},
  booktitle = {2010 13th International Conference on Information Fusion},
  author = {Reece, Steven and Roberts, Stephen},
  year = {2010},
  pages = {1--9},
  organization = {IEEE},
  file = {/home/marnix/Zotero/storage/YUSG87RH/Reece and Roberts - 2010 - An introduction to Gaussian processes for the Kalm.pdf}
}

@incollection{Remes2017,
  title = {Non-{{Stationary Spectral Kernels}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Remes, Sami and Heinonen, Markus and Kaski, Samuel},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {4642--4651},
  publisher = {Curran Associates, Inc.},
  urldate = {2020-05-01},
  file = {/home/marnix/Zotero/storage/92XYYLXH/Remes et al. - 2017 - Non-Stationary Spectral Kernels.pdf;/home/marnix/Zotero/storage/XVMNAPLQ/7050-non-stationary-spectral-kernels.html}
}

@misc{Ren2024,
  title = {{{TimeChat}}: {{A Time-sensitive Multimodal Large Language Model}} for {{Long Video Understanding}}},
  shorttitle = {{{TimeChat}}},
  author = {Ren, Shuhuai and Yao, Linli and Li, Shicheng and Sun, Xu and Hou, Lu},
  year = {2024},
  month = mar,
  number = {arXiv:2312.02051},
  eprint = {2312.02051},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.02051},
  urldate = {2024-04-19},
  abstract = {This work proposes TimeChat, a time-sensitive multimodal large language model specifically designed for long video understanding. Our model incorporates two key architectural contributions: (1) a timestamp-aware frame encoder that binds visual content with the timestamp of each frame, and (2) a sliding video Q-Former that produces a video token sequence of varying lengths to accommodate videos of various durations. Additionally, we construct an instruction-tuning dataset, encompassing 6 tasks and a total of 125K instances, to further enhance TimeChat's instruction-following performance. Experiment results across various video understanding tasks, such as dense captioning, temporal grounding, and highlight detection, demonstrate TimeChat's strong zero-shot temporal localization and reasoning capabilities. For example, it achieves +9.2 F1 score and +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5) on Charades-STA, compared to state-of-the-art video large language models, holding the potential to serve as a versatile video assistant for long-form video comprehension tasks and satisfy realistic user requirements.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/marnix/Zotero/storage/AEX7VEN3/Ren et al. - 2024 - TimeChat A Time-sensitive Multimodal Large Langua.pdf;/home/marnix/Zotero/storage/67NF2B6X/2312.html}
}

@misc{Requeima2019,
  title = {The {{Gaussian Process Autoregressive Regression Model}} ({{GPAR}})},
  author = {Requeima, James and Tebbutt, Will and Bruinsma, Wessel and Turner, Richard E.},
  year = {2019},
  month = feb,
  number = {arXiv:1802.07182},
  eprint = {1802.07182},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.07182},
  urldate = {2025-09-16},
  abstract = {Multi-output regression models must exploit dependencies between outputs to maximise predictive performance. The application of Gaussian processes (GPs) to this setting typically yields models that are computationally demanding and have limited representational power. We present the Gaussian Process Autoregressive Regression (GPAR) model, a scalable multi-output GP model that is able to capture nonlinear, possibly input-varying, dependencies between outputs in a simple and tractable way: the product rule is used to decompose the joint distribution over the outputs into a set of conditionals, each of which is modelled by a standard GP. GPAR's efficacy is demonstrated on a variety of synthetic and real-world problems, outperforming existing GP models and achieving state-of-the-art performance on established benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/ZSRWFZDC/Requeima et al. - 2019 - The Gaussian Process Autoregressive Regression Model (GPAR).pdf;/home/marnix/Zotero/storage/4PGED6WT/1802.html}
}

@article{Rice1944,
  title = {Mathematical {{Analysis}} of {{Random Noise}}},
  author = {Rice, S. O.},
  year = {1944},
  journal = {Bell System Technical Journal},
  volume = {23},
  number = {3},
  pages = {282--332},
  issn = {1538-7305},
  doi = {10.1002/j.1538-7305.1944.tb00874.x},
  urldate = {2019-03-18},
  copyright = {{\copyright} 1944 The Bell System Technical Journal},
  langid = {english},
  file = {/home/marnix/Zotero/storage/RB88K34J/Rice - 1944 - Mathematical Analysis of Random Noise.pdf;/home/marnix/Zotero/storage/UN4CC7XA/j.1538-7305.1944.tb00874.html}
}

@article{Richard2007,
  title = {Efficient High-Dimensional Importance Sampling},
  author = {Richard, Jean-Francois and Zhang, Wei},
  year = {2007},
  journal = {Journal of Econometrics},
  volume = {141},
  number = {2},
  pages = {1385--1411},
  publisher = {Elsevier}
}

@article{Richie2014,
  title = {Modeling the {{Emergence}} of {{Lexicons}} in {{Homesign Systems}}},
  author = {Richie, Russell and Yang, Charles and Coppola, Marie},
  year = {2014},
  journal = {Topics in Cognitive Science},
  volume = {6},
  number = {1},
  pages = {183--195},
  issn = {1756-8765},
  doi = {10.1111/tops.12076},
  urldate = {2020-12-22},
  abstract = {It is largely acknowledged that natural languages emerge not just from human brains but also from rich communities of interacting human brains (Senghas, 2005). Yet the precise role of such communities and such interaction in the emergence of core properties of language has largely gone uninvestigated in naturally emerging systems, leaving the few existing computational investigations of this issue at an artificial setting. Here, we take a step toward investigating the precise role of community structure in the emergence of linguistic conventions with both naturalistic empirical data and computational modeling. We first show conventionalization of lexicons in two different classes of naturally emerging signed systems: (a) protolinguistic ``homesigns'' invented by linguistically isolated Deaf individuals, and (b) a natural sign language emerging in a recently formed rich Deaf community. We find that the latter conventionalized faster than the former. Second, we model conventionalization as a population of interacting individuals who adjust their probability of sign use in response to other individuals' actual sign use, following an independently motivated model of language learning (Yang, 2002, 2004). Simulations suggest that a richer social network, like that of natural (signed) languages, conventionalizes faster than a sparser social network, like that of homesign systems. We discuss our behavioral and computational results in light of other work on language emergence, and other work of behavior on complex networks.},
  copyright = {Copyright {\copyright} 2013 Cognitive Science Society, Inc.},
  langid = {english},
  keywords = {Agent-based modeling,Conventionalization,Homesign,Language emergence,Lexicon,Sign language,Social networks},
  file = {/home/marnix/Zotero/storage/NVZQCPRQ/Richie et al. - 2014 - Modeling the Emergence of Lexicons in Homesign Sys.pdf;/home/marnix/Zotero/storage/7S2G3EWG/tops.html}
}

@article{Richmond2006,
  title = {A Trajectory Mixture Density Network for the Acoustic-Articulatory Inversion Mapping},
  author = {Richmond, Korin},
  year = {2006},
  journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
  volume = {2},
  pages = {577--580},
  issn = {19909772},
  doi = {10.1007/978-3-540-77347-4_23},
  abstract = {This paper proposes a trajectory model which is based on a mixture{\textbackslash}ndensity network trained with target features augmented with dynamic{\textbackslash}nfeatures together with an algorithm for estimating maximum likelihood{\textbackslash}ntrajectories which respects constraints between the static and derived{\textbackslash}ndynamic features. This model was evaluated on an inversion mapping task.{\textbackslash}nWe found the introduction of the trajectory model successfully reduced{\textbackslash}nroot mean square error by up to 7.5\%, as well as increasing correlation{\textbackslash}nscores.},
  isbn = {9781604234497},
  keywords = {Acoustic-articulatory inversion,Conditional trajectory model,Mixture density network},
  file = {/home/marnix/Zotero/storage/9H8PLU5M/Richmond2006 A trajectory mixture density network.pdf}
}

@article{Richmond2009,
  title = {Preliminary Inversion Mapping Results with a New {{EMA}} Corpus},
  author = {Richmond, Korin},
  year = {2009},
  journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
  pages = {2835--2838},
  issn = {19909772},
  abstract = {In this paper, we apply our inversion mapping method, the trajectory mixture density network (TMDN), to a new corpus of articulatory data, recorded with a Carstens AG500 electromagnetic articulograph. This new data set, mngu0, is relatively large and phonetically rich, among other beneficial characteristics. We obtain good results, with a root mean square (RMS) error of only 0.99mm. This compares very well with our previous lowest result of 1.54mm RMS error for equivalent coils of the MOCHA fsew0 EMA data. We interpret this as showing the mngu0 data set is potentially more consistent than the fsew0 data set, and is very useful for research which calls for articulatory trajectory data. It also supports our view that the TMDN is very much suited to the inversion mapping problem.},
  keywords = {Acoustic-articulatory,Inversion mapping,Neural network},
  file = {/home/marnix/Zotero/storage/6TETL2AH/Richmond2009 Preliminary Inversion Mapping Results with a New EMA Corpus.pdf}
}

@article{Richters2011,
  title = {Trust Transitivity in Social Networks},
  author = {Richters, Oliver and Peixoto, Tiago P.},
  year = {2011},
  journal = {PLoS ONE},
  volume = {6},
  number = {4},
  eprint = {1012.1358},
  issn = {19326203},
  doi = {10.1371/journal.pone.0018384},
  abstract = {Non-centralized recommendation-based decision making is a central feature of{\textbackslash}nseveral social and technological processes, such as market dynamics,{\textbackslash}npeer-to-peer file-sharing and the web of trust of digital certification. We{\textbackslash}ninvestigate the properties of trust propagation on networks, based on a simple{\textbackslash}nmetric of trust transitivity. We investigate analytically the percolation{\textbackslash}nproperties of trust transitivity in random networks with arbitrary in/out-degree{\textbackslash}ndistributions, and compare with numerical realizations. We find that the{\textbackslash}nexistence of a non-zero fraction of absolute trust (i.e.{\textbackslash}nentirely confident trust) is a requirement for the viability of global trust{\textbackslash}npropagation in large systems: The average pair-wise trust is marked by a{\textbackslash}ndiscontinuous transition at a specific fraction of absolute trust, below which{\textbackslash}nit vanishes. Furthermore, we perform an extensive analysis of the Pretty Good{\textbackslash}nPrivacy (PGP) web of trust, in view of the concepts introduced. We compare{\textbackslash}ndifferent scenarios of trust distribution: community- and authority-centered. We{\textbackslash}nfind that these scenarios lead to sharply different patterns of trust{\textbackslash}npropagation, due to the segregation of authority hubs and densely-connected{\textbackslash}ncommunities. While the authority-centered scenario is more efficient, and leads{\textbackslash}nto higher average trust values, it favours weakly-connected ``fringe''{\textbackslash}nnodes, which are directly trusted by authorities. The community-centered scheme,{\textbackslash}non the other hand, favours nodes with intermediate in/out-degrees, in detriment{\textbackslash}nof the authorities and its ``fringe'' peers.},
  archiveprefix = {arXiv},
  arxivid = {1012.1358},
  isbn = {9781577355090},
  pmid = {21483683},
  file = {/home/marnix/Zotero/storage/KEYQI4V6/Richters, Peixoto - 2011 - Trust transitivity in social networks.pdf}
}

@article{Ridder2016,
  title = {Detection and Localization of Change Points in Temporal Networks with the Aid of Stochastic Block Models},
  author = {Ridder, Simon De and Vandermarliere, Benjamin and Ryckebusch, Jan},
  year = {2016},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2016},
  number = {11},
  pages = {113302},
  abstract = {A framework based on generalized hierarchical random graphs (GHRGs) for the detection of change points in the structure of temporal networks has recently been developed by Peel and Clauset (2015 Proc. 29th AAAI Conf. on Artificial Intelligence ). We build on this methodology and extend it to also include the versatile stochastic block models (SBMs) as a parametric family for reconstructing the empirical networks. We use five different techniques for change point detection on prototypical temporal networks, including empirical and synthetic ones. We find that none of the considered methods can consistently outperform the others when it comes to detecting and locating the expected change points in empirical temporal networks. With respect to the precision and the recall of the results of the change points, we find that the method based on a degree-corrected SBM has better recall properties than other dedicated methods, especially for sparse networks and smaller sliding time window widths.},
  file = {/home/marnix/Zotero/storage/V4L357S8/Ridder, Vandermarliere, Ryckebusch - 2016 - Detection and localization of change points in temporal networks with the aid of stochastic.pdf}
}

@book{Riley2002,
  title = {Mathematical {{Methods}} for {{Physics}} and {{Engineering}}: {{A Comprehensive Guide}}},
  shorttitle = {Mathematical {{Methods}} for {{Physics}} and {{Engineering}}},
  author = {Riley, K. F. and Hobson, M. P. and Bence, S. J.},
  year = {2002},
  edition = {2},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9781139164979},
  urldate = {2021-06-02},
  abstract = {The new edition of this highly acclaimed textbook contains several major additions, including more than four hundred new exercises (with hints and answers). To match the mathematical preparation of current senior college and university entrants, the authors have included a preliminary chapter covering areas such as polynomial equations, trigonometric identities, coordinate geometry, partial fractions, binomial expansions, induction, and the proof of necessary and sufficient conditions. Elsewhere, matrix decompositions, nearly-singular matrices and non-square sets of linear equations are treated in detail. The presentation of probability has been reorganised and greatly extended, and includes all physically important distributions. New topics covered in a separate statistics chapter include estimator efficiency, distributions of samples, t- and F-tests for comparing means and variances, applications of the chi-squared distribution, and maximum likelihood and least-squares fitting. In other chapters the following topics have been added: linear recurrence relations, curvature, envelopes, curve-sketching, and more refined numerical methods.},
  file = {/home/marnix/Zotero/storage/X6CUIHGT/Riley et al. - 2002 - Mathematical Methods for Physics and Engineering .pdf;/home/marnix/Zotero/storage/HM4D6KDY/911A43AE1CF224743D32707FCC4AE0EB.html}
}

@misc{Rios2015,
  title = {A {{Prior Distribution}} over {{Directed Acyclic Graphs}} for {{Sparse Bayesian Networks}}},
  author = {Rios, Felix L. and Noble, John M. and Koski, Timo J. T.},
  year = {2015},
  month = apr,
  number = {arXiv:1504.06701},
  eprint = {1504.06701},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1504.06701},
  urldate = {2022-09-16},
  abstract = {The main contribution of this article is a new prior distribution over directed acyclic graphs, which gives larger weight to sparse graphs. This distribution is intended for structured Bayesian networks, where the structure is given by an ordered block model. That is, the nodes of the graph are objects which fall into categories (or blocks); the blocks have a natural ordering. The presence of a relationship between two objects is denoted by an arrow, from the object of lower category to the object of higher category. The models considered here were introduced in Kemp et al. (2004) for relational data and extended to multivariate data in Mansinghka et al. (2006). The prior over graph structures presented here has an explicit formula. The number of nodes in each layer of the graph follow a Hoppe Ewens urn model. We consider the situation where the nodes of the graph represent random variables, whose joint probability distribution factorises along the DAG. We describe Monte Carlo schemes for finding the optimal aposteriori structure given a data matrix and compare the performance with Mansinghka et al. (2006) and also with the uniform prior.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/CSH3XI2P/Rios et al. - 2015 - A Prior Distribution over Directed Acyclic Graphs .pdf}
}

@article{Rissanen1978,
  title = {Modeling by Shortest Data Description},
  author = {Rissanen, J.},
  year = {1978},
  month = sep,
  journal = {Automatica},
  volume = {14},
  number = {5},
  pages = {465--471},
  issn = {00051098},
  doi = {10.1016/0005-1098(78)90005-5},
  urldate = {2019-05-06},
  abstract = {The number of digits it takes to write down an observed sequence xl,...,x N of a time series depends on the model with its parameters that one assumes to have generated the observed data. Accordingly, by finding the model which minimizes the description length one obtains estimates of both the integer-valued structure parameters and the realvalued system parameters.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/YVCNPHPQ/Rissanen - 1978 - Modeling by shortest data description.pdf}
}

@article{Rissanen1983,
  title = {A {{Universal Prior}} for {{Integers}} and {{Estimation}} by {{Minimum Description Length}}},
  author = {Rissanen, Jorma},
  year = {1983},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {11},
  number = {2},
  pages = {416--431},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176346150},
  urldate = {2019-02-07},
  abstract = {An earlier introduced estimation principle, which calls for minimization of the number of bits required to write down the observed data, has been reformulated to extend the classical maximum likelihood principle. The principle permits estimation of the number of the parameters in statistical models in addition to their values and even of the way the parameters appear in the models; i.e., of the model structures. The principle rests on a new way to interpret and construct a universal prior distribution for the integers, which makes sense even when the parameter is an individual object. Truncated real-valued parameters are converted to integers by dividing them by their precision, and their prior is determined from the universal prior for the integers by optimizing the precision.},
  langid = {english},
  mrnumber = {MR696056},
  zmnumber = {0513.62005},
  keywords = {Likelihood,modeling,parameters},
  file = {/home/marnix/Zotero/storage/QRANAEUS/Rissanen - 1983 - A Universal Prior for Integers and Estimation by M.pdf;/home/marnix/Zotero/storage/WTYK9EBG/1176346150.html}
}

@article{Riutort-Mayol2020,
  title = {Practical {{Hilbert}} Space Approximate {{Bayesian Gaussian}} Processes for Probabilistic Programming},
  author = {{Riutort-Mayol}, Gabriel and B{\"u}rkner, Paul-Christian and Andersen, Michael R. and Solin, Arno and Vehtari, Aki},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.11408 [stat]},
  eprint = {2004.11408},
  primaryclass = {stat},
  urldate = {2022-01-18},
  abstract = {Gaussian processes are powerful non-parametric probabilistic models for stochastic functions. However they entail a complexity that is computationally intractable when the number of observations is large, especially when estimated with fully Bayesian methods such as Markov chain Monte Carlo. In this paper, we focus on a novel approach for low-rank approximate Bayesian Gaussian processes, based on a basis function approximation via Laplace eigenfunctions for stationary covariance functions. The main contribution of this paper is a detailed analysis of the performance and practical implementation of the method in relation to key factors such as the number of basis functions, domain of the prediction space, and smoothness of the latent function. We provide intuitive visualizations and recommendations for choosing the values of these factors, which make it easier for users to improve approximation accuracy and computational performance. We also propose diagnostics for checking that the number of basis functions and the domain of the prediction space are adequate given the data. The proposed approach is simple and exhibits an attractive computational complexity due to its linear structure, and it is easy to implement in probabilistic programming frameworks. Several illustrative examples of the performance and applicability of the method in the probabilistic programming language Stan are presented together with the underlying Stan model code.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/home/marnix/Zotero/storage/9NDRM225/Riutort-Mayol et al. - 2020 - Practical Hilbert space approximate Bayesian Gauss.pdf;/home/marnix/Zotero/storage/NKHYNBGU/2004.html}
}

@misc{Rizvi2017,
  title = {A {{Novel Approach}} to {{Forecasting Financial Volatility}} with {{Gaussian Process Envelopes}}},
  author = {Rizvi, Syed Ali Asad and Roberts, Stephen J. and Osborne, Michael A. and Nyikosa, Favour},
  year = {2017},
  month = may,
  number = {arXiv:1705.00891},
  eprint = {1705.00891},
  primaryclass = {cs, q-fin, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1705.00891},
  urldate = {2022-11-12},
  abstract = {In this paper we use Gaussian Process (GP) regression to propose a novel approach for predicting volatility of financial returns by forecasting the envelopes of the time series. We provide a direct comparison of their performance to traditional approaches such as GARCH. We compare the forecasting power of three approaches: GP regression on the absolute and squared returns; regression on the envelope of the returns and the absolute returns; and regression on the envelope of the negative and positive returns separately. We use a maximum a posteriori estimate with a Gaussian prior to determine our hyperparameters. We also test the effect of hyperparameter updating at each forecasting step. We use our approaches to forecast out-of-sample volatility of four currency pairs over a 2 year period, at half-hourly intervals. From three kernels, we select the kernel giving the best performance for our data. We use two published accuracy measures and four statistical loss functions to evaluate the forecasting ability of GARCH vs GPs. In mean squared error the GP's perform 20\% better than a random walk model, and 50\% better than GARCH for the same data.},
  archiveprefix = {arXiv},
  keywords = {{Computer Science - Computational Engineering, Finance, and Science},Quantitative Finance - Statistical Finance,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/CWZWB3XS/Rizvi et al. - 2017 - A Novel Approach to Forecasting Financial Volatili.pdf;/home/marnix/Zotero/storage/4634W72C/1705.html}
}

@article{Roberts2002,
  title = {Variational {{Bayes}} for Generalized Autoregressive Models},
  author = {Roberts, S.J. and Penny, W.D.},
  year = {2002},
  month = sep,
  journal = {IEEE Transactions on Signal Processing},
  volume = {50},
  number = {9},
  pages = {2245--2257},
  issn = {1053-587X},
  doi = {10.1109/TSP.2002.801921},
  urldate = {2025-04-26},
  abstract = {We describe a variational Bayes (VB) learning algorithm for generalized autoregressive (GAR) models. The noise is modeled as a mixture of Gaussians rather than the usual single Gaussian. This allows different data points to be associated with different noise levels and effectively provides robust estimation of AR coefficients. The VB framework is used to prevent overfitting and provides model-order selection criteria both for AR order and noise model order. We show that for the special case of Gaussian noise and uninformative priors on the noise and weight precisions, the VB framework reduces to the Bayesian evidence framework. The algorithm is applied to synthetic and real data with encouraging results.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/marnix/Zotero/storage/2LRJWF6P/Roberts and Penny - 2002 - Variational Bayes for generalized autoregressive models.pdf}
}

@article{Roberts2013,
  title = {Gaussian Processes for Time-Series Modelling},
  author = {Roberts, Stephen and Osborne, Michael and Ebden, Mark and Reece, Steven and Gibson, Neale and Aigrain, Suzanne},
  year = {2013},
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {371},
  number = {1984},
  pages = {20110550},
  publisher = {The Royal Society Publishing},
  file = {/home/marnix/Zotero/storage/4HCQIMZZ/Roberts et al. - 2012 - Gaussian processes for time-series modelling.pdf}
}

@article{Rodriguez2002,
  title = {Entropic Priors for Discrete Probabilistic Networks and for Mixtures of {{Gaussians}} Models},
  author = {Rodriguez, C. C.},
  year = {2002},
  journal = {AIP Conference Proceedings},
  volume = {617},
  eprintclass = {physics},
  pages = {410--432},
  issn = {0094243X},
  doi = {10.1063/1.1477063},
  abstract = {The ongoing unprecedented exponential explosion of available computing power, has radically transformed the methods of statistical inference. What used to be a small minority of statisticians advocating for the use of priors and a strict adherence to bayes theorem, it is now becoming the norm across disciplines. The evolutionary direction is now clear. The trend is towards more realistic, flexible and complex likelihoods characterized by an ever increasing number of parameters. This makes the old question of: What should the prior be? to acquire a new central importance in the modern bayesian theory of inference. Entropic priors provide one answer to the problem of prior selection. The general definition of an entropic prior has existed since 1988, but it was not until 1998 that it was found that they provide a new notion of complete ignorance. This paper re-introduces the family of entropic priors as minimizers of mutual information between the data and the parameters, as in [rodriguez98b], but with a small change and a correction. The general formalism is then applied to two large classes of models: Discrete probabilistic networks and univariate finite mixtures of gaussians. It is also shown how to perform inference by efficiently sampling the corresponding posterior distributions.},
  arxiv = {0201016},
  arxivid = {physics/0201016},
  keywords = {approximation to gig,bayesian belief networks,chain monte carlo,entropic priors,gamma,generalized inverse gaussian distribution,markov,mcmc,mixture models},
  file = {/home/marnix/Zotero/storage/6W9QHL3I/Rodriguez2002 entropic priors.pdf}
}

@article{Rombach2012,
  title = {Core-{{Periphery Structure}} in {{Networks}}},
  author = {Rombach, M. Puck and Porter, Mason A. and Fowler, James H. and Mucha, Peter J.},
  year = {2012},
  eprint = {1202.2684},
  pages = {1--27},
  issn = {0036-1399},
  doi = {10.1137/120881683},
  abstract = {Intermediate-scale (or `meso-scale') structures in networks have received considerable attention, as the algorithmic detection of such structures makes it possible to discover network features that are not apparent either at the local scale of nodes and edges or at the global scale of summary statistics. Numerous types of meso-scale structures can occur in networks, but investigations of such features have focused predominantly on the identification and study of community structure. In this paper, we develop a new method to investigate the meso-scale feature known as core-periphery structure, which entails identifying densely-connected core nodes and sparsely-connected periphery nodes. In contrast to communities, the nodes in a core are also reasonably well-connected to those in the periphery. Our new method of computing core-periphery structure can identify multiple cores in a network and takes different possible cores into account. We illustrate the differences between our method and several existing methods for identifying which nodes belong to a core, and we use our technique to examine core-periphery structure in examples of friendship, collaboration, transportation, and voting networks.},
  archiveprefix = {arXiv},
  arxivid = {1202.2684},
  isbn = {0036-1399},
  pmid = {1000314943},
  keywords = {core-periphery,networks,topology,weighted\{{\textbackslash}\_\}networks},
  file = {/home/marnix/Zotero/storage/LZT6D9ZH/Rombach et al. - 2012 - Core-Periphery Structure in Networks.pdf}
}

@book{Rose2002,
  title = {Forensic Speaker Identification},
  author = {Rose, Phil},
  year = {2002},
  publisher = {cRc Press},
  file = {/home/marnix/Zotero/storage/DEQNMNJI/Rose - 2002 - Forensic speaker identification.pdf}
}

@article{Ross2021,
  title = {Learning {{Nonparametric Volterra Kernels}} with {{Gaussian Processes}}},
  author = {Ross, Magnus and Smith, Michael T. and {\'A}lvarez, Mauricio A.},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.05582 [cs, stat]},
  eprint = {2106.05582},
  primaryclass = {cs, stat},
  urldate = {2021-11-26},
  abstract = {This paper introduces a method for the nonparametric Bayesian learning of nonlinear operators, through the use of the Volterra series with kernels represented using Gaussian processes (GPs), which we term the nonparametric Volterra kernels model (NVKM). When the input function to the operator is unobserved and has a GP prior, the NVKM constitutes a powerful method for both single and multiple output regression, and can be viewed as a nonlinear and nonparametric latent force model. When the input function is observed, the NVKM can be used to perform Bayesian system identification. We use recent advances in efficient sampling of explicit functions from GPs to map process realisations through the Volterra series without resorting to numerical integration, allowing scalability through doubly stochastic variational inference, and avoiding the need for Gaussian approximations of the output processes. We demonstrate the performance of the model for both multiple output regression and system identification using standard benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/G2TRH3RZ/Ross et al. - 2021 - Learning Nonparametric Volterra Kernels with Gauss.pdf}
}

@article{Rossa2013,
  title = {Profiling Core-Periphery Network Structure by Random Walkers},
  author = {Rossa, Fabio D and Dercole, Fabio and Piccardi, Carlo},
  year = {2013},
  month = mar,
  journal = {Scientific Reports},
  volume = {3},
  issn = {2045-2322},
  doi = {10.1038/srep01467},
  abstract = {Disclosing the main features of the structure of a network is crucial to understand a number of static and dynamic properties, such as robustness to failures, spreading dynamics, or collective behaviours. Among the possible characterizations, the core-periphery paradigm models the network as the union of a dense core with a sparsely connected periphery, highlighting the role of each node on the basis of its topological position. Here we show that the core-periphery structure can effectively be profiled by elaborating the behaviour of a random walker. A curve---the core-periphery profile---and a numerical indicator are derived, providing a global topological portrait. Simultaneously, a coreness value is attributed to each node, qualifying its position and role. The application to social, technological, economical, and biological networks reveals the power of this technique in disclosing the overall network structure and the peculiar role of some specific nodes.},
  keywords = {complex\{{\textbackslash}\_\}networks,core\{{\textbackslash}\_\}periphery,network\{{\textbackslash}\_\}structure,random\{{\textbackslash}\_\}walk},
  file = {/home/marnix/Zotero/storage/3PWJWK4N/Rossa, Dercole, Piccardi - 2013 - Profiling core-periphery network structure by random walkers.pdf}
}

@article{Rousseau2012,
  title = {{{TED-LIUM}}: An {{Automatic Speech Recognition}} Dedicated Corpus},
  author = {Rousseau, Anthony and Del{\'e}glise, Paul and Est{\`e}ve, Yannick},
  year = {2012},
  journal = {Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012)},
  pages = {125--129},
  abstract = {This paper presents the corpus developed by the LIUM for Automatic Speech Recognition (ASR), based on the TED Talks. This corpus was built during the IWSLT 2011 Evaluation Campaign, and is composed of 118 hours of speech with its accompanying automatically aligned transcripts. We describe the content of the corpus, how the data was collected and processed, how it will be publicly available and how we built an ASR system using this data leading to a WER score of 17.4\%. The official results we obtained at the IWSLT 2011 evaluation campaign are also discussed.},
  isbn = {978-2-9517408-7-7},
  keywords = {data collection,speech recognition,unsupervised learning}
}

@article{Rover2011,
  title = {Modelling Coloured Residual Noise in Gravitational-Wave Signal Processing},
  author = {R{\"o}ver, Christian and Meyer, Renate and Christensen, Nelson},
  year = {2011},
  month = jan,
  journal = {Classical and Quantum Gravity},
  volume = {28},
  number = {1},
  eprint = {0804.3853},
  pages = {015010},
  issn = {0264-9381, 1361-6382},
  doi = {10.1088/0264-9381/28/1/015010},
  urldate = {2019-03-27},
  abstract = {We introduce a signal processing model for signals in non-white noise, where the exact noise spectrum is a priori unknown. The model is based on a Student's t distribution and constitutes a natural generalization of the widely used normal (Gaussian) model. This way, it allows for uncertainty in the noise spectrum, or more generally is also able to accommodate outliers (heavy-tailed noise) in the data. Examples are given pertaining to data from gravitational wave detectors.},
  archiveprefix = {arXiv},
  keywords = {{Physics - Data Analysis, Statistics and Probability},General Relativity and Quantum Cosmology,Statistics - Methodology},
  file = {/home/marnix/Zotero/storage/HZCD55R4/Röver et al. - 2011 - Modelling coloured residual noise in gravitational.pdf;/home/marnix/Zotero/storage/WZRPIUFD/0804.html}
}

@article{Roweis1999,
  title = {A {{Unifying Review}} of {{Linear Gaussian Models}}},
  author = {Roweis, Sam and Ghahramani, Zoubin},
  year = {1999},
  month = feb,
  journal = {Neural Computation},
  volume = {11},
  number = {2},
  pages = {305--345},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976699300016674},
  urldate = {2020-07-01},
  langid = {english},
  file = {/home/marnix/Zotero/storage/A54GXDJS/Roweis and Ghahramani - 1999 - A Unifying Review of Linear Gaussian Models.pdf}
}

@article{Roweis2005,
  title = {Automatic Speech Processing by Inference in Generative Models},
  author = {Roweis, S},
  year = {2005},
  journal = {Speech Separation by Humans and Machines},
  pages = {97--134},
  keywords = {articulatory mod-,denoising,f0 estimation,inference,pitch,probabilistic generative models,source separation,time scale modification,tracking},
  file = {/home/marnix/Zotero/storage/YD2XRJ4A/Roweis2005 ASR by inference in generative models.pdf}
}

@article{Roy2024,
  title = {Reparameterization Invariance in Approximate {{Bayesian}} Inference},
  author = {Roy, Hrittik and Miani, Marco and Ek, Carl Henrik and Hennig, Philipp and Pf{\"o}rtner, Marvin and Tatzel, Lukas and Hauberg, S{\o}ren},
  year = {2024},
  journal = {Advances in Neural Information Processing Systems},
  volume = {37},
  pages = {8132--8164},
  file = {/home/marnix/Zotero/storage/IHW4LGRW/Roy et al. - Reparameterization invariance in approximate Bayesian inference.pdf}
}

@inproceedings{Rudoy2007,
  title = {Conditionally Linear {{Gaussian}} Models for Estimating Vocal Tract Resonances},
  booktitle = {{{INTERSPEECH}}},
  author = {Rudoy, Daniel and Spendley, Daniel N. and Wolfe, Patrick J.},
  year = {2007},
  abstract = {Optimization techniques are implemented by means of a program analyzer used in connection with a program compiler to optimize usage of limited register resources in a computer processor. The first optimization technique, called interprocedural global variable promotion allows the global variables of a program to be accessed in common registers across a plurality of procedures. Moreover, a single common register can be used for different global variables in distinct regions of a program call graph. This is realized by identifying subgraphs, of the program call graph, called webs, where the variable is used. The second optimization technique, called spill code motion, involves the identification of regions of the call graph, called clusters, that facilitate the movement of spill instructions to procedures which are executed relatively less often. This decreases the overhead of register saves and restores which must be executed for procedure calls.},
  keywords = {Acoustic cryptanalysis,Autoregressive model,Benchmark (computing),Cepstrum,Cross-correlation,Database,Kalman filter,Machine perception,Mean squared error,Statistical model,Tract (literature),Vector autoregression}
}

@article{Rudzicz2010,
  title = {Correcting {{Errors}} in {{Speech Recognition}} with {{Articulatory Dynamics}}},
  author = {Rudzicz, Frank},
  year = {2010},
  journal = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
  number = {July},
  pages = {60--68},
  abstract = {We introduce a novel mechanism for incorporating articulatory dynamics into speech recognition with the theory of task dynamics. This system reranks sentencelevel hypotheses by the likelihoods of their hypothetical articulatory realizations which are derived from relationships learned with aligned acoustic/articulatory data. Experiments compare this with two baseline systems, namely an acoustic hidden Markov model and a dynamic Bayes network augmented with discretized representations of the vocal tract. Our system based on task dynamics reduces worderror rates significantly by 10.2\% relative to the best baseline models. {\copyright} 2010 Association for Computational Linguistics.},
  isbn = {9781617388088},
  file = {/home/marnix/Zotero/storage/MILJEBRL/Rudzicz2010 Correcting errors in speech recognition with articulatory dynamics.pdf.pdf}
}

@article{Rumelhart1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = oct,
  journal = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  urldate = {2020-10-01},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  copyright = {1986 Nature Publishing Group},
  langid = {english},
  file = {/home/marnix/Zotero/storage/B9QCAAT4/Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf;/home/marnix/Zotero/storage/U5NAIMQR/323533a0.html}
}

@article{Russel2019,
  title = {Model {{Selection}} and {{Parameter Inference}} in {{Phylogenetics Using Nested Sampling}}},
  author = {Russel, Patricio Maturana and Brewer, Brendon J and Klaere, Steffen and Bouckaert, Remco R},
  year = {2019},
  month = mar,
  journal = {Systematic Biology},
  volume = {68},
  number = {2},
  pages = {219--233},
  issn = {1063-5157},
  doi = {10.1093/sysbio/syy050},
  urldate = {2021-04-09},
  abstract = {Bayesian inference methods rely on numerical algorithms for both model selection and parameter inference. In general, these algorithms require a high computational effort to yield reliable estimates. One of the major challenges in phylogenetics is the estimation of the marginal likelihood. This quantity is commonly used for comparing different evolutionary models, but its calculation, even for simple models, incurs high computational cost. Another interesting challenge relates to the estimation of the posterior distribution. Often, long Markov chains are required to get sufficient samples to carry out parameter inference, especially for tree distributions. In general, these problems are addressed separately by using different procedures. Nested sampling (NS) is a Bayesian computation algorithm, which provides the means to estimate marginal likelihoods together with their uncertainties, and to sample from the posterior distribution at no extra cost. The methods currently used in phylogenetics for marginal likelihood estimation lack in practicality due to their dependence on many tuning parameters and their inability of most implementations to provide a direct way to calculate the uncertainties associated with the estimates, unlike NS. In this article, we introduce NS to phylogenetics. Its performance is analysed under different scenarios and compared to established methods. We conclude that NS is a competitive and attractive algorithm for phylogenetic inference. An implementation is available as a package for BEAST 2 under the LGPL licence, accessible at https://github.com/BEAST2-Dev/nested-sampling.},
  file = {/home/marnix/Zotero/storage/RJNH8DZF/Russel et al. - 2019 - Model Selection and Parameter Inference in Phyloge.pdf;/home/marnix/Zotero/storage/KU878Q3N/5046926.html}
}

@article{Russell1992,
  title = {Understanding the Term Structure of Interest Rates: {{The}} Expectations Theory},
  author = {Russell, Steven H},
  year = {1992},
  journal = {Review},
  volume = {74},
  file = {/home/marnix/Zotero/storage/8L4XFLRL/Russell - 1992 - Understanding the term structure of interest rates The expectations theory.pdf}
}

@article{Saatci2010,
  title = {Gaussian {{Process Change Point Models}}},
  author = {Saatci, Yunus and Turner, Ryan and Rasmussen, Carl Edward},
  year = {2010},
  abstract = {We combine Bayesian online change point detection with Gaussian processes to create a nonparametric time series model which can handle change points. The model can be used to locate change points in an online manner; and, unlike other Bayesian online change point detection algorithms, is applicable when temporal correlations in a regime are expected. We show three variations on how to apply Gaussian processes in the change point context, each with their own advantages. We present methods to reduce the computational burden of these models and demonstrate it on several real world data sets.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/2JS47FKS/Saatçi et al. - Gaussian Process Change Point Models.pdf}
}

@article{Sainath2009,
  title = {Applications of {{Broad Class Knowledge}} for {{Noise Robust Speech Recognition}}},
  author = {Sainath, Tara N.},
  year = {2009},
  journal = {PhD Thesis, MIT},
  number = {2004},
  pages = {164},
  file = {/home/marnix/Zotero/storage/C68SDYBX/Sainath2009.pdf}
}

@incollection{Salimbeni2017,
  title = {Doubly {{Stochastic Variational Inference}} for {{Deep Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Salimbeni, Hugh and Deisenroth, Marc},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {4588--4599},
  publisher = {Curran Associates, Inc.},
  urldate = {2020-07-01},
  file = {/home/marnix/Zotero/storage/URQ5E6BS/Salimbeni and Deisenroth - 2017 - Doubly Stochastic Variational Inference for Deep G.pdf;/home/marnix/Zotero/storage/CLQ6JHXZ/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes.html}
}

@article{Salimbeni2018,
  title = {Natural {{Gradients}} in {{Practice}}: {{Non-Conjugate Variational Inference}} in {{Gaussian Process Models}}},
  shorttitle = {Natural {{Gradients}} in {{Practice}}},
  author = {Salimbeni, Hugh and Eleftheriadis, Stefanos and Hensman, James},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.09151 [cs, stat]},
  eprint = {1803.09151},
  primaryclass = {cs, stat},
  urldate = {2021-02-19},
  abstract = {The natural gradient method has been used effectively in conjugate Gaussian process models, but the non-conjugate case has been largely unexplored. We examine how natural gradients can be used in non-conjugate stochastic settings, together with hyperparameter learning. We conclude that the natural gradient can significantly improve performance in terms of wall-clock time. For ill-conditioned posteriors the benefit of the natural gradient method is especially pronounced, and we demonstrate a practical setting where ordinary gradients are unusable. We show how natural gradients can be computed efficiently and automatically in any parameterization, using automatic differentiation. Our code is integrated into the GPflow package.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/7TE3BUSA/Salimbeni et al. - 2018 - Natural Gradients in Practice Non-Conjugate Varia.pdf;/home/marnix/Zotero/storage/5QZLDRMF/1803.html}
}

@misc{Samo2015,
  title = {Generalized {{Spectral Kernels}}},
  author = {Samo, Yves-Laurent Kom and Roberts, Stephen},
  year = {2015},
  month = oct,
  number = {arXiv:1506.02236},
  eprint = {1506.02236},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1506.02236},
  urldate = {2025-07-01},
  abstract = {In this paper we propose a family of tractable kernels that is dense in the family of bounded positive semi-definite functions (i.e. can approximate any bounded kernel with arbitrary precision). We start by discussing the case of stationary kernels, and propose a family of spectral kernels that extends existing approaches such as spectral mixture kernels and sparse spectrum kernels. Our extension has two primary advantages. Firstly, unlike existing spectral approaches that yield infinite differentiability, the kernels we introduce allow learning the degree of differentiability of the latent function in Gaussian process (GP) models and functions in the reproducing kernel Hilbert space (RKHS) in other kernel methods. Secondly, we show that some of the kernels we propose require fewer parameters than existing spectral kernels for the same accuracy, thereby leading to faster and more robust inference. Finally, we generalize our approach and propose a flexible and tractable family of spectral kernels that we prove can approximate any continuous bounded nonstationary kernel.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/U2F63M9A/Samo and Roberts - 2015 - Generalized Spectral Kernels.pdf;/home/marnix/Zotero/storage/MZJ9FYFG/1506.html}
}

@incollection{Sanchez1989,
  title = {Application of Classical, Bayesian and Maximum Entropy Spectrum Analysis to Nonstationary Time Series Data},
  booktitle = {Maximum Entropy and Bayesian Methods},
  author = {Sanchez, Juana},
  year = {1989},
  pages = {309--319},
  publisher = {Springer}
}

@article{Sanjekdar2017,
  title = {Implementing {{Speech Emotion Recognition}} to {{Decrease Suicide Rates}}},
  author = {Sanjekdar, Abdelrahman},
  year = {2017},
  pages = {1--5},
  file = {/home/marnix/Zotero/storage/YDNVQ5UB/Sanjekdar - 2017 - Implementing Speech Emotion Recognition to Decrease Suicide Rates.pdf}
}

@inproceedings{Sarkka2011,
  title = {Linear {{Operators}} and {{Stochastic Partial Differential Equations}} in {{Gaussian Process Regression}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} -- {{ICANN}} 2011},
  author = {S{\"a}rkk{\"a}, Simo},
  editor = {Honkela, Timo and Duch, W{\l}odzis{\l}aw and Girolami, Mark and Kaski, Samuel},
  year = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {151--158},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-21738-8_20},
  abstract = {In this paper we shall discuss an extension to Gaussian process (GP) regression models, where the measurements are modeled as linear functionals of the underlying GP and the estimation objective is a general linear operator of the process. We shall show how this framework can be used for modeling physical processes involved in measurement of the GP and for encoding physical prior information into regression models in form of stochastic partial differential equations (SPDE). We shall also illustrate the practical applicability of the theory in a simulated application.},
  isbn = {978-3-642-21738-8},
  langid = {english},
  keywords = {Gaussian process regression,inverse problem,linear operator,stochastic partial differential equation},
  file = {/home/marnix/Zotero/storage/7A9EZ2EI/Särkkä - 2011 - Linear Operators and Stochastic Partial Differenti.pdf}
}

@misc{Sarkka2019,
  title = {The {{Use}} of {{Gaussian Processes}} in {{System Identification}}},
  author = {S{\"a}rkk{\"a}, Simo},
  year = {2019},
  month = jul,
  number = {arXiv:1907.06066},
  eprint = {1907.06066},
  primaryclass = {cs, eess, stat},
  publisher = {arXiv},
  urldate = {2024-02-26},
  abstract = {Gaussian processes are used in machine learning to learn input-output mappings from observed data. Gaussian process regression is based on imposing a Gaussian process prior on the unknown regressor function and statistically conditioning it on the observed data. In system identification, Gaussian processes are used to form time series prediction models such as non-linear finite-impulse response (NFIR) models as well as non-linear autoregressive (NARX) models. Gaussian process state-space models (GPSS) can be used to learn the dynamic and measurement models for a state-space representation of the input-output data. Temporal and spatio-temporal Gaussian processes can be directly used to form regressor on the data in the time domain. The aim of this article is to briefly outline the main directions in system identification methods using Gaussian processes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/ZVPS6RWR/Särkkä - 2019 - The Use of Gaussian Processes in System Identifica.pdf;/home/marnix/Zotero/storage/4EWVVRV4/1907.html}
}

@inproceedings{Saxena2009,
  title = {Learning Sound Location from a Single Microphone},
  booktitle = {2009 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Saxena, A. and Ng, A.Y.},
  year = {2009},
  month = may,
  pages = {1737--1742},
  publisher = {IEEE},
  doi = {10.1109/ROBOT.2009.5152861},
  isbn = {978-1-4244-2788-8},
  file = {/home/marnix/Zotero/storage/J63BPHAC/Saxena2009 Learning Sound Location from a Single Microphone.pdf}
}

@article{Sayadi2009,
  title = {A Model-Based {{Bayesian}} Framework for {{ECG}} Beat Segmentation},
  author = {Sayadi, O. and Shamsollahi, M. B.},
  year = {2009},
  journal = {Physiological Measurement},
  volume = {30},
  number = {3},
  pages = {335--352},
  issn = {09673334},
  doi = {10.1088/0967-3334/30/3/008},
  abstract = {The study of electrocardiogram (ECG) waveform amplitudes, timings and patterns has been the subject of intense research, for it provides a deep insight into the diagnostic features of the heart's functionality. In some recent works, a Bayesian filtering paradigm has been proposed for denoising and compression of ECG signals. In this paper, it is shown that this framework may be effectively used for ECG beat segmentation and extraction of fiducial points. Analytic expressions for the determination of points and intervals are derived and evaluated on various real ECG signals. Simulation results show that the method can contribute to and enhance the clinical ECG beat segmentation performance.},
  isbn = {0967-3334 (Print). 0967-3334 (Linking)},
  pmid = {19242046},
  keywords = {Correction algorithm,ECG dynamical model,Extended KalmanFilter,Fiducial points extraction,Fluctuating estimates,Segmentation},
  file = {/home/marnix/Zotero/storage/UZFQNKSR/Sayadi2009 A model-based Bayesian framework for ECG beat segmentation.pdf}
}

@inproceedings{Sayir1999,
  title = {Arithmetic Coding for Noisy Channels},
  booktitle = {Proceedings of the 1999 {{IEEE Information Theory}} and {{Communications Workshop}} ({{Cat}}. {{No}}. {{99EX253}})},
  author = {Sayir, J.},
  year = {1999},
  pages = {69--71},
  publisher = {IEEE},
  address = {Kruger National Park, South Africa},
  doi = {10.1109/ITCOM.1999.781412},
  urldate = {2020-03-05},
  abstract = {Arithmetic Coding is generalized to provide a method for transforming source output se quences into sequences whose probability distribution approaches any desired probability distribution. A further modi cation in which gaps are introduced be tween source intervals allows arithmetic encoders to produce code sequences with any information rate, enabling the encoder to perform joint source-chan nel coding. The results of a simulation are presented where arithmetic coding is used as a channel encoder in conjunction with a sequential decoder.},
  isbn = {978-0-7803-5268-1},
  langid = {english},
  file = {/home/marnix/Zotero/storage/P22UHKRK/Sayir - 1999 - Arithmetic coding for noisy channels.pdf}
}

@inproceedings{Scetbon2021,
  title = {A {{Spectral Analysis}} of {{Dot-product Kernels}}},
  booktitle = {Proceedings of {{The}} 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Scetbon, Meyer and Harchaoui, Zaid},
  year = {2021},
  month = mar,
  pages = {3394--3402},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-05-28},
  abstract = {We present eigenvalue decay estimates of integral operators associated with compositional dot-product kernels. The estimates improve on previous ones established for power series kernels on spheres. This allows us to obtain the volumes of balls in the corresponding reproducing kernel Hilbert spaces. We discuss the consequences on statistical estimation with compositional dot product kernels and highlight interesting trade-offs between the approximation error and the statistical error depending on the number of compositions and the smoothness of the kernels.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/IW9KQVL9/Scetbon and Harchaoui - 2021 - A Spectral Analysis of Dot-product Kernels.pdf}
}

@misc{Schaffland2022,
  title = {The {{Mechanical Neural Network}}({{MNN}}) -- {{A}} Physical Implementation of a Multilayer Perceptron for Education and Hands-on Experimentation},
  author = {Schaffland, Axel},
  year = {2022},
  month = jul,
  number = {arXiv:2207.07482},
  eprint = {2207.07482},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.07482},
  urldate = {2023-09-11},
  abstract = {In this paper the Mechanical Neural Network(MNN) is introduced, a physical implementation of a multilayer perceptron(MLP) with ReLU activation functions, two input neurons, four hidden neurons and two output neurons. This physical model of a MLP is used in education to give a hands on experience and allow students to experience the effect of changing the parameters of the network on the output. Neurons are small wooden levers which are connected by threads. Students can adapt the weights between the neurons by moving the clamps connecting a neuron via a thread to the next. The MNN can model real valued functions and logical operators including XOR.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,I.2.6,K.3.2},
  file = {/home/marnix/Zotero/storage/RER9K76T/Schaffland - 2022 - The Mechanical Neural Network(MNN) -- A physical i.pdf;/home/marnix/Zotero/storage/MYWJKK9T/2207.html}
}

@article{Scheiber2015,
  title = {On the {{Chebyshev}} Approximation of a Function with Two Variables},
  author = {Scheiber, Ernest},
  year = {2015},
  month = apr,
  journal = {arXiv:1504.04693 [math]},
  eprint = {1504.04693},
  primaryclass = {math},
  urldate = {2021-10-13},
  abstract = {There is presented an approach to find an approximation polynomial of a function with two variables based on the two dimensional discrete Fourier transform. The approximation polynomial is expressed through Chebyshev polynomials. There is given an uniform convergence result.},
  archiveprefix = {arXiv},
  keywords = {{65D15(Primary), 40-04 (Secondary)},Mathematics - Numerical Analysis},
  file = {/home/marnix/Zotero/storage/T4X8CTSQ/Scheiber - 2015 - On the Chebyshev approximation of a function with .pdf;/home/marnix/Zotero/storage/Q5E3D8F4/1504.html}
}

@misc{Schick2023,
  title = {Toolformer: {{Language Models Can Teach Themselves}} to {{Use Tools}}},
  shorttitle = {Toolformer},
  author = {Schick, Timo and {Dwivedi-Yu}, Jane and Dess{\`i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  year = {2023},
  month = feb,
  number = {arXiv:2302.04761},
  eprint = {2302.04761},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.04761},
  urldate = {2023-05-10},
  abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q{\textbackslash}\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/marnix/Zotero/storage/VZTK2SZV/Schick et al. - 2023 - Toolformer Language Models Can Teach Themselves t.pdf;/home/marnix/Zotero/storage/WXLD4P2Q/2302.html}
}

@inproceedings{Schiel2018,
  title = {Evaluation of {{Automatic Formant Trackers}}},
  booktitle = {Proceedings of the 11th {{Language Resources}} and {{Evaluation Conference}}},
  author = {Schiel, Florian and Zitzelsberger, Thomas},
  year = {2018},
  month = may,
  publisher = {European Language Resource Association},
  address = {Miyazaki, Japan},
  urldate = {2019-04-27},
  file = {/home/marnix/Zotero/storage/3QQ2NGIJ/Schiel and Zitzelsberger - 2018 - Evaluation of Automatic Formant Trackers.pdf}
}

@techreport{Schleusing2012,
  title = {Multi-Parametric Source-Filter Separation of Speech and Prosodic Voice Restoration},
  author = {Schleusing, Olaf},
  year = {2012},
  institution = {EPFL},
  file = {/home/marnix/Zotero/storage/F3TATEPJ/Schleusing - 2012 - Multi-parametric source-filter separation of speec.pdf}
}

@article{Schneider2019,
  title = {Wav2vec: {{Unsupervised Pre-training}} for {{Speech Recognition}}},
  shorttitle = {Wav2vec},
  author = {Schneider, Steffen and Baevski, Alexei and Collobert, Ronan and Auli, Michael},
  year = {2019},
  month = sep,
  journal = {arXiv:1904.05862 [cs]},
  eprint = {1904.05862},
  primaryclass = {cs},
  urldate = {2020-09-14},
  abstract = {We explore unsupervised pre-training for speech recognition by learning representations of raw audio. wav2vec is trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training. We pre-train a simple multi-layer convolutional neural network optimized via a noise contrastive binary classification task. Our experiments on WSJ reduce WER of a strong character-based log-mel filterbank baseline by up to 36\% when only a few hours of transcribed data is available. Our approach achieves 2.43\% WER on the nov92 test set. This outperforms Deep Speech 2, the best reported character-based system in the literature while using two orders of magnitude less labeled training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/marnix/Zotero/storage/J447RQQL/Schneider et al. - 2019 - wav2vec Unsupervised Pre-training for Speech Reco.pdf;/home/marnix/Zotero/storage/VNVM7Q8D/1904.html}
}

@book{Schnupp2011,
  title = {Auditory Neuroscience: {{Making}} Sense of Sound},
  author = {Schnupp, Jan and Nelken, Israel and King, Andrew},
  year = {2011},
  publisher = {MIT press}
}

@book{Scholkopf2002,
  title = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
  shorttitle = {Learning with Kernels},
  author = {Sch{\"o}lkopf, Bernhard and Smola, Alexander J.},
  year = {2002},
  series = {Adaptive Computation and Machine Learning},
  publisher = {MIT Press},
  address = {Cambridge, Mass},
  isbn = {978-0-262-19475-4},
  langid = {english},
  lccn = {Q325.5 .S32 2002},
  keywords = {Kernel functions,Support vector machines},
  file = {/home/marnix/Zotero/storage/W33QZLBE/Schölkopf and Smola - 2002 - Learning with kernels support vector machines, re.pdf}
}

@book{Schroeder1999,
  title = {Computer {{Speech}}: {{Recognition}}, {{Compression}}, {{Synthesis}}},
  shorttitle = {Computer {{Speech}}},
  author = {Schroeder, Manfred R.},
  year = {1999},
  series = {Springer {{Series}} in {{Information Sciences}}},
  publisher = {Springer-Verlag},
  address = {Berlin Heidelberg},
  doi = {10.1007/978-3-662-03861-1},
  urldate = {2020-02-06},
  abstract = {Computer Speech is an introduction to multimedia speech applications and is suitable for nonspecialists. It treats such contemporary subjects as automatic speech recognition and speaker verification for banking by computer and privileged (medical, military, diplomatic) information and control access. The book also focusses on speech and audio compression for mobile communication and the Internet. The importance of subjective quality criteria is stressed. A brief history of speech research summarizes the development from the first talking machines in rococo Europe to modern x-ray methods of articulatory analysis. The book also contains introductions to human monaural and binaural hearing, and the basic concepts of signal analysis.},
  isbn = {978-3-662-03861-1},
  langid = {english},
  file = {/home/marnix/Zotero/storage/E5X5C3XE/Schroeder - 1999 - Computer Speech Recognition, Compression, Synthes.pdf;/home/marnix/Zotero/storage/WABQGK58/9783662038611.html}
}

@book{Schroeder2006,
  title = {Number {{Theory}} in {{Science}} and {{Communication}}: {{With Applications}} in {{Cryptography}}, {{Physics}}, {{Digital Information}}, {{Computing}}, and {{Self-Similarity}}},
  shorttitle = {Number {{Theory}} in {{Science}} and {{Communication}}},
  author = {Schroeder, M. R.},
  year = {2006},
  series = {Springer {{Series}} in {{Information Sciences}}},
  edition = {4},
  publisher = {Springer-Verlag},
  address = {Berlin Heidelberg},
  doi = {10.1007/b137861},
  urldate = {2020-03-10},
  abstract = {"Number Theory in Science and Communication" is a well-known introduction for non-mathematicians to this fascinating and useful branch of applied mathematics . It stresses intuitive understanding rather than abstract theory and highlights important concepts such as continued fractions, the golden ratio, quadratic residues and Chinese remainders, trapdoor functions, pseudoprimes and primitive elements. Their applications to problems in the real world are one of the main themes of the book. This revised fourth edition is augmented by recent advances in primes in progressions, twin primes, prime triplets, prime quadruplets and quintruplets, factoring with elliptic curves, quantum factoring, Golomb rulers and "baroque" integers. From reviews of earlier editions -- "I continue to find [Schroeder's] Number Theory a goldmine of valuable information. It is a marvellous book, in touch with the most recent applications of number theory and written with great clarity and humor.' Philip Morrison (Scientific American) "A light-hearted and readable volume with a wide range of applications to which the author has been a productive contributor -- useful mathematics outside the formalities of theorem and proof." Martin Gardner},
  isbn = {978-3-540-26598-6},
  langid = {english},
  file = {/home/marnix/Zotero/storage/H6QX32LK/Schroeder - 2006 - Number Theory in Science and Communication With A.djvu;/home/marnix/Zotero/storage/TWH4UKDZ/9783540265986.html}
}

@book{Scripture1904,
  title = {The Elements of Experimental Phonetics},
  author = {Scripture, Edward Wheeler},
  year = {1904},
  publisher = {C. Scribner's Sons}
}

@book{Seeger2004,
  title = {Low {{Rank Updates}} for the {{Cholesky Decomposition}}},
  editor = {Seeger, Matthias},
  year = {2004},
  abstract = {Usage of the Sherman-Morrison-Woodbury formula to update linear systems after low rank modifications of the system matrix is widespread in machine learning. However, it is well known that this formula can lead to serious instabilities in the presence of roundoff error. If the system matrix is symmetric positive definite, it is almost always possible to use a representation based on the Cholesky decomposition which renders the same results (in exact arithmetic) at the same or less operational cost, but typically is much more numerically stable. In this note, we show how the Cholesky decomposition can be updated to incorporate low rank additions or downdated for low rank subtractions. We also discuss a special case of an indefinite update of rank two. The methods discussed here are well-known in the numerical mathematics literature, and code for most of them can be found in the LINPACK suite},
  keywords = {Cholesky decomposition,Low rank formulae,Numerical mathematics}
}

@article{Sellier2023,
  title = {Bayesian Online Change Point Detection with {{Hilbert}} Space Approximate {{Student-t}} Process},
  author = {Sellier, Jeremy and Dellaportas, Petros},
  year = {2023},
  langid = {english},
  file = {/home/marnix/Zotero/storage/HQRPHYJ7/Sellier and Dellaportas - Bayesian online change point detection with Hilber.pdf}
}

@phdthesis{Serwy2017,
  title = {Hilbert Phase Methods for Glottal Activity Detection},
  author = {Serwy, Roger David},
  year = {2017},
  school = {University of Illinois at Urbana-Champaign},
  file = {/home/marnix/Zotero/storage/TBTBSJGT/Serwy - 2017 - Hilbert phase methods for glottal activity detecti.pdf}
}

@article{Sewell2009,
  title = {Probabilistic Electoral Methods, Representative Probability, and Maximum Entropy},
  author = {Sewell, Roger and MacKay, David and McLean, Iain},
  year = {2009},
  journal = {Voting matters},
  number = {26},
  pages = {16--38},
  abstract = {Aprobabilistic electoral systemis described in a context accessible to readers not familiar with social choice theory. This systemsatisfies axioms of: identical treatment of each voter and of each candidate; universal domain; fair representation of the pairwise preferences of the electorate; independenceof irrelevant alter- natives; and clarity of voting for pairwise out- comes; and henceArrow's other axioms (weak Pareto and no dictator) are also satisfied. It produces in an information-theoretic sense the least surprising outcome given any candidate- symmetric prior beliefs on the voters' prefer- ences, and is shown to be able to compromise appropriately in situations where a Condorcet winner would not be elected top under many other systems. However, difficulties can arise with this systemin situations where one politi- cal party is permitted to flood the candidate list with large numbers of their own candidates. The empirical properties of this system are explored and compared with the systems known as ``Majority (or Plurality) Rule'' and ``RandomDictator''. We also make the case for using a proba- bilistic system even in the simple 2-candidate case.},
  file = {/home/marnix/Zotero/storage/944KZIIJ/Probabilistic electoral methods.pdf}
}

@article{Shadle2016,
  title = {Comparing Measurement Errors for Formants in Synthetic and Natural Vowels},
  author = {Shadle, Christine H. and Nam, Hosung and Whalen, D. H.},
  year = {2016},
  month = feb,
  journal = {The Journal of the Acoustical Society of America},
  volume = {139},
  number = {2},
  pages = {713--727},
  issn = {0001-4966},
  doi = {10.1121/1.4940665},
  urldate = {2019-07-22},
  abstract = {The measurement of formant frequencies of vowels is among the most common measurements in speech studies, but measurements are known to be biased by the particular fundamental frequency (F0) exciting the formants. Approaches to reducing the errors were assessed in two experiments. In the first, synthetic vowels were constructed with five different first formant (F1) values and nine different F0 values; formant bandwidths, and higher formant frequencies, were constant. Input formant values were compared to manual measurements and automatic measures using the linear prediction coding-Burg algorithm, linear prediction closed-phase covariance, the weighted linear prediction-attenuated main excitation (WLP-AME) algorithm [Alku, Pohjalainen, Vainio, Laukkanen, and Story (2013). J. Acoust. Soc. Am. 134(2), 1295--1313], spectra smoothed cepstrally and by averaging repeated discrete Fourier transforms. Formants were also measured manually from pruned reassigned spectrograms (RSs) [Fulop (2011). Speech Spectrum Analysis (Springer, Berlin)]. All but WLP-AME and RS had large errors in the direction of the strongest harmonic; the smallest errors occur with WLP-AME and RS. In the second experiment, these methods were used on vowels in isolated words spoken by four speakers. Results for the natural speech show that F0 bias affects all automatic methods, including WLP-AME; only the formants measured manually from RS appeared to be accurate. In addition, RS coped better with weaker formants and glottal fry.},
  pmcid = {PMC4752539},
  pmid = {26936555},
  file = {/home/marnix/Zotero/storage/M8STFNEJ/Shadle et al. - 2016 - Comparing measurement errors for formants in synth.pdf}
}

@article{Shadle2024,
  title = {Assessing Accuracy of Resonances Obtained with Reassigned Spectrograms from the ``Ground Truth'' of Physical Vocal Tract Models},
  author = {Shadle, Christine H. and Fulop, Sean A. and Chen, Wei-Rong and Whalen, D. H.},
  year = {2024},
  month = feb,
  journal = {The Journal of the Acoustical Society of America},
  volume = {155},
  number = {2},
  pages = {1253--1263},
  issn = {0001-4966},
  doi = {10.1121/10.0024548},
  urldate = {2024-02-15},
  abstract = {The reassigned spectrogram (RS) has emerged as the most accurate way to infer vocal tract resonances from the acoustic signal [Shadle, Nam, and Whalen (2016). ``Comparing measurement errors for formants in synthetic and natural vowels,'' J. Acoust. Soc. Am. 139(2), 713--727]. To date, validating its accuracy has depended on formant synthesis for ground truth values of these resonances. Synthesis is easily controlled, but it has many intrinsic assumptions that do not necessarily accurately realize the acoustics in the way that physical resonances would. Here, we show that physical models of the vocal tract with derivable resonance values allow a separate approach to the ground truth, with a different range of limitations. Our three-dimensional printed vocal tract models were excited by white noise, allowing an accurate determination of the resonance frequencies. Then, sources with a range of fundamental frequencies were implemented, allowing a direct assessment of whether RS avoided the systematic bias towards the nearest strong harmonic to which other analysis techniques are prone. RS was indeed accurate at fundamental frequencies up to 300\,Hz; above that, accuracy was somewhat reduced. Future directions include testing mechanical models with the dimensions of children's vocal tracts and making RS more broadly useful by automating the detection of resonances.},
  file = {/home/marnix/Zotero/storage/NZZCAMHC/Assessing-accuracy-of-resonances-obtained-with.html}
}

@inproceedings{Shah2014,
  title = {Student-t Processes as Alternatives to {{Gaussian}} Processes},
  booktitle = {Artificial Intelligence and Statistics},
  author = {Shah, Amar and Wilson, Andrew and Ghahramani, Zoubin},
  year = {2014},
  pages = {877--885},
  file = {/home/marnix/Zotero/storage/2YXKFGSV/Shah et al. - Student-t Processes as Alternatives to Gaussian Pr.pdf}
}

@article{Shannon1948,
  title = {A {{Mathematical Theory}} of {{Communication}}},
  author = {Shannon, Claude Elwood},
  year = {1948},
  volume = {27},
  number = {April 1924},
  pages = {379--423},
  file = {/home/marnix/Zotero/storage/F5CJSKRK/Shannon1948 A Mathematical Theory of Communication.pdf}
}

@article{Shen2019,
  title = {Learning Spectrograms with Convolutional Spectral Kernels},
  author = {Shen, Zheyang and Heinonen, Markus and Kaski, Samuel},
  year = {2019},
  month = oct,
  journal = {arXiv:1905.09917 [cs, stat]},
  eprint = {1905.09917},
  primaryclass = {cs, stat},
  urldate = {2020-05-01},
  abstract = {We introduce the convolutional spectral kernel (CSK), a novel family of non-stationary, nonparametric covariance kernels for Gaussian process (GP) models, derived from the convolution between two imaginary radial basis functions. We present a principled framework to interpret CSK, as well as other deep probabilistic models, using approximated Fourier transform, yielding a concise representation of input-frequency spectrogram. Observing through the lens of the spectrogram, we provide insight on the interpretability of deep models. We then infer the functional hyperparameters using scalable variational and MCMC methods. On small- and medium-sized spatiotemporal datasets, we demonstrate improved generalization of GP models when equipped with CSK, and their capability to extract non-stationary periodic patterns.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/Q3E6KIYR/Shen et al. - 2019 - Learning spectrograms with convolutional spectral .pdf;/home/marnix/Zotero/storage/LPVBHS82/1905.html}
}

@inproceedings{Shen2019a,
  title = {Harmonizable Mixture Kernels with Variational {{Fourier}} Features},
  booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics},
  author = {Shen, Zheyang and Heinonen, Markus and Kaski, Samuel},
  year = {2019},
  pages = {3273--3282},
  organization = {PMLR},
  file = {/home/marnix/Zotero/storage/Q8CUVJZ2/Shen et al. - Harmonizable mixture kernels with variational Four.pdf}
}

@article{Shewchuk1994,
  title = {An Introduction to the Conjugate Gradient Method without the Agonizing Pain},
  author = {Shewchuk, Jonathan Richard},
  year = {1994},
  publisher = {Carnegie-Mellon University. Department of Computer Science},
  file = {/home/marnix/Zotero/storage/T6W2JVU6/Shewchuk and others - 1994 - An introduction to the conjugate gradient method w.pdf}
}

@inproceedings{Shi2003,
  title = {Spectrogram-Based Formant Tracking via Particle Filters},
  booktitle = {2003 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}, 2003. {{Proceedings}}. ({{ICASSP}} '03).},
  author = {Shi, Yu and Chang, E.},
  year = {2003},
  month = apr,
  volume = {1},
  pages = {I-I},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2003.1198743},
  abstract = {The paper presents a particle-filtering method for estimating formant frequencies of speech signals from spectrograms. First, frequency bands corresponding to the analyzed formants are extracted via a two-step dynamic programming based algorithm. A particle-filtering method is then used to locate accurately formants in every formant area based on the posterior PDF described by a set of support points with associated weights. Formant trajectories of voiced frames of a group of 81 utterances were manually tracked and labeled, partly for model training and partly for algorithm evaluation. In the experiments, the proposed method obtains average estimation errors of 72, 115, and 113 Hz for the first three formants, respectively, whereas the LPC based method induces 118, 172, and 250 Hz deviations. The experimental results show that the formants estimated by the proposed method are quite reliable and the trajectories are more accurate than LPC.},
  keywords = {Algorithm design and analysis,dynamic programming,Dynamic programming,estimation errors,formant frequency estimation,formant tracking,formant trajectories,frequency estimation,Frequency estimation,Heuristic algorithms,linear predictive coding,Linear predictive coding,LPC,model training,Monte Carlo methods,nonlinear filters,particle filters,Particle filters,Particle tracking,posterior PDF,sequential Monte Carlo methods,spectral analysis,Spectrogram,Speech,speech processing,speech signal spectrograms,statistical analysis,tracking,Trajectory,voiced frames},
  file = {/home/marnix/Zotero/storage/5IP54CUX/Yu Shi and Chang - 2003 - Spectrogram-based formant tracking via particle fi.pdf;/home/marnix/Zotero/storage/B49TIAWH/1198743.html}
}

@inproceedings{Shi2017,
  title = {A Variational {{EM}} Method for Pole-Zero Modeling of Speech with Mixed Block Sparse and {{Gaussian}} Excitation},
  booktitle = {2017 25th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Shi, Liming and Nielsen, Jesper Kj{\ae}r and Jensen, Jesper Rindom and Christensen, Mads Gr{\oe}sb{$\oslash$}ll},
  year = {2017},
  month = aug,
  pages = {1784--1788},
  issn = {2076-1465},
  doi = {10.23919/EUSIPCO.2017.8081516},
  abstract = {The modeling of speech can be used for speech synthesis and speech recognition. We present a speech analysis method based on pole-zero modeling of speech with mixed block sparse and Gaussian excitation. By using a pole-zero model, instead of the all-pole model, a better spectral fitting can be expected. Moreover, motivated by the block sparse glottal flow excitation during voiced speech and the white noise excitation for unvoiced speech, we model the excitation sequence as a combination of block sparse signals and white noise. A variational EM (VEM) method is proposed for estimating the posterior PDFs of the block sparse residuals and point estimates of modelling parameters within a sparse Bayesian learning framework. Compared to conventional pole-zero and all-pole based methods, experimental results show that the proposed method has lower spectral distortion and good performance in reconstructing of the block sparse excitation.},
  keywords = {all-pole based methods,all-pole model,Analytical models,Bayes methods,block sparse excitation,block sparse residuals,block sparse signals,Estimation,Europe,excitation sequence,expectation-maximisation algorithm,expectation-maximization algorithm,Gaussian excitation,glottal flow excitation,mixed block sparse,pole-zero modeling,poles and zeros,Probability density function,sparse Bayesian learning framework,Speech,Speech analysis,speech analysis method,speech processing,speech recognition,speech synthesis,unvoiced speech,variational EM method,voiced speech,white noise,White noise,white noise excitation},
  file = {/home/marnix/Zotero/storage/YXIKNEA4/Shi et al. - 2017 - A variational EM method for pole-zero modeling of .pdf;/home/marnix/Zotero/storage/S7DITG9R/8081516.html}
}

@article{Shore1980,
  title = {Axiomatic Derivation of the Principle of Maximum Entropy and the Principle of Minimum Cross-Entropy},
  author = {Shore, J. and Johnson, R.},
  year = {1980},
  month = jan,
  journal = {IEEE Transactions on Information Theory},
  volume = {26},
  number = {1},
  pages = {26--37},
  issn = {1557-9654},
  doi = {10.1109/TIT.1980.1056144},
  abstract = {Jaynes's principle of maximum entropy and Kullbacks principle of minimum cross-entropy (minimum directed divergence) are shown to be uniquely correct methods for inductive inference when new information is given in the form of expected values. Previous justifications use intuitive arguments and rely on the properties of entropy and cross-entropy as information measures. The approach here assumes that reasonable methods of inductive inference should lead to consistent results when there are different ways of taking the same information into account (for example, in different coordinate system). This requirement is formalized as four consistency axioms. These are stated in terms of an abstract information operator and make no reference to information measures. It is proved that the principle of maximum entropy is correct in the following sense: maximizing any function but entropy will lead to inconsistency unless that function and entropy have identical maxima. In other words given information in the form of constraints on expected values, there is only one (distribution satisfying the constraints that can be chosen by a procedure that satisfies the consistency axioms; this unique distribution can be obtained by maximizing entropy. This result is established both directly and as a special case (uniform priors) of an analogous result for the principle of minimum cross-entropy. Results are obtained both for continuous probability densities and for discrete distributions.},
  keywords = {Entropy functions},
  file = {/home/marnix/Zotero/storage/FN2XENHL/Shore and Johnson - 1980 - Axiomatic derivation of the principle of maximum e.pdf;/home/marnix/Zotero/storage/7DTTRPC5/1056144.html;/home/marnix/Zotero/storage/HDA7IRCP/1056144.html}
}

@article{Silva2015,
  title = {Network {{Structure Analysis}} of the {{Brazilian Interbank Market}}},
  author = {Silva, Thiago Christiano and Rubens, Sergio and Souza, Stancato De and Tabak, Benjamin Miranda},
  year = {2015},
  journal = {Working Papers},
  volume = {391},
  number = {June},
  isbn = {1518-3548},
  keywords = {complex network,financial stability,interbank market,network analysis,systemic risk},
  file = {/home/marnix/Zotero/storage/9TXKLBE3/Silva et al. - 2015 - Network Structure Analysis of the Brazilian Interbank Market.pdf}
}

@techreport{Silva2015a,
  title = {Liquidity {{Performance Evaluation}} of the {{Brazilian Interbank Market}} Using a {{Network-Based Approach}}},
  author = {Silva, Thiago Christiano and {da Silva}, Marcos Soares and Tabak, Benjamin Miranda and {Others}},
  year = {2015},
  file = {/home/marnix/Zotero/storage/TJNMU5VK/Silva et al. - 2015 - Liquidity Performance Evaluation of the Brazilian Interbank Market using a Network-Based Approach.pdf}
}

@inproceedings{Simpson2020,
  title = {Marginalised Spectral Mixture Kernels with Nested Sampling},
  booktitle = {Third Symposium on Advances in Approximate Bayesian Inference},
  author = {Simpson, Fergus and Lalchand, Vidhi and Rasmussen, Carl Edward},
  year = {2020},
  file = {/home/marnix/Zotero/storage/VVHJTSVK/Simpson et al. - Marginalised Spectral Mixture Kernels with Nested .pdf}
}

@inproceedings{Singh2016,
  title = {Forensic Anthropometry from Voice: {{An}} Articulatory-Phonetic Approach},
  shorttitle = {Forensic Anthropometry from Voice},
  booktitle = {2016 39th {{International Convention}} on {{Information}} and {{Communication Technology}}, {{Electronics}} and {{Microelectronics}} ({{MIPRO}})},
  author = {Singh, R. and Raj, B. and Gencaga, D.},
  year = {2016},
  month = may,
  pages = {1375--1380},
  doi = {10.1109/MIPRO.2016.7522354},
  abstract = {This paper addresses a problem that is of paramount importance in solving crimes wherein voice may be key evidence, or the only evidence: that of describing the perpetrator. The term Forensic anthropometry from voice refers to the deduction of the speaker's physical dimensions from voice. There are multiple studies in the literature that approach this problem in different ways, many of which depend on the availability of sufficient volumes of speech for analysis. However, in the case of many voice-based crimes, the voice evidence available may be limited. In such cases it is especially advantageous to regard the recorded signal as comprising multiple pieces of evidence. In this paper, we show how this can be done. We explain why, for any anthropometric measurement from speech, it makes sense to consider the contributions of each articulatory-phonetic unit independently of others, and to aggregate the deductions from them only in the aftermath. This approach is based on the hypothesis that the relative evidence given by different compositional units of speech can be more indicative of the anthropometric factor being deduced, than the evidence derived from the aggregate voice signal. We explain the applicability of this approach through experiments on standard speech databases.},
  keywords = {Acoustics,aggregate voice signal,anthropometric factor,anthropometric measurement,anthropometry,Anthropometry,articulatory phonetic unit,compositional units,Context,digital forensics,forensic anthropometry,Forensics,Production,Speech,speech processing,standard speech databases,Tongue,voice-based crimes},
  file = {/home/marnix/Zotero/storage/H8FXTIUE/Singh et al. - 2016 - Forensic anthropometry from voice An articulatory.pdf;/home/marnix/Zotero/storage/HBBMIBN2/7522354.html}
}

@article{Sivia1992,
  title = {Molecular Spectroscopy and {{Bayesian}} Spectral Analysis---How Many Lines Are There?},
  author = {Sivia, D. S. and Carlile, C. J.},
  year = {1992},
  month = jan,
  journal = {The Journal of Chemical Physics},
  volume = {96},
  number = {1},
  pages = {170--178},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.462505},
  urldate = {2021-04-07},
  file = {/home/marnix/Zotero/storage/HHJNG9TR/Sivia and Carlile - 1992 - Molecular spectroscopy and Bayesian spectral analy.pdf;/home/marnix/Zotero/storage/872ZIALM/1.html}
}

@article{Sivia1994,
  title = {A {{Bayesian}} Approach to Extracting Structure-Factor Amplitudes from Powder Diffraction Data},
  author = {Sivia, D. S. and David, W. I. F.},
  year = {1994},
  month = nov,
  journal = {Acta Crystallographica Section A Foundations of Crystallography},
  volume = {50},
  number = {6},
  pages = {703--714},
  issn = {0108-7673},
  doi = {10.1107/S0108767394003235},
  urldate = {2021-01-08},
  file = {/home/marnix/Zotero/storage/7SSHLFNR/Sivia and David - 1994 - A Bayesian approach to extracting structure-factor.pdf}
}

@book{Sivia2006,
  title = {Data Analysis: A {{Bayesian}} Tutorial},
  shorttitle = {Data Analysis},
  author = {Sivia, D. S. and Skilling, J.},
  year = {2006},
  series = {Oxford Science Publications},
  edition = {2nd ed},
  publisher = {Oxford University Press},
  address = {Oxford ; New York},
  isbn = {978-0-19-856831-5},
  langid = {english},
  lccn = {QA279.5 .S55 2006},
  keywords = {Bayesian statistical decision theory,Maximum entropy method},
  file = {/home/marnix/Zotero/storage/YEQSPTFM/Sivia and Skilling - 2006 - Data analysis a Bayesian tutorial.pdf}
}

@article{Skilling1985,
  title = {Prior {{Probabilities}}},
  author = {Skilling, John},
  year = {1985},
  journal = {Synthese},
  volume = {63},
  number = {1},
  eprint = {20116116},
  eprinttype = {jstor},
  pages = {1--34},
  publisher = {Springer},
  issn = {0039-7857},
  urldate = {2022-05-23},
  abstract = {The theoretical construction and practical use of prior probabilities, in particular for systems having many degrees of freedom are investigated. It becomes clear that it is operationally unsound to use mutually consistent priors if one wishes to draw sensible conclusions from practical experiments. The prior cannot usefully be identified with a state of knowledge, and indeed it is not so identified in common scientific practice. Rather, it can be identified with the question one asks. Accordingly, priors are free constructions. Their informal ill-defined and subjective characteristics must carry over into the conclusions one chooses to draw from experiments or observations.},
  file = {/home/marnix/Zotero/storage/8MHVRT8M/Skilling - 1985 - Prior Probabilities.pdf}
}

@book{Skilling1989,
  title = {Maximum {{Entropy}} and {{Bayesian Methods}}: {{Cambridge}}, {{England}}, 1988},
  shorttitle = {Maximum {{Entropy}} and {{Bayesian Methods}}},
  editor = {Skilling, J.},
  year = {1989},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-015-7860-8},
  urldate = {2025-09-02},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-90-481-4044-2 978-94-015-7860-8},
  langid = {english},
  file = {/home/marnix/Zotero/storage/L4ZRRNWF/Skilling - 1989 - Maximum Entropy and Bayesian Methods Cambridge, England, 1988.pdf}
}

@incollection{Skilling1989a,
  title = {The {{Eigenvalues}} of {{Mega-dimensional Matrices}}},
  booktitle = {Maximum {{Entropy}} and {{Bayesian Methods}}},
  author = {Skilling, John},
  editor = {Skilling, J.},
  year = {1989},
  pages = {455--466},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-015-7860-8_48},
  urldate = {2025-09-02},
  abstract = {Often, we need to know some integral property of the eigenvalues \{X\} of a large N x N symmetric matrix A. For example, determinants det(A) = exp({\textasciitilde} log (x)) playa role in the classic maximum entropy algorithm [Gull, 1988]. Likewise in physics, the specific heat of a system is a temperature- -dependent sum over the eigenvalues of the Hamiltonian matrix. However, the matrix may be so large that direct 0 (N 3 ) calculation of all N eigenvalues is prohibited. Indeed, if A is coded as a "fast" procedure, then 0 (N 2 ) operations may also be prohibited. Then the only permitted use of A is to apply it to one or a few vectors vo, VI, V2, .... We use the resulting vectors in an entropic Bayesian algorithm to estimate the eigenvalue spectrum of A, and thence its integral properties.},
  isbn = {978-90-481-4044-2 978-94-015-7860-8},
  langid = {english},
  file = {/home/marnix/Zotero/storage/D5XUPPGV/Skilling - 1989 - The Eigenvalues of Mega-dimensional Matrices.pdf}
}

@incollection{Skilling1993,
  title = {Bayesian {{Numerical Analysis}}},
  booktitle = {Physics and {{Probability}}},
  author = {Skilling, John},
  editor = {Grandy, Jr, W. T. and Milonni, P. W.},
  year = {1993},
  month = sep,
  edition = {1},
  pages = {207--222},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511524448.020},
  urldate = {2025-10-06},
  copyright = {https://www.cambridge.org/core/terms},
  isbn = {978-0-521-43471-3 978-0-521-61710-9 978-0-511-52444-8}
}

@inproceedings{Skilling2004,
  title = {Nested {{Sampling}}},
  booktitle = {{{AIP Conference Proceedings}}},
  author = {Skilling, John},
  year = {2004},
  volume = {735},
  pages = {395--405},
  publisher = {AIP},
  address = {Garching (Germany)},
  issn = {0094243X},
  doi = {10.1063/1.1835238},
  urldate = {2021-01-19},
  langid = {english},
  file = {/home/marnix/Zotero/storage/94HYLYAK/Skilling - 2004 - Nested Sampling.pdf}
}

@inproceedings{Skilling2005,
  title = {Bayesics},
  booktitle = {{{AIP}} Conference Proceedings},
  author = {Skilling, John},
  year = {2005},
  volume = {803},
  pages = {3--24},
  organization = {American Institute of Physics},
  file = {/home/marnix/Zotero/storage/SPKYHI4Y/Skilling - 2005 - Bayesics.pdf}
}

@article{Skilling2006,
  title = {Nested Sampling for General {{Bayesian}} Computation},
  author = {Skilling, John},
  year = {2006},
  month = dec,
  journal = {Bayesian Analysis},
  volume = {1},
  number = {4},
  pages = {833--859},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/06-BA127},
  urldate = {2019-05-18},
  abstract = {Nested sampling estimates directly how the likelihood function relates to prior mass. The evidence (alternatively the marginal likelihood, marginal density of the data, or the prior predictive) is immediately obtained by summation. It is the prime result of the computation, and is accompanied by an estimate of numerical uncertainty. Samples from the posterior distribution are an optional by-product, obtainable for any temperature. The method relies on sampling within a hard constraint on likelihood value, as opposed to the softened likelihood of annealing methods. Progress depends only on the shape of the "nested" contours of likelihood, and not on the likelihood values. This invariance (over monotonic re-labelling) allows the method to deal with a class of phase-change problems which effectively defeat thermal annealing.},
  langid = {english},
  mrnumber = {MR2282208},
  zmnumber = {1332.62374},
  keywords = {algorithm,annealing,Bayesian computation,evidence,marginal likelihood,model selection,nest,phase change},
  file = {/home/marnix/Zotero/storage/7JAF2ZFX/Skilling - 2006 - Nested sampling for general Bayesian computation.pdf;/home/marnix/Zotero/storage/66EJ2NHB/1340370944.html}
}

@article{Skilling2008,
  title = {This {{Physicist}}'s View of {{Gelman}}'s {{Bayes}}},
  author = {Skilling, John},
  year = {2008},
  journal = {Bayesian Analysis},
  file = {/home/marnix/Zotero/storage/4S9EET5H/Skilling - 2008 - This Physicist’s view of Gelman’s Bayes.pdf;/home/marnix/Zotero/storage/A74HFPE9/Skilling - 2008 - This Physicist’s view of Gelman’s Bayes.pdf}
}

@inproceedings{Skilling2009,
  title = {Nested {{Sampling}}'s {{Convergence}}},
  booktitle = {{{BAYESIAN INFERENCE AND MAXIMUM ENTROPY METHODS IN SCIENCE AND ENGINEERING}}: {{The}} 29th {{International Workshop}} on {{Bayesian Inference}} and {{Maximum Entropy Methods}} in {{Science}} and {{Engineering}}},
  author = {Skilling, John},
  year = {2009},
  pages = {277--291},
  address = {Oxford (Mississippi)},
  doi = {10.1063/1.3275625},
  urldate = {2021-09-06},
  file = {/home/marnix/Zotero/storage/QXWGJM3E/Skilling et al. - 2009 - Nested Sampling’s Convergence.pdf}
}

@inproceedings{Skilling2009a,
  title = {Conjugate {{Gradient}} for {{Bayesian Computation}}},
  booktitle = {{{BAYESIAN INFERENCE AND MAXIMUM ENTROPY METHODS IN SCIENCE AND ENGINEERING}}: {{The}} 29th {{International Workshop}} on {{Bayesian Inference}} and {{Maximum Entropy Methods}} in {{Science}} and {{Engineering}}},
  author = {Skilling, John},
  year = {2009},
  pages = {269--276},
  address = {Oxford (Mississippi)},
  doi = {10.1063/1.3275624},
  urldate = {2022-10-29},
  abstract = {Data analysis suffers the curses of dimensionality and ill-conditioning. These are often connected, incomplete data being an extreme form of ill-conditioning. The conjugate gradient algorithm models the parameter-to-data transform to yield a convergent approximation that can largely compensate for ill-conditioning. Long seen as restricted to optimisation, conjugate gradient can in fact be used to focus MCMC exploration, with potential gains in power.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/D9Q9P45A/Skilling - 2009 - Conjugate Gradient for Bayesian Computation.pdf}
}

@article{Skilling2012,
  title = {Bayesian Computation in Big Spaces-Nested Sampling and {{Galilean Monte Carlo}}},
  author = {Skilling, John},
  year = {2012},
  month = may,
  journal = {AIP Conference Proceedings},
  volume = {1443},
  number = {1},
  pages = {145},
  issn = {0094-243X},
  doi = {10.1063/1.3703630},
  urldate = {2019-10-30},
  abstract = {We hold this truth to be self-evident, that good principle and good practice go hand in hand. The principles of Bayesian analysis derive from elementary symmetries, and nothing more. In sympathy with those same symmetries, and noting that every invariance broken is generality forgone, we develop the practice of Bayesian computation. This approach leads to nested sampling and Galilean Monte Carlo. Nested sampling is the canonical prior-to-posterior compression algorithm, and Galilean Monte Carlo (GMC) is the canonical multidimensional exploration strategy. Though inspired by high dimension, these general methods apply to problems of all size.We hold this truth to be self-evident, that good principle and good practice go hand in hand. The principles of Bayesian analysis derive from elementary symmetries, and nothing more. In sympathy with those same symmetries, and noting that every invariance broken is generality forgone, we develop the practice of Bayesian computation. This approach leads to nested sampling and Galilean Monte Carlo. Nested sampling is the canonical prior-to-posterior compression algorithm, and Galilean Monte Carlo (GMC) is the canonical multidimensional exploration strategy. Though inspired by high dimension, these general methods apply to problems of all size.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/DLM33F9S/Skilling - 2012 - Bayesian computation in big spaces-nested sampling.pdf;/home/marnix/Zotero/storage/AZUJY7YG/1.html}
}

@inproceedings{Skilling2017,
  title = {David {{MacKay}}'s Wooden Blocks},
  booktitle = {{{BAYESIAN INFERENCE AND MAXIMUM ENTROPY METHODS IN SCIENCE AND ENGINEERING}}: {{Proceedings}} of the 36th {{International Workshop}} on {{Bayesian Inference}} and {{Maximum Entropy Methods}} in {{Science}} and {{Engineering}} ({{MaxEnt}} 2016)},
  author = {Skilling, John},
  year = {2017},
  pages = {110002},
  address = {Ghent, Belgium},
  doi = {10.1063/1.4985377},
  urldate = {2022-12-02},
  file = {/home/marnix/Zotero/storage/LSCECV6B/Skilling - 2017 - David MacKay’s wooden blocks.pdf}
}

@inproceedings{Skilling2019,
  title = {Galilean and Hamiltonian Monte Carlo},
  booktitle = {Multidisciplinary Digital Publishing Institute Proceedings},
  author = {Skilling, John},
  year = {2019},
  volume = {33},
  number = {1},
  pages = {19},
  file = {/home/marnix/Zotero/storage/5INTTLKP/Skilling - 2019 - Galilean and hamiltonian monte carlo.pdf}
}

@article{Skilling2019a,
  title = {The {{Symmetrical Foundation}} of {{Measure}}, {{Probability}}, and {{Quantum Theories}}},
  author = {Skilling, John and Knuth, Kevin H.},
  year = {2019},
  journal = {Annalen der Physik},
  volume = {531},
  number = {3},
  pages = {1800057},
  issn = {1521-3889},
  doi = {10.1002/andp.201800057},
  urldate = {2020-08-21},
  abstract = {Quantification starts with sum and product rules that express combination and partition. These rules rest on elementary symmetries that have wide applicability, which explains why arithmetical adding up and splitting into proportions are ubiquitous. Specifically, measure theory formalizes addition, and probability theory formalizes inference in terms of proportions. Quantum theory rests on the same simple symmetries, but is formalized in two dimensions, not just one, in order to track an object through its binary interactions with other objects. The symmetries still require sum and product rules (here known as the Feynman rules), but they apply to complex numbers instead of real scalars, with observable probabilities being modulus squared (known as the Born rule). The standard quantum formalism follows. There is no mystery or weirdness, just ordinary probabilistic inference.},
  copyright = {{\copyright} 2018 WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
  langid = {english},
  keywords = {associativity,Bayesian probability theory,Feynman rules,measure theory,probability theory,quantum foundations,quantum theory},
  file = {/home/marnix/Zotero/storage/5M22FJJ4/Skilling and Knuth - 2019 - The Symmetrical Foundation of Measure, Probability.pdf;/home/marnix/Zotero/storage/MA56XBA8/andp.html}
}

@article{Skilling2020,
  title = {The {{Arithmetic}} of {{Uncertainty}} Unifies {{Quantum Formalism}} and {{Relativistic Spacetime}}},
  author = {Skilling, John and Knuth, Kevin},
  year = {2020},
  month = dec,
  publisher = {Preprints},
  doi = {10.20944/preprints202012.0603.v1},
  urldate = {2021-01-04},
  abstract = {The theories of quantum mechanics and relativity dramatically altered our understanding of the universe ushering in the era of modern physics. Quantum theory deals with objects probabilistically at small scales, whereas relativity deals classically with motion in space and time. We show here that the mathematical structures of quantum theory and of relativity follow together from pure thought, defined and uniquely constrained by the same elementary ``combining and sequencing'' symmetries that underlie standard arithmetic and probability. The key is uncertainty, which inevitably accompanies observation of quantity and imposes the use of pairs of numbers. The symmetries then lead directly to the use of complex {\textbackslash}sqrt\{-1\} arithmetic, the standard calculus of quantum mechanics, and the Lorentz transformations of relativistic spacetime. One dimension of time and three dimensions of space are thus derived as the profound and inevitable framework of physics.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/TALY45RY/Skilling and Knuth - 2020 - The Arithmetic of Uncertainty unifies Quantum Form.pdf;/home/marnix/Zotero/storage/REXYSBPN/v1.html}
}

@article{Skilling2021,
  title = {The {{ABC}} of {{Physics}}},
  author = {Skilling, John and Knuth, Kevin},
  year = {2021},
  month = dec,
  journal = {Physical Sciences Forum},
  volume = {3},
  number = {1},
  pages = {9},
  issn = {2673-9984},
  doi = {10.3390/psf2021003009},
  urldate = {2021-12-14},
  abstract = {Why quantum? Why spacetime? We find that the key idea underlying both is uncertainty. In a world lacking probes of unlimited delicacy, our knowledge of quantities is necessarily accompanied by uncertainty. Consequently, physics requires a calculus of number pairs and not only scalars for quantity alone. Basic symmetries of shuffling and sequencing dictate that pairs obey ordinary component-wise addition, but they can have three different multiplication rules. We call those rules A, B and C. ``A'' shows that pairs behave as complex numbers, which is why quantum theory is complex. However, consistency with the ordinary scalar rules of probability shows that the fundamental object is not a particle on its Hilbert sphere but a stream represented by a Gaussian distribution. ``B'' is then applied to pairs of complex numbers (qubits) and produces the Pauli matrices for which its operation defines the space of four vectors. ``C'' then allows integration of what can then be recognised as energy-momentum into time and space. The picture is entirely consistent. Spacetime is a construct of quantum and not a container for it.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/KCTG23FJ/Skilling and Knuth - 2021 - The ABC of Physics.pdf}
}

@article{Skilling2024,
  title = {Nested {{Sampling}}---{{The Idea}}},
  author = {Skilling, John},
  year = {2024},
  journal = {Physical Sciences Forum},
  volume = {9},
  number = {1},
  pages = {22},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2673-9984},
  doi = {10.3390/psf2023009022},
  urldate = {2024-01-12},
  abstract = {We seek to add up Q={$\int$}fdX over unit volume in arbitrary dimension. Nested sampling locates the bulk of Q by geometrical compression, using a Monte Carlo ensemble constrained within a progressively more restrictive lower limit f{$\leq$}f*. This domain is divided into a core f{$>$}f* and a shell f=f*, with the core kept adequately populated.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {nested sampling},
  file = {/home/marnix/Zotero/storage/SKBAZREA/Skilling - 2024 - Nested Sampling—The Idea.pdf}
}

@article{Slater1946,
  title = {Physics and the Wave Equation},
  author = {Slater, John Clarke},
  year = {1946},
  journal = {Bulletin of the American Mathematical Society},
  volume = {52},
  number = {5},
  pages = {392--400},
  file = {/home/marnix/Zotero/storage/BPSBA35H/Slater - 1946 - Physics and the wave equation.pdf}
}

@article{Smerlak2016,
  title = {Minimum Relative Entropy Distributions with a Large Mean Are {{Gaussian}}},
  author = {Smerlak, Matteo},
  year = {2016},
  month = dec,
  journal = {Physical Review E},
  volume = {94},
  number = {6},
  eprint = {1605.08259},
  pages = {062107},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.94.062107},
  urldate = {2022-02-14},
  abstract = {We consider the following frustrated optimization problem: given a prior probability distribution \$q\$, find the distribution \$p\$ minimizing the relative entropy with respect to \$q\$ such that \${\textbackslash}textrm\{mean\}(p)\$ is fixed and large. We show that solutions to this problem are asymptotically Gaussian. As an application we derive an \$H\$-type theorem for evolutionary dynamics: the entropy of the (standardized) distribution of fitness of a population evolving under natural selection is eventually increasing.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Statistical Mechanics,Mathematics - Probability,Mathematics - Statistics Theory,Quantitative Biology - Populations and Evolution},
  file = {/home/marnix/Zotero/storage/APU9EE64/Smerlak - 2016 - Minimum relative entropy distributions with a larg.pdf;/home/marnix/Zotero/storage/HHU36HWT/1605.html}
}

@article{Smith2006,
  title = {Efficient Auditory Coding},
  author = {Smith, Evan C. and Lewicki, Michael S.},
  year = {2006},
  journal = {Nature},
  volume = {439},
  number = {7079},
  eprint = {1011.1669v3},
  pages = {978--982},
  issn = {14764687},
  doi = {10.1038/nature04485},
  abstract = {The auditory neural code must serve a wide range of auditory tasks that require great sensitivity in time and frequency and be effective over the diverse array of sounds present in natural acoustic environments. It has been suggested that sensory systems might have evolved highly efficient coding strategies to maximize the information conveyed to the brain while minimizing the required energy and neural resources. Here we show that, for natural sounds, the complete acoustic waveform can be represented efficiently with a nonlinear model based on a population spike code. In this model, idealized spikes encode the precise temporal positions and magnitudes of underlying acoustic features. We find that when the features are optimized for coding either natural sounds or speech, they show striking similarities to time-domain cochlear filter estimates, have a frequency-bandwidth dependence similar to that of auditory nerve fibres, and yield significantly greater coding efficiency than conventional signal representations. These results indicate that the auditory code might approach an information theoretic optimum and that the acoustic structure of speech might be adapted to the coding capacity of the mammalian auditory system.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1011.1669v3},
  isbn = {1476-4687 (Electronic)\r0028-0836 (Linking)},
  pmid = {16495999},
  file = {/home/marnix/Zotero/storage/ZSVAT3JL/Smith2006 Efficient auditory coding.pdf}
}

@article{Smith2014,
  title = {Models of Language Evolution and Change: {{Language}} Evolution and Change},
  shorttitle = {Models of Language Evolution and Change},
  author = {Smith, Andrew D.M.},
  year = {2014},
  month = may,
  journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
  volume = {5},
  number = {3},
  pages = {281--293},
  issn = {19395078},
  doi = {10.1002/wcs.1285},
  urldate = {2022-08-18},
  langid = {english},
  file = {/home/marnix/Zotero/storage/33DFHGCV/Smith - 2014 - Models of language evolution and change Language .pdf}
}

@inproceedings{Snelson2006,
  title = {Sparse {{Gaussian Processes}} Using {{Pseudo-inputs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Snelson, Edward and Ghahramani, Zoubin},
  year = {2006},
  volume = {18},
  publisher = {MIT Press},
  urldate = {2021-11-05},
  file = {/home/marnix/Zotero/storage/3CX944BB/Snelson and Ghahramani - 2006 - Sparse Gaussian Processes using Pseudo-inputs.pdf}
}

@article{Sokal1958,
  title = {A Statistical Method for Evaluating Systematic Relationships},
  author = {Sokal, R R and Michener, C D},
  year = {1958},
  journal = {University of Kansas Scientific Bulletin},
  volume = {28},
  pages = {1409--1438},
  keywords = {method,phylogeny,upgma}
}

@article{Sole-Ribalta2013,
  title = {Spectral Properties of the {{Laplacian}} of Multiplex Networks},
  author = {{Sol{\'e}-Ribalta}, A. and De Domenico, M. and Kouvaris, N. E. and {\'D}iaz-Guilera, A. and G{\'o}mez, S. and Arenas, A.},
  year = {2013},
  journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
  volume = {88},
  number = {3},
  eprint = {1307.2090},
  pages = {1--8},
  issn = {15393755},
  doi = {10.1103/PhysRevE.88.032807},
  abstract = {One of the more challenging tasks in the understanding of dynamical properties of models on top of complex networks is to capture the precise role of multiplex topologies. In a recent paper, Gomez et al. [Phys. Rev. Lett. 101, 028701 (2013)] proposed a framework for the study of diffusion processes in such networks. Here, we extend the previous framework to deal with general configurations in several layers of networks, and analyze the behavior of the spectrum of the Laplacian of the full multiplex. We derive an interesting decoupling of the problem that allow us to unravel the role played by the interconnections of the multiplex in the dynamical processes on top of them. Capitalizing on this decoupling we perform an asymptotic analysis that allow us to derive analytical expressions for the full spectrum of eigenvalues. This spectrum is used to gain insight into physical phenomena on top of multiplex, specifically, diffusion processes and synchronizability.},
  archiveprefix = {arXiv},
  arxivid = {1307.2090},
  isbn = {1539-3755},
  file = {/home/marnix/Zotero/storage/2RCSS4QJ/Solé-Ribalta et al. - 2013 - Spectral properties of the Laplacian of multiplex networks.pdf}
}

@inproceedings{Solin2014,
  title = {Explicit {{Link Between Periodic Covariance Functions}} and {{State Space Models}}},
  booktitle = {Proceedings of the {{Seventeenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Solin, Arno and S{\"a}rkk{\"a}, Simo},
  year = {2014},
  month = apr,
  pages = {904--912},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2025-04-25},
  abstract = {This paper shows how periodic covariance functions in Gaussian process regression can be reformulated as state space models, which can be solved with classical Kalman filtering theory. This reduces the problematic cubic complexity of Gaussian process regression in the number of time steps into linear time complexity. The representation is based on expanding periodic covariance functions into a series of stochastic resonators. The explicit representation of the canonical periodic covariance function is written out and the expansion is shown to uniformly converge to the exact covariance function with a known convergence rate. The framework is generalized to quasi-periodic covariance functions by introducing damping terms in the system and applied to two sets of real data. The approach could be easily extended to non-stationary and spatio-temporal variants.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/7VMAZDN4/Solin and Särkkä - 2014 - Explicit Link Between Periodic Covariance Functions and State Space Models.pdf}
}

@inproceedings{Solin2018,
  title = {Infinite-{{Horizon Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Solin, Arno and Hensman, James and Turner, Richard E},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-04-25},
  abstract = {Gaussian processes provide a flexible framework for forecasting, removing noise, and interpreting long temporal datasets. State space modelling (Kalman filtering) enables these non-parametric models to be deployed on long datasets by reducing the complexity to linear in the number of data points. The complexity is still cubic in the state dimension m which is an impediment to practical application. In certain special cases (Gaussian likelihood, regular spacing) the GP posterior will reach a steady posterior state when the data are very long. We leverage this and formulate an inference scheme for GPs with general likelihoods, where inference is based on single-sweep EP (assumed density filtering). The infinite-horizon model tackles the cubic cost in the state dimensionality and reduces the cost in the state dimension m to O(m{\textasciicircum}2) per data point. The model is extended to online-learning of hyperparameters. We show examples for large finite-length modelling problems, and present how the method runs in real-time on a smartphone on a continuous data stream updated at 100 Hz.},
  file = {/home/marnix/Zotero/storage/L7VQGH9G/Solin et al. - 2018 - Infinite-Horizon Gaussian Processes.pdf}
}

@article{Solin2020,
  title = {Hilbert Space Methods for Reduced-Rank {{Gaussian}} Process Regression},
  author = {Solin, Arno and S{\"a}rkk{\"a}, Simo},
  year = {2020},
  month = mar,
  journal = {Statistics and Computing},
  volume = {30},
  number = {2},
  pages = {419--446},
  issn = {1573-1375},
  doi = {10.1007/s11222-019-09886-w},
  urldate = {2021-11-08},
  abstract = {This paper proposes a novel scheme for reduced-rank Gaussian process regression. The method is based on an approximate series expansion of the covariance function in terms of an eigenfunction expansion of the Laplace operator in a compact subset of \$\${\textbackslash}mathbb \{R\}{\textasciicircum}d\$\$. On this approximate eigenbasis, the eigenvalues of the covariance function can be expressed as simple functions of the spectral density of the Gaussian process, which allows the GP inference to be solved under a computational cost scaling as \$\${\textbackslash}mathcal \{O\}(nm{\textasciicircum}2)\$\$ (initial) and \$\${\textbackslash}mathcal \{O\}(m{\textasciicircum}3)\$\$ (hyperparameter learning) with m basis functions and n data points. Furthermore, the basis functions are independent of the parameters of the covariance function, which allows for very fast hyperparameter learning. The approach also allows for rigorous error analysis with Hilbert space theory, and we show that the approximation becomes exact when the size of the compact subset and the number of eigenfunctions go to infinity. We also show that the convergence rate of the truncation error is independent of the input dimensionality provided that the differentiability order of the covariance function increases appropriately, and for the squared exponential covariance function it is always bounded by \$\$\{{\textbackslash}sim \}1/m\$\$ regardless of the input dimensionality. The expansion generalizes to Hilbert spaces with an inner product which is defined as an integral over a specified input density. The method is compared to previously proposed methods theoretically and through empirical tests with simulated and real data.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/8EMYBK3E/Solin and Särkkä - 2020 - Hilbert space methods for reduced-rank Gaussian pr.pdf}
}

@inproceedings{Sollich2004,
  title = {Using the {{Equivalent Kernel}} to {{Understand Gaussian Process Regression}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sollich, Peter and Williams, Christopher},
  year = {2004},
  volume = {17},
  publisher = {MIT Press},
  urldate = {2024-02-27},
  abstract = {The equivalent kernel [1] is a way of understanding how Gaussian pro-             cess regression works for large sample sizes based on a continuum limit.             In this paper we show (1) how to approximate the equivalent kernel of the             widely-used squared exponential (or Gaussian) kernel and related ker-             nels, and (2) how analysis using the equivalent kernel helps to understand             the learning curves for Gaussian processes.},
  file = {/home/marnix/Zotero/storage/8R8XKA8A/Sollich and Williams - 2004 - Using the Equivalent Kernel to Understand Gaussian.pdf}
}

@article{Soramaki2007,
  title = {The Topology of Interbank Payment Flows},
  author = {Soram{\"a}ki, Kimmo and Bech, Morten L and Arnold, Jeffrey and Glass, Robert J and Beyeler, Walter E},
  year = {2007},
  month = jun,
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {379},
  number = {1},
  pages = {317--333},
  issn = {03784371},
  doi = {10.1016/j.physa.2006.11.093},
  abstract = {We explore the network topology of the interbank payments transferred between commercial banks over the Fedwire{\textregistered} Funds Service. We find that the network has both a low average path length and low connectivity. The network includes a tightly connected core of banks to which most other banks connect. The degree distribution is scale free over a substantial range. We find that the properties of the network changed considerably in the immediate aftermath of the events of September 11, 2001.},
  keywords = {financial-networks,systemic-risk,topology}
}

@article{Sorber2012,
  title = {Unconstrained Optimization of Real Functions in Complex Variables},
  author = {Sorber, Laurent and Barel, Marc Van and Lathauwer, Lieven De},
  year = {2012},
  journal = {SIAM Journal on Optimization},
  volume = {22},
  number = {3},
  pages = {879--898},
  publisher = {SIAM},
  file = {/home/marnix/Zotero/storage/9UN5SXKR/Sorber et al. - 2012 - Unconstrained optimization of real functions in co.pdf}
}

@article{Sorbye2017,
  title = {Penalised {{Complexity Priors}} for {{Stationary Autoregressive Processes}}},
  author = {S{\o}rbye, Sigrunn Holbek and Rue, H{\aa}vard},
  year = {2017},
  month = nov,
  journal = {Journal of Time Series Analysis},
  volume = {38},
  number = {6},
  pages = {923--935},
  issn = {0143-9782, 1467-9892},
  doi = {10.1111/jtsa.12242},
  urldate = {2025-05-13},
  abstract = {The autoregressive process of order p (AR(p)) is a central model in time series analysis. A Bayesian approach requires the user to define a prior distribution for the coefficients of the AR(p) model. Although it is easy to write down some prior, it is not at all obvious how to understand and interpret the prior distribution, to ensure that it behaves according to the users prior knowledge. In this paper, we approach this problem using the recently developed ideas of penalised complexity (PC) priors. These prior have important properties like robustness and invariance to reparameterisations, as well as a clear interpretation. A PC prior is computed based on specific principles, where model component complexity is penalised in terms of deviation from simple base model formulations. In the AR(1) case, we discuss two natural base model choices, corresponding to either independence in time or no change in time. The latter case is illustrated in a survival model with possible timedependent frailty. For higher-order processes, we propose a sequential approach, where the base model for AR(p) is the corresponding AR(p - 1) model expressed using the partial autocorrelations. The properties of the new prior distribution is compared with the reference prior in a simulation study.},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  file = {/home/marnix/Zotero/storage/5RRJBD3T/Sørbye and Rue - 2017 - Penalised Complexity Priors for Stationary Autoregressive Processes.pdf}
}

@article{Speagle2019,
  title = {Dynesty: {{A Dynamic Nested Sampling Package}} for {{Estimating Bayesian Posteriors}} and {{Evidences}}},
  shorttitle = {Dynesty},
  author = {Speagle, Joshua S.},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.02180 [astro-ph, stat]},
  eprint = {1904.02180},
  primaryclass = {astro-ph, stat},
  urldate = {2020-01-30},
  abstract = {We present dynesty, a public, open-source, Python package to estimate Bayesian posteriors and evidences (marginal likelihoods) using Dynamic Nested Sampling. By adaptively allocating samples based on posterior structure, Dynamic Nested Sampling has the benefits of Markov Chain Monte Carlo algorithms that focus exclusively on posterior estimation while retaining Nested Sampling's ability to estimate evidences and sample from complex, multi-modal distributions. We provide an overview of Nested Sampling, its extension to Dynamic Nested Sampling, the algorithmic challenges involved, and the various approaches taken to solve them. We then examine dynesty's performance on a variety of toy problems along with several astronomical applications. We find in particular problems dynesty can provide substantial improvements in sampling efficiency compared to popular MCMC approaches in the astronomical literature. More detailed statistical results related to Nested Sampling are also included in the Appendix.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Statistics - Computation},
  file = {/home/marnix/Zotero/storage/7M9Y96TY/Speagle - 2019 - dynesty A Dynamic Nested Sampling Package for Est.pdf;/home/marnix/Zotero/storage/W5KLJV2Q/1904.html}
}

@article{Speed2008,
  title = {Network Inference Using Informative Priors},
  author = {Mukherjee, Sach and Speed, Terence P.},
  year = {2008},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {105},
  number = {38},
  pages = {14313--14318},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.0802272105},
  urldate = {2022-09-16},
  file = {/home/marnix/Zotero/storage/5Z3S9F9W/Mukherjee and Speed - 2008 - Network inference using informative priors.pdf}
}

@article{Spiller2023,
  title = {The {{Zero Problem}}: {{Gaussian Process Emulators}} for {{Range-Constrained Computer Models}}},
  shorttitle = {The {{Zero Problem}}},
  author = {Spiller, Elaine T. and Wolpert, Robert L. and Tierz, Pablo and Asher, Taylor G.},
  year = {2023},
  month = jun,
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  volume = {11},
  number = {2},
  pages = {540--566},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/21M1467420},
  urldate = {2024-01-07},
  abstract = {Direct coupling of computer models is often difficult for computational and logistical reasons. We propose coupling computer models by linking independently developed Gaussian process emulators (GaSPs) of these models. Linked emulators are developed that are closed form, namely normally distributed with closed form predictive mean and variance functions. These are compared with a more direct emulation strategy, namely running the coupled computer models and directly emulating the system; perhaps surprisingly, this direct emulator was inferior in all illustrations. Pedagogical examples are given as well as an application to coupling of real computer models.},
  file = {/home/marnix/Zotero/storage/VWVKMX63/The Zero Problem Gaussian Process Emulators for R.pdf}
}

@book{Squartini2017,
  title = {Maximum-{{Entropy Networks}}: {{Pattern Detection}}, {{Network Reconstruction}} and {{Graph Combinatorics}}},
  shorttitle = {Maximum-{{Entropy Networks}}},
  author = {Squartini, Tiziano and Garlaschelli, Diego},
  year = {2017},
  series = {{{SpringerBriefs}} in {{Complexity}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-69438-2},
  urldate = {2021-12-14},
  isbn = {978-3-319-69436-8 978-3-319-69438-2},
  langid = {english},
  file = {/home/marnix/Zotero/storage/AT3TV5NM/Squartini and Garlaschelli - 2017 - Maximum-Entropy Networks Pattern Detection, Netwo.pdf}
}

@article{Stanley2016,
  title = {Clustering {{Network Layers}} with the {{Strata Multilayer Stochastic Block Model}}},
  author = {Stanley, Natalie and Shai, Saray and Taylor, Dane and Mucha, Peter J.},
  year = {2016},
  journal = {IEEE Transactions on Network Science and Engineering},
  volume = {3},
  number = {2},
  eprint = {1507.01826},
  pages = {95--105},
  issn = {23274697},
  doi = {10.1109/TNSE.2016.2537545},
  abstract = {Multilayer networks are a useful data structure for simultaneously capturing multiple types of relationships between a set of nodes. In such networks, each relational definition gives rise to a layer. While each layer provides its own set of information, community structure across layers can be collectively utilized to discover and quantify underlying relational patterns between nodes. To concisely extract information from a multilayer network, we propose to identify and combine sets of layers with meaningful similarities in community structure. In this paper, we describe the "strata multilayer stochastic block model'' (sMLSBM), a probabilistic model for multilayer community structure. The central extension of the model is that there exist groups of layers, called "strata'', which are defined such that all layers in a given stratum have community structure described by a common stochastic block model (SBM). That is, layers in a stratum exhibit similar node-to-community assignments and SBM probability parameters. Fitting the sMLSBM to a multilayer network provides a joint clustering that yields node-to-community and layer-to-stratum assignments, which cooperatively aid one another during inference. We describe an algorithm for separating layers into their appropriate strata and an inference technique for estimating the SBM parameters for each stratum. We demonstrate our method using synthetic networks and a multilayer network inferred from data collected in the Human Microbiome Project.},
  archiveprefix = {arXiv},
  arxivid = {1507.01826},
  keywords = {Clustering,Stochastic Block Models,Strata},
  file = {/home/marnix/Zotero/storage/8QSIM592/Stanley2015 Clustering network layers with the strata multilayer SBM.pdf}
}

@inproceedings{Stegle2011,
  title = {Efficient Inference in Matrix-Variate {{Gaussian}} Models with {\textbackslash}textbackslash Iid Observation Noise},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Stegle, Oliver and Lippert, Christoph and Mooij, Joris M and Lawrence, Neil and Borgwardt, Karsten},
  year = {2011},
  volume = {24},
  publisher = {Curran Associates, Inc.},
  urldate = {2022-10-11},
  abstract = {Inference in matrix-variate Gaussian models has major applications for multi- output prediction and joint learning of row and column covariances from matrix- variate data. Here, we discuss an approach for efficient inference in such models that explicitly account for iid observation noise. Computational tractability can be retained by exploiting the Kronecker product between row and column covariance matrices. Using this framework, we show how to generalize the Graphical Lasso in order to learn a sparse inverse covariance between features while accounting for a low-rank confounding covariance between samples. We show practical utility on applications to biology, where we model covariances with more than 100,000 di- mensions. We find greater accuracy in recovering biological network structures and are able to better reconstruct the confounders.},
  file = {/home/marnix/Zotero/storage/PEIWC9LM/Stegle et al. - 2011 - Efficient inference in matrix-variate Gaussian mod.pdf}
}

@article{Steininger2019,
  title = {{{NIFTy}} 3--{{Numerical Information Field Theory}}: {{A Python Framework}} for {{Multicomponent Signal Inference}} on {{HPC Clusters}}},
  author = {Steininger, Theo and Dixit, Jait and Frank, Philipp and Greiner, Maksim and Hutschenreuter, Sebastian and Knollm{\"u}ller, Jakob and Leike, Reimar and Porqueres, Natalia and Pumpe, Daniel and Reinecke, Martin and others},
  year = {2019},
  journal = {Annalen der Physik},
  volume = {531},
  number = {3},
  pages = {1800290},
  publisher = {Wiley Online Library},
  file = {/home/marnix/Zotero/storage/9Q2MCD8Y/Steininger et al. - 2019 - NIFTy 3--Numerical Information Field Theory A Pyt.pdf}
}

@phdthesis{Steinruecken2014,
  title = {Lossless Data Compression},
  author = {Steinruecken, Christian},
  year = {2014},
  school = {University of Cambridge},
  file = {/home/marnix/Zotero/storage/9GBWYR5F/Steinruecken - Lossless Data Compression.pdf}
}

@incollection{Steinruecken2019,
  title = {The {{Automatic Statistician}}},
  booktitle = {Automated {{Machine Learning}}},
  author = {Steinruecken, Christian and Smith, Emma and Janz, David and Lloyd, James and Ghahramani, Zoubin},
  editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  year = {2019},
  pages = {161--173},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-05318-5_9},
  urldate = {2022-06-11},
  abstract = {The Automatic Statistician project aims to automate data science, producing predictions and human-readable reports from raw datasets with minimal human intervention. Alongside basic graphs and statistics, the generated reports contain a curation of high-level insights about the dataset that are obtained from (1) an automated construction of models for the dataset, (2) a comparison of these models, and (3) a software component that turns these results into natural language descriptions. This chapter describes the common architecture of such Automatic Statistician systems, and discusses some of the design decisions and technical challenges.},
  isbn = {978-3-030-05317-8 978-3-030-05318-5},
  langid = {english},
  file = {/home/marnix/Zotero/storage/89FBCLQD/Steinruecken et al. - 2019 - The Automatic Statistician.pdf}
}

@article{Stephens2000,
  title = {Dealing with Label Switching in Mixture Models},
  author = {Stephens, Matthew},
  year = {2000},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {62},
  number = {4},
  pages = {795--809},
  issn = {1467-9868},
  doi = {10.1111/1467-9868.00265},
  urldate = {2021-04-08},
  abstract = {In a Bayesian analysis of finite mixture models, parameter estimation and clustering are sometimes less straightforward than might be expected. In particular, the common practice of estimating parameters by their posterior mean, and summarizing joint posterior distributions by marginal distributions, often leads to nonsensical answers. This is due to the so-called `label switching' problem, which is caused by symmetry in the likelihood of the model parameters. A frequent response to this problem is to remove the symmetry by using artificial identifiability constraints. We demonstrate that this fails in general to solve the problem, and we describe an alternative class of approaches, relabelling algorithms, which arise from attempting to minimize the posterior expected loss under a class of loss functions. We describe in detail one particularly simple and general relabelling algorithm and illustrate its success in dealing with the label switching problem on two examples.},
  langid = {english},
  keywords = {Bayesian approach,Classification,Clustering,Identifiability,Markov chain Monte Carlo methods,Mixture model,Multimodal posterior},
  file = {/home/marnix/Zotero/storage/4TYTY854/Stephens - 2000 - Dealing with label switching in mixture models.pdf;/home/marnix/Zotero/storage/ZNX98UML/1467-9868.html}
}

@article{Stephenson2002,
  title = {Dynamic {{Bayesian Network Based Speech Recognition}} with {{Pitch}} and {{Energy}} as {{Auxiliary Variables}}},
  author = {{Stephenson}},
  year = {2002},
  file = {/home/marnix/Zotero/storage/RJFPX6LY/Stephenson - 2002 - Dynamic Bayesian Network Based Speech Recognition with Pitch and Energy as Auxiliary Variables.pdf}
}

@book{Stevens2000,
  title = {Acoustic Phonetics},
  author = {Stevens, Kenneth N},
  year = {2000},
  publisher = {MIT press}
}

@book{Stoica2005,
  title = {Spectral Analysis of Signals},
  author = {Stoica, Petre and Moses, Randolph L.},
  year = {2005},
  publisher = {Pearson/Prentice Hall},
  address = {Upper Saddle River, N.J},
  isbn = {978-0-13-113956-5},
  langid = {english},
  lccn = {QA320 .S864 2005},
  keywords = {Spectral theory (Mathematics)},
  file = {/home/marnix/Zotero/storage/WZLQG9HT/Stoica and Moses - 2005 - Spectral analysis of signals.pdf}
}

@article{Strachan2004,
  title = {Improper Priors with Well Defined {{Bayes Factors}}},
  author = {Strachan, Rodney and {van Dijk}, Herman},
  year = {2004},
  month = may,
  journal = {Report / Econometric Institute, Erasmus University Rotterdam},
  issn = {1566-7294},
  urldate = {2020-02-20},
  abstract = {A sensible Bayesian model selection or comparison strategy implies selecting the model with the highest posterior probability. While some improper priors have attractive properties such as, e.g., low frequentist risk, it is generally claimed that Bartlett's paradox implies that using improper priors for the parameters in alternative models results in Bayes factors that are not well defined, thus preventing model comparison in this case. In this paper we demonstrate this latter result is not generally true and expand the class of priors that may be used for computing posterior odds to include some improper priors. Our approach is to give a new representation of the issue of undefined Bayes factors and, from this representation, develop classes of improper priors from which well defined Bayes factors may be derived. This approach involves either augmenting or normalising the prior measure for the parameters. One of these classes of priors includes the well known and commonly employed shrinkage prior. Estimation of Bayes factors is demonstrated for a reduced rank model.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/RIY4XE6H/Strachan and van Dijk - 2004 - Improper priors with well defined Bayes Factors.pdf;/home/marnix/Zotero/storage/MFKE7LUE/1277.html}
}

@book{Strang1986,
  title = {Introduction to Applied Mathematics},
  author = {Strang, Gilbert},
  year = {1986},
  publisher = {Wesley-Cambridge Press},
  address = {Massachusetts},
  file = {/home/marnix/Zotero/storage/R4K3IN9Z/Strang - 1986 - Introduction to applied mathematics.djvu}
}

@book{Strang1996,
  title = {Wavelets and Filter Banks},
  author = {Strang, Gilbert and Nguyen, Truong},
  year = {1996},
  publisher = {SIAM},
  file = {/home/marnix/Zotero/storage/FH2QZSQ4/Strang and Nguyen - 1996 - Wavelets and filter banks.pdf}
}

@book{Strang2006,
  title = {Linear Algebra and Its Applications},
  author = {Strang, {\relax Gilbert}.},
  year = {2006},
  edition = {4},
  abstract = {"Renowned professor and author Gilbert Strang demonstrates that linear algebra is a fascinating subject by showing both its beauty and value. While the mathematics is there, the effort is not all concentrated on proofs. Strang's emphasis is on understanding. He explains concepts, rather than deduces. This book is written in an informal and personal style and teaches real mathematics. The gears change in Chapter 2 as students reach the introduction of vector spaces. Throughout the book, the theory is motivated and reinforced by genuine applications, allowing pure mathematicians to teach applied mathematics."--},
  isbn = {0-03-010567-6 978-0-03-010567-8 0-534-42200-4 978-0-534-42200-4},
  langid = {english},
  file = {/home/marnix/Zotero/storage/BMXUGE5S/Strang - 2006 - Linear algebra and its applications.pdf}
}

@book{Strang2019,
  title = {Linear Algebra and Learning from Data},
  author = {Strang, Gilbert},
  year = {2019},
  publisher = {Wellesley-Cambridge Press Cambridge},
  file = {/home/marnix/Zotero/storage/HQETTUX2/Strang - 2019 - Linear algebra and learning from data.pdf}
}

@article{Strelioff2007,
  title = {Inferring {{Markov}} Chains: {{Bayesian}} Estimation, Model Comparison, Entropy Rate, and out-of-Class Modeling},
  shorttitle = {Inferring {{Markov}} Chains},
  author = {Strelioff, Christopher C. and Crutchfield, James P. and H{\"u}bler, Alfred W.},
  year = {2007},
  month = jul,
  journal = {Physical Review E},
  volume = {76},
  number = {1},
  pages = {011106},
  doi = {10.1103/PhysRevE.76.011106},
  urldate = {2019-01-28},
  abstract = {Markov chains are a natural and well understood tool for describing one-dimensional patterns in time or space. We show how to infer kth order Markov chains, for arbitrary k, from finite data by applying Bayesian methods to both parameter estimation and model-order selection. Extending existing results for multinomial models of discrete data, we connect inference to statistical mechanics through information-theoretic (type theory) techniques. We establish a direct relationship between Bayesian evidence and the partition function which allows for straightforward calculation of the expectation and variance of the conditional relative entropy and the source entropy rate. Finally, we introduce a method that uses finite data-size scaling with model-order comparison to infer the structure of out-of-class processes.},
  file = {/home/marnix/Zotero/storage/AY4W6N5B/Strelioff et al. - 2007 - Inferring Markov chains Bayesian estimation, mode.pdf;/home/marnix/Zotero/storage/VFLDMBEQ/PhysRevE.76.html}
}

@book{Strutt1894,
  title = {The {{Theory}} of {{Sound}}},
  author = {Strutt, John William},
  year = {1894},
  edition = {2nd ed},
  publisher = {Dover},
  address = {New York, NY},
  abstract = {The Nobel Laureate's classic sums up all research in the field prior to 1877, then presents Rayleigh's own original contributions.Volume One covers harmonic vibrations, systems with one degree of freedom, vibrating systems in general, transverse vibrations of strings, longitudinal and torsional vibrations of bars, vibrations of membranes and plates, curved shells and plates, and electrical vibrations},
  isbn = {978-0-486-14043-8 978-0-486-60292-9},
  langid = {english},
  lccn = {534 RAY},
  keywords = {Other Fields of Physics,Sound,Vibration},
  file = {/home/marnix/Zotero/storage/NVXPRPCA/theorysound00raylgoog.pdf}
}

@inproceedings{Sun2020,
  title = {Neural {{Networks}} as {{Inter-Domain Inducing Points}}},
  booktitle = {Third {{Symposium}} on {{Advances}} in {{Approximate Bayesian Inference}}},
  author = {Sun, Shengyang and Shi, Jiaxin and Grosse, Roger Baker},
  year = {2020},
  month = nov,
  urldate = {2025-09-17},
  abstract = {Equivalences between infinite neural networks and Gaussian processes have been established for explaining the functional prior and training dynamics of deep learning models. In this paper we cast the hidden units of finite-width neural networks as the inter-domain inducing points of a kernel, then a one-hidden-layer network becomes a kernel regression model. For dot-product kernels on both \$R{\textasciicircum}d\$ and \$S{\textasciicircum}\{d-1\}\$, we derive the kernel functions for inducing points. Empirically we conduct toy experiments to validate the proposed approaches.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/SGZJGX37/Sun et al. - 2020 - Neural Networks as Inter-Domain Inducing Points.pdf}
}

@misc{Sun2021,
  title = {Scalable {{Variational Gaussian Processes}} via {{Harmonic Kernel Decomposition}}},
  author = {Sun, Shengyang and Shi, Jiaxin and Wilson, Andrew Gordon and Grosse, Roger},
  year = {2021},
  month = jun,
  number = {arXiv:2106.05992},
  eprint = {2106.05992},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.05992},
  urldate = {2025-09-17},
  abstract = {We introduce a new scalable variational Gaussian process approximation which provides a high fidelity approximation while retaining general applicability. We propose the harmonic kernel decomposition (HKD), which uses Fourier series to decompose a kernel as a sum of orthogonal kernels. Our variational approximation exploits this orthogonality to enable a large number of inducing points at a low computational cost. We demonstrate that, on a range of regression and classification problems, our approach can exploit input space symmetries such as translations and reflections, and it significantly outperforms standard variational methods in scalability and accuracy. Notably, our approach achieves state-of-the-art results on CIFAR-10 among pure GP models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/TGGBYFRE/Sun et al. - 2021 - Scalable Variational Gaussian Processes via Harmonic Kernel Decomposition.pdf;/home/marnix/Zotero/storage/3RM4NV42/2106.html}
}

@article{Sundberg1978,
  title = {Synthesis of Singing},
  author = {Sundberg, Johan},
  year = {1978},
  journal = {Swedish Journal of Musicology},
  number = {1},
  pages = {107--112}
}

@article{Sundberg1988,
  title = {Vocal Tract Resonance in Singing},
  author = {Sundberg, Johan},
  year = {1988},
  journal = {The NATS Journal},
  volume = {44},
  number = {4},
  pages = {11--20},
  file = {/home/marnix/Zotero/storage/S87464PU/Sundberg - 1988 - Vocal Tract Resonance In Singing.pdf}
}

@article{Sutter,
  title = {Generalized Maximum Entropy Estimation},
  author = {Sutter, Tobias and Sutter, David and Esfahani, Peyman Mohajerin and Lygeros, John},
  pages = {29},
  abstract = {We consider the problem of estimating a probability distribution that maximizes the entropy while satisfying a finite number of moment constraints, possibly corrupted by noise. Based on duality of convex programming, we present a novel approximation scheme using a smoothed fast gradient method that is equipped with explicit bounds on the approximation error. We further demonstrate how the presented scheme can be used for approximating the chemical master equation through the zero-information moment closure method, and for an approximate dynamic programming approach in the context of constrained Markov decision processes with uncountable state and action spaces.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/N34MFM9H/Sutter et al. - Generalized maximum entropy estimation.pdf}
}

@article{Svec2021,
  title = {Integrative {{Insights}} into the {{Myoelastic-Aerodynamic Theory}} and {{Acoustics}} of {{Phonation}}. {{Scientific Tribute}} to {{Donald G}}. {{Miller}}},
  author = {Svec, Jan G. and Schutte, Harm K. and Chen, C. Julian and Titze, Ingo R.},
  year = {2021},
  month = mar,
  journal = {Journal of Voice},
  issn = {0892-1997},
  doi = {10.1016/j.jvoice.2021.01.023},
  urldate = {2021-07-01},
  abstract = {In this tribute article to D.G. Miller, we review some historical and recent contributions to understanding the myoelastic-aerodynamic (MEAD) theory of phonation and the related acoustic phenomena in subglottal and vocal tract. At the time of the formulation of MEAD by van den Berg in late 1950s, it was assumed that vocal fold oscillations are self-sustained thanks to increased subglottal pressure pushing the glottis to open and decreased subglottal pressure allowing the glottis to close. In vivo measurements of subglottal pressures during phonation invalidated these assumptions, however, and showed that at low fundamental frequencies subglottal pressure rather tends to reach a maximum value at the beginning of glottal closure and then exhibits damped oscillations. These events can be interpreted as transient acoustic resonance phenomena in the subglottal tract that are triggered by glottal closure. They are analogous to the transient acoustic phenomena seen in the vocal tract. Rather than subglottal pressure oscillations, a more efficient mechanism of transfer of aerodynamic energy to the vocal fold vibrations has been identified in the vertical phase differences (mucosal waves) making the glottal shape more convergent during glottis opening than during glottis closing. Along with other discoveries, these findings form the basis of our current understanding of MEAD.},
  langid = {english},
  keywords = {Myoelastic-aerodynamic theory of phonation,Subglottal resonances Vocal tract resonances,Voice production Vocal fold vibration},
  file = {/home/marnix/Zotero/storage/PZYMWPM2/Švec et al. - 2021 - Integrative Insights into the Myoelastic-Aerodynam.pdf;/home/marnix/Zotero/storage/FZH67H6P/S0892199721000552.html}
}

@article{Svensen2005,
  title = {Robust {{Bayesian}} Mixture Modelling},
  author = {Svens{\'e}n, Markus and Bishop, Christopher M.},
  year = {2005},
  month = mar,
  journal = {Neurocomputing},
  series = {Trends in {{Neurocomputing}}: 12th {{European Symposium}} on {{Artificial Neural Networks}} 2004},
  volume = {64},
  pages = {235--252},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2004.11.018},
  urldate = {2020-06-24},
  abstract = {Bayesian approaches to density estimation and clustering using mixture distributions allow the automatic determination of the number of components in the mixture. Previous treatments have focussed on mixtures having Gaussian components, but these are well known to be sensitive to outliers, which can lead to excessive sensitivity to small numbers of data points and consequent over-estimates of the number of components. In this paper we develop a Bayesian approach to mixture modelling based on Student-t distributions, which are heavier tailed than Gaussians and hence more robust. By expressing the Student-t distribution as a marginalization over additional latent variables we are able to derive a tractable variational inference algorithm for this model, which includes Gaussian mixtures as a special case. Results on a variety of real data sets demonstrate the improved robustness of our approach.},
  langid = {english},
  keywords = {Latent variable model,Model selection,Outliers,Student-t distribution,Variational inference},
  file = {/home/marnix/Zotero/storage/RMVUMVI4/Svensén and Bishop - 2005 - Robust Bayesian mixture modelling.pdf;/home/marnix/Zotero/storage/MJAJHYPF/S0925231204005181.html}
}

@misc{Swersky2014,
  title = {Freeze-{{Thaw Bayesian Optimization}}},
  author = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan Prescott},
  year = {2014},
  month = jun,
  number = {arXiv:1406.3896},
  eprint = {1406.3896},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-07-19},
  abstract = {In this paper we develop a dynamic form of Bayesian optimization for machine learning models with the goal of rapidly finding good hyperparameter settings. Our method uses the partial information gained during the training of a machine learning model in order to decide whether to pause training and start a new model, or resume the training of a previously-considered model. We specifically tailor our method to machine learning problems by developing a novel positive-definite covariance kernel to capture a variety of training curves. Furthermore, we develop a Gaussian process prior that scales gracefully with additional temporal observations. Finally, we provide an information-theoretic framework to automate the decision process. Experiments on several common machine learning models show that our approach is extremely effective in practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/VRLFEWWK/Swersky et al. - 2014 - Freeze-Thaw Bayesian Optimization.pdf;/home/marnix/Zotero/storage/4U9MREVU/1406.html}
}

@article{Swiler2020,
  title = {A Survey of Constrained Gaussian Process Regression: Approaches and Implementation Challenges},
  shorttitle = {A Survey of Constrained Gaussian Process Regression},
  author = {Swiler, Laura P. and Gulian, Mamikon and Frankel, Ari L. and Safta, Cosmin and Jakeman, John D.},
  year = {2020},
  journal = {Journal of Machine Learning for Modeling and Computing},
  volume = {1},
  number = {2},
  pages = {119--156},
  issn = {2689-3967},
  doi = {10.1615/JMachLearnModelComput.2020035155},
  urldate = {2024-01-07},
  abstract = {Gaussian process regression is a popular Bayesian framework for surrogate modeling of expensive data sources. As part of a broader effort in scientific machine learning, many recent works have incorporated physical constraints or other a priori information within Gaussian process regression to supplement limited data and regularize the behavior of the model. We provide an overview and survey of several classes of Gaussian process constraints, including positivity or bound constraints, monotonicity and convexity constraints, differential equation constraints provided by linear PDEs, and boundary condition constraints. We compare the strategies behind each approach as well as the differences in implementation, concluding with a discussion of the computational challenges introduced by constraints.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/AJVVT6I2/Swiler et al. - 2020 - A SURVEY OF CONSTRAINED GAUSSIAN PROCESS REGRESSIO.pdf}
}

@article{Talagrand1995,
  title = {Concentration of Measure and Isoperimetric Inequalities in Product Spaces},
  author = {Talagrand, Michel},
  year = {1995},
  journal = {Publications Math{\'e}matiques de l'Institut des Hautes {\'E}tudes Scientifiques},
  volume = {81},
  number = {1},
  eprintclass = {math},
  pages = {73--205},
  issn = {1618-1913},
  doi = {10.1007/BF02699376},
  abstract = {The concentration of measure phenomenon in product spaces roughly states that, if a set A in a product \${\textohm}\$N of probability spaces has measure at least one half, ``most'' of the points of \${\textohm}\$n are ``close'' to A. We proceed to a systematic exploration of this phenomenon. The meaning of the word ``most'' is made rigorous by isoperimetrictype inequalities that bound the measure of the exceptional sets. The meaning of the work ``close'' is defined in three main ways, each of them giving rise to related, but different inequalities. The inequalities are all proved through a common scheme of proof. Remarkably, this simple approach not only yields qualitatively optimal results, but, in many cases, captures near optimal numerical constants. A large number of applications are given, in particular to Percolation, Geometric Probability, Probability in Banach Spaces, to demonstrate in concrete situations the extremely wide range of application of the abstract tools.},
  arxiv = {9406212},
  arxivid = {math/9406212},
  isbn = {0073-8301},
  keywords = {28A35,60G99,68C15,AMS Classification numbers: Primary 60E15,Secondary 60G15},
  file = {/home/marnix/Zotero/storage/HBGXDDK5/Talagrand - 1995 - Concentration of measure and isoperimetric inequalities in product spaces.pdf}
}

@article{Taylor2010,
  title = {The Contribution of Source--Filter Theory to Mammal Vocal Communication Research},
  author = {Taylor, A. M. and Reby, D.},
  year = {2010},
  journal = {Journal of Zoology},
  volume = {280},
  number = {3},
  pages = {221--236},
  issn = {1469-7998},
  doi = {10.1111/j.1469-7998.2009.00661.x},
  urldate = {2020-02-05},
  abstract = {The field of animal vocal communication has benefited greatly from improved understanding of vocal production mechanisms and specifically from the generalization of the source--filter theory of speech production to non-human mammals. The application of the source--filter theory has enabled researchers to decompose the acoustic structure of vocal signals according to their mode of production and thereby to predict the acoustic variation that is caused by anatomical or physiological attributes of the caller. The source--filter theory states that vocal signals result from a two-stage production, with the glottal wave generated in the larynx (the source), being subsequently filtered in the supralaryngeal vocal tract (the filter). This theory predicts that independent indexical information such as body size, weight, age and sex can be contained in both the glottal wave (mostly characterized by its fundamental frequency), and the spectral envelope of the radiated vocalization (mostly characterized by the vocal tract resonances or formant frequencies). Additionally, physiological fluctuations in emotional or motivational state have been found to influence the acoustic characteristics of signals in a reliable and predictable manner that is perceptually available to receivers. While animal vocalizations contain some dynamic attributes, their static attributes are sufficient to provide an effective means of acoustic individual discrimination both within and across call types. In this paper, we draw together a wealth of experimental work conducted within the source--filter framework over the last decade and we review how such experiments have elucidated the communicative value of animal vocalizations.},
  copyright = {{\copyright} 2009 The Authors. Journal compilation {\copyright} 2009 The Zoological Society of London},
  langid = {english},
  keywords = {acoustic signals,source--filter theory,vocal communication,vocal production},
  file = {/home/marnix/Zotero/storage/9C4IRZDF/Taylor and Reby - 2010 - The contribution of source–filter theory to mammal.pdf;/home/marnix/Zotero/storage/N27AI5U9/j.1469-7998.2009.00661.html}
}

@misc{Tebbutt2021,
  title = {Combining {{Pseudo-Point}} and {{State Space Approximations}} for {{Sum-Separable Gaussian Processes}}},
  author = {Tebbutt, Will and Solin, Arno and Turner, Richard E.},
  year = {2021},
  month = jun,
  number = {arXiv:2106.10210},
  eprint = {2106.10210},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.10210},
  urldate = {2025-04-08},
  abstract = {Gaussian processes (GPs) are important probabilistic tools for inference and learning in spatio-temporal modelling problems such as those in climate science and epidemiology. However, existing GP approximations do not simultaneously support large numbers of off-the-grid spatial data-points and long time-series which is a hallmark of many applications. Pseudo-point approximations, one of the gold-standard methods for scaling GPs to large data sets, are well suited for handling off-the-grid spatial data. However, they cannot handle long temporal observation horizons effectively reverting to cubic computational scaling in the time dimension. State space GP approximations are well suited to handling temporal data, if the temporal GP prior admits a Markov form, leading to linear complexity in the number of temporal observations, but have a cubic spatial cost and cannot handle off-the-grid spatial data. In this work we show that there is a simple and elegant way to combine pseudo-point methods with the state space GP approximation framework to get the best of both worlds. The approach hinges on a surprising conditional independence property which applies to space--time separable GPs. We demonstrate empirically that the combined approach is more scalable and applicable to a greater range of spatio-temporal problems than either method on its own.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/XKGSSBGZ/Tebbutt et al. - 2021 - Combining Pseudo-Point and State Space Approximations for Sum-Separable Gaussian Processes.pdf;/home/marnix/Zotero/storage/5RSRQ6NX/2106.html}
}

@book{Thijssen1999,
  title = {Computational {{Physics}}},
  author = {Thijssen, J M},
  year = {1999},
  publisher = {Cambridge University Press},
  address = {New York, NY, USA},
  isbn = {0-521-57588-5}
}

@article{Titze1992,
  title = {Vocal Intensity in Speakers and Singers},
  author = {Titze, Ingo R and Sundberg, Johan},
  year = {1992},
  journal = {the Journal of the Acoustical Society of America},
  volume = {91},
  number = {5},
  pages = {2936--2946},
  publisher = {Acoustical Society of America},
  file = {/home/marnix/Zotero/storage/73YMPTVM/Titze and Sundberg - 1992 - Vocal intensity in speakers and singers.pdf}
}

@book{Titze1995,
  title = {Workshop on Acoustic Voice Analysis: {{Summary}} Statement},
  author = {Titze, Ingo R},
  year = {1995},
  publisher = {{National Center for Voice and Speech}},
  file = {/home/marnix/Zotero/storage/IUICME8I/Titze - 1995 - Workshop on acoustic voice analysis Summary state.pdf}
}

@article{Titze2000,
  title = {Principles of Voice Production (Second Printing)},
  author = {Titze, Ingo R},
  year = {2000},
  journal = {Iowa City, IA: National Center for Voice and Speech},
  file = {/home/marnix/Zotero/storage/UM72SGXT/Titze - 2000 - Principles of voice production (second printing).pdf}
}

@article{Titze2015,
  title = {Toward a Consensus on Symbolic Notation of Harmonics, Resonances, and Formants in Vocalization},
  author = {Titze, Ingo R. and Baken, Ronald J. and Bozeman, Kenneth W. and Granqvist, Svante and Henrich, Nathalie and Herbst, Christian T. and Howard, David M. and Hunter, Eric J. and Kaelin, Dean and Kent, Raymond D. and Kreiman, Jody and Kob, Malte and L{\"o}fqvist, Anders and McCoy, Scott and Miller, Donald G. and No{\'e}, Hubert and Scherer, Ronald C. and Smith, John R. and Story, Brad H. and {\v S}vec, Jan G. and Ternstr{\"o}m, Sten and Wolfe, Joe},
  year = {2015},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {137},
  number = {5},
  pages = {3005--3007},
  issn = {0001-4966},
  doi = {10.1121/1.4919349},
  urldate = {2021-02-23},
  pmcid = {PMC5392060},
  pmid = {25994732},
  file = {/home/marnix/Zotero/storage/CVCLCMP9/Titze et al. - 2015 - Toward a consensus on symbolic notation of harmoni.pdf}
}

@article{Tjandra2015,
  title = {{{COMBINATION OFTWO-DIMENSIONAL COCHLEOGRAM AND SPECTROGRAM FEATURES FOR DEEP LEARNING-BASED ASR}}},
  author = {{Tjandra}},
  year = {2015},
  issn = {15206149},
  doi = {10.1007/978-81-322-2126-5},
  isbn = {978-81-322-2125-8},
  file = {/home/marnix/Zotero/storage/HM28YIHM/Tjandra2015 Combination of two-dimensional cochleogram and spectrogram features for deep learning-based ASR.pdf}
}

@inproceedings{Tobar2015,
  title = {Learning {{Stationary Time Series}} Using {{Gaussian Processes}} with {{Nonparametric Kernels}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Tobar, Felipe and Bui, Thang D and Turner, Richard E},
  year = {2015},
  volume = {28},
  publisher = {Curran Associates, Inc.},
  urldate = {2021-11-06},
  file = {/home/marnix/Zotero/storage/NDR2ZNPD/Tobar et al. - 2015 - Learning Stationary Time Series using Gaussian Pro.pdf}
}

@inproceedings{Tobar2016,
  title = {Modelling Time Series via Automatic Learning of Basis Functions},
  booktitle = {2016 {{IEEE Sensor Array}} and {{Multichannel Signal Processing Workshop}} ({{SAM}})},
  author = {Tobar, Felipe and Turner, Richard E.},
  year = {2016},
  month = jul,
  pages = {1--5},
  issn = {2151-870X},
  doi = {10.1109/SAM.2016.7569727},
  abstract = {We present a model for time series consisting of an infinite mixture of basis functions, whereby the bases and the mixing process are modelled as posterior means of latent Gaussian processes (GPs). Conditional to observed data, the bases and the mixing process are learnt using a parametric approximation based on pseudo-observations, where the complexity and accuracy of the method are controlled by the number of pseudo-observations (Nx and Nh). The resulting model is linear the pseudo-observations, and its likelihood function has a complexity O(NNhNx), Nx {$<$}; N, Nh {$\ll$} N, which is lower than that of the standard GP O(N3) - where N is the number of observations. We validate the proposed approach using synthetic data, where we recovered latent GPs with five different kernels from noisy observations; and using a real-world heart-rate signal to assess the proposed model's computational complexity and performance.},
  keywords = {automatic learning,basis functions,cardiology,computational complexity,Computational complexity,Computational modeling,Gaussian processes,heart-rate signal,Kernel,latent Gaussian process,latent GP recovery,likelihood function,Mathematical model,medical signal processing,mixing process,noisy observations,parametric approximation,pseudoobservations,Standards,time series,Time series analysis,time series modelling},
  file = {/home/marnix/Zotero/storage/DYATMN34/Tobar and Turner - 2016 - Modelling time series via automatic learning of ba.pdf;/home/marnix/Zotero/storage/TFDTJK5W/7569727.html}
}

@incollection{Tobar2018,
  title = {Bayesian {{Nonparametric Spectral Estimation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Tobar, Felipe},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {10127--10137},
  publisher = {Curran Associates, Inc.},
  urldate = {2020-09-14},
  file = {/home/marnix/Zotero/storage/928UW925/tobar_NIPS2018_supp.pdf;/home/marnix/Zotero/storage/RW68UIUC/Tobar - 2018 - Bayesian Nonparametric Spectral Estimation.pdf;/home/marnix/Zotero/storage/38IGNZIE/8216-bayesian-nonparametric-spectral-estimation.html}
}

@incollection{Tobar2019,
  title = {Band-{{Limited Gaussian Processes}}: {{The Sinc Kernel}}},
  shorttitle = {Band-{{Limited Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Tobar, Felipe},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {12749--12759},
  publisher = {Curran Associates, Inc.},
  urldate = {2020-04-24},
  file = {/home/marnix/Zotero/storage/Z3P4WQCC/Tobar - 2019 - Band-Limited Gaussian Processes The Sinc Kernel.pdf;/home/marnix/Zotero/storage/CGCMP55F/9436-band-limited-gaussian-processes-the-sinc-kernel.html}
}

@article{Tobar2021,
  title = {Bayesian {{Reconstruction}} of {{Fourier Pairs}}},
  author = {Tobar, Felipe and {Araya-Hern{\'a}ndez}, Lerko and Huijse, Pablo and Djuri{\'c}, Petar M.},
  year = {2021},
  journal = {IEEE Transactions on Signal Processing},
  volume = {69},
  pages = {73--87},
  issn = {1941-0476},
  doi = {10.1109/TSP.2020.3038135},
  urldate = {2023-12-18},
  abstract = {In a number of data-driven applications such as detection of arrhythmia, interferometry or audio compression, observations are acquired indistinctly in the time or frequency domains: temporal observations allow us to study the spectral content of signals (e.g., audio), while frequency-domain observations are used to reconstruct temporal/spatial data (e.g., MRI). Classical approaches for spectral analysis rely either on i) a discretisation of the time and frequency domains, where the fast Fourier transform stands out as the de facto off-the-shelf resource, or ii) stringent parametric models with closed-form spectra. However, the general literature fails to cater for missing observations and noise-corrupted data. Our aim is to address the lack of a principled treatment of data acquired indistinctly in the temporal and frequency domains in a way that is robust to missing or noisy observations, and that at the same time models uncertainty effectively. To achieve this aim, we first define a joint probabilistic model for the temporal and spectral representations of signals, to then perform a Bayesian model update in the light of observations, thus jointly reconstructing the complete (latent) time and frequency representations. The proposed model is analysed from a classical spectral analysis perspective, and its implementation is illustrated through intuitive examples. Lastly, we show that the proposed model is able to perform joint time and frequency reconstruction of real-world audio, healthcare and astronomy signals, while successfully dealing with missing data and handling uncertainty (noise) naturally against both classical and modern approaches for spectral estimation.},
  file = {/home/marnix/Zotero/storage/53ZZUZBV/Tobar et al. - 2021 - Bayesian Reconstruction of Fourier Pairs.pdf;/home/marnix/Zotero/storage/AC8Z7U8I/9259115.html}
}

@article{Tobar2023,
  title = {Gaussian Process Deconvolution},
  author = {Tobar, Felipe and Robert, Arnaud and Silva, Jorge F.},
  year = {2023},
  month = jul,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {479},
  number = {2275},
  eprint = {2305.04871},
  primaryclass = {cs, eess, stat},
  pages = {20220648},
  issn = {1364-5021, 1471-2946},
  doi = {10.1098/rspa.2022.0648},
  urldate = {2023-12-18},
  abstract = {Let us consider the deconvolution problem, that is, to recover a latent source \$x({\textbackslash}cdot)\$ from the observations \${\textbackslash}mathbf\{y\} = [y\_1,{\textbackslash}ldots,y\_N]\$ of a convolution process \$y = x{\textbackslash}star h + {\textbackslash}eta\$, where \${\textbackslash}eta\$ is an additive noise, the observations in \${\textbackslash}mathbf\{y\}\$ might have missing parts with respect to \$y\$, and the filter \$h\$ could be unknown. We propose a novel strategy to address this task when \$x\$ is a continuous-time signal: we adopt a Gaussian process (GP) prior on the source \$x\$, which allows for closed-form Bayesian nonparametric deconvolution. We first analyse the direct model to establish the conditions under which the model is well defined. Then, we turn to the inverse problem, where we study i) some necessary conditions under which Bayesian deconvolution is feasible, and ii) to which extent the filter \$h\$ can be learnt from data or approximated for the blind deconvolution case. The proposed approach, termed Gaussian process deconvolution (GPDC) is compared to other deconvolution methods conceptually, via illustrative examples, and using real-world datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/LNMK3WHR/Tobar et al. - 2023 - Gaussian process deconvolution.pdf;/home/marnix/Zotero/storage/W3QFZ53L/2305.html}
}

@misc{Touvron2023,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09288},
  eprint = {2307.09288},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.09288},
  urldate = {2023-09-02},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/marnix/Zotero/storage/TUZ7A7HW/Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf;/home/marnix/Zotero/storage/XE8JXH9V/2307.html}
}

@article{Tran2023,
  title = {Learning, Inference, and Prediction on Probability Density Functions with Constrained {{Gaussian}} Processes},
  author = {Tran, Tien-Tam and Fradi, Anis and Samir, Chafik},
  year = {2023},
  month = sep,
  journal = {Information Sciences},
  volume = {642},
  pages = {119068},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2023.119068},
  urldate = {2024-01-07},
  abstract = {Probability density functions (PDFs) play an important role in machine learning, statistics, and in many real-world applications. In this paper, we introduce a new framework to learn, infer and predict nonparametric PDFs with a Gaussian process prior. This is a challenging problem since there is no explicit formulas for conditional expectations and covariances. The proposed methods have two main advantages: Analyzing PDFs with an intrinsic manifold structure and establishing the connection with the Hilbert sphere when endowed with an appropriate metric. Therefore, all solutions are valid PDFs with non-negative values and integrate to one. We formulate the problem as constrained spherical Gaussian processes leading to an efficient solution with a spherical Hamiltonian Monte Carlo (HMC) sampling. We test and evaluate different strategies with extensive experiments on both simulations and real dataset.},
  keywords = {Constrained Gaussian process,Inference,Learning,Prediction,Spherical HMC},
  file = {/home/marnix/Zotero/storage/HCTG6RYS/S0020025523006539.html}
}

@article{Trassinelli2017,
  title = {Bayesian Data Analysis Tools for Atomic Physics},
  author = {Trassinelli, Martino},
  year = {2017},
  month = oct,
  journal = {Nuclear Instruments and Methods in Physics Research Section B: Beam Interactions with Materials and Atoms},
  series = {Proceedings of the 18th {{International Conference}} on the {{Physics}} of {{Highly Charged Ions}} ({{HCI-2016}}), {{Kielce}}, {{Poland}}, 11-16 {{September}} 2016},
  volume = {408},
  pages = {301--312},
  issn = {0168-583X},
  doi = {10.1016/j.nimb.2017.05.030},
  urldate = {2021-03-12},
  abstract = {We present an introduction to some concepts of Bayesian data analysis in the context of atomic physics. Starting from basic rules of probability, we present the Bayes' theorem and its applications. In particular we discuss about how to calculate simple and joint probability distributions and the Bayesian evidence, a model dependent quantity that allows to assign probabilities to different hypotheses from the analysis of a same data set. To give some practical examples, these methods are applied to two concrete cases. In the first example, the presence or not of a satellite line in an atomic spectrum is investigated. In the second example, we determine the most probable model among a set of possible profiles from the analysis of a statistically poor spectrum. We show also how to calculate the probability distribution of the main spectral component without having to determine uniquely the spectrum modeling. For these two studies, we implement the program Nested\_fit to calculate the different probability distributions and other related quantities. Nested\_fit is a Fortran90/Python code developed during the last years for analysis of atomic spectra. As indicated by the name, it is based on the nested algorithm, which is presented in details together with the program itself.},
  langid = {english},
  keywords = {Atomic physics,Bayesian data analysis,Model testing,Nested sampling},
  file = {/home/marnix/Zotero/storage/A9Q5ZPGY/Trassinelli - 2017 - Bayesian data analysis tools for atomic physics.pdf;/home/marnix/Zotero/storage/IYWSS5WX/S0168583X17306122.html}
}

@book{Trefethen2018,
  title = {Approximation Theory and Approximation Practice},
  author = {Trefethen, Lloyd N},
  year = {2018},
  publisher = {SIAM},
  file = {/home/marnix/Zotero/storage/PSVDNX4U/Trefethen - Approximation Theory and Approximation Practice.pdf}
}

@inproceedings{Tronarp2018,
  title = {Mixture {{Representation}} of the {{Mat{\'e}rn Class}} with {{Applications}} in {{State Space Approximations}} and {{Bayesian Quadrature}}},
  booktitle = {2018 {{IEEE}} 28th {{International Workshop}} on {{Machine Learning}} for {{Signal Processing}} ({{MLSP}})},
  author = {Tronarp, Filip and Karvonen, Toni and Sarkka, Simo},
  year = {2018},
  month = sep,
  pages = {1--6},
  publisher = {IEEE},
  address = {Aalborg},
  doi = {10.1109/MLSP.2018.8516992},
  urldate = {2024-02-27},
  abstract = {In this paper, the connection between the Mat{\'e}rn kernel and scale mixtures of squared exponential kernels is explored. It is shown that the Mat{\'e}rn kernel can be approximated by a finite scale mixture of squared exponential kernels through a quadrature approximation which in turn allows for (i) state space approximations of the Mat{\'e}rn kernel for arbitrary smoothness parameters using established state space approximations of the squared exponential kernel and (ii) exact calculation of the Bayesian quadrature weights for the approximate kernel under a Gaussian measure. The method is demonstrated in inference in a log-Gaussian Cox process as well as in approximating a Gaussian integral arising from a financial problem using Bayesian quadrature.},
  isbn = {978-1-5386-5477-4},
  langid = {english},
  file = {/home/marnix/Zotero/storage/KL9SSM3M/Tronarp et al. - 2018 - MIXTURE REPRESENTATION OF THE MATÉRN CLASS WITH AP.pdf}
}

@book{Tunstall2022,
  title = {Natural Language Processing with Transformers},
  author = {Tunstall, Lewis and Von Werra, Leandro and Wolf, Thomas},
  year = {2022},
  publisher = {" O'Reilly Media, Inc."},
  file = {/home/marnix/Zotero/storage/MFVT2EIW/Tunstall et al. - 2022 - Natural language processing with transformers.pdf}
}

@article{Turing1950,
  title = {Computing Machinery and Intelligence},
  author = {Turing, A. M.},
  year = {1950},
  month = oct,
  journal = {Mind},
  volume = {LIX},
  number = {236},
  pages = {433--460},
  publisher = {Oxford Academic},
  issn = {0026-4423},
  doi = {10.1093/mind/LIX.236.433},
  urldate = {2020-06-18},
  abstract = {I propose to consider the question, `Can machines think?' This should begin with definitions of the meaning of the terms `machine' and `think'. The definitions},
  langid = {english},
  file = {/home/marnix/Zotero/storage/FM5VJ6QH/Turing - 1950 - I.—COMPUTING MACHINERY AND INTELLIGENCE.pdf;/home/marnix/Zotero/storage/6G3PEZQU/986238.html}
}

@book{Turkle2011,
  title = {Alone Together: {{Why}} We Expect More from Technology and Less from Each Other},
  author = {Turkle, Sherry},
  year = {2011},
  publisher = {Basic Books},
  address = {New York, NY},
  isbn = {9780465031463}
}

@article{Turner2003,
  title = {An Analysis of the Size Information in Classical Formant Data: {{Peterson}} and {{Barney}} (1952) Revisited},
  author = {Turner, Richard E and Patterson, Roy D},
  year = {2003},
  journal = {J. Acoust. Soc. Jpn},
  volume = {33},
  number = {9},
  pages = {585--589},
  file = {/home/marnix/Zotero/storage/9MH5F3FZ/Turner and Patterson - 2003 - An analysis of the size information in classical f.pdf}
}

@article{Turner2009,
  title = {A Statistical, Formant-Pattern Model for Segregating Vowel Type and Vocal-Tract Length in Developmental Formant Data},
  author = {Turner, Richard E. and Walters, Thomas C. and Monaghan, Jessica J. M. and Patterson, Roy D.},
  year = {2009},
  month = apr,
  journal = {The Journal of the Acoustical Society of America},
  volume = {125},
  number = {4},
  pages = {2374--2386},
  issn = {0001-4966},
  doi = {10.1121/1.3079772},
  urldate = {2020-10-22},
  abstract = {This paper investigates the theoretical basis for estimating vocal-tract length (VTL) from the formant frequencies of vowel sounds. A statistical inference model was developed to characterize the relationship between vowel type and VTL, on the one hand, and formant frequency and vocal cavity size, on the other. The model was applied to two well known developmental studies of formant frequency. The results show that VTL is the major source of variability after vowel type and that the contribution due to other factors like developmental changes in oral-pharyngeal ratio is small relative to the residual measurement noise. The results suggest that speakers adjust the shape of the vocal tract as they grow to maintain a specific pattern of formant frequencies for individual vowels. This formant-pattern hypothesis motivates development of a statistical-inference model for estimating VTL from formant-frequency data. The technique is illustrated using a third developmental study of formant frequencies. The VTLs of the speakers are estimated and used to provide a more accurate description of the complicated relationship between VTL and glottal pulse rate as children mature into adults.},
  pmcid = {PMC2824129},
  pmid = {19354411},
  file = {/home/marnix/Zotero/storage/X52M7H9H/Turner et al. - 2009 - A statistical, formant-pattern model for segregati.pdf}
}

@phdthesis{Turner2010,
  title = {Statistical Models for Natural Sounds},
  author = {Turner, Richard E},
  year = {2010},
  school = {UCL (University College London)},
  file = {/home/marnix/Zotero/storage/WR6R2EZH/Turner - Statistical models for natural sounds.pdf}
}

@article{Turner2014,
  title = {Time-Frequency Analysis as Probabilistic Inference},
  author = {Turner, Richard E. and Sahani, Maneesh},
  year = {2014},
  journal = {IEEE Transactions on Signal Processing},
  volume = {62},
  number = {23},
  pages = {6171--6183},
  issn = {1053587X},
  doi = {10.1109/TSP.2014.2362100},
  abstract = {This paper proposes a new view of time-frequency analysis framed in terms of probabilistic inference. Natural signals are assumed to be formed by the superposition of distinct time-frequency components, with the analytic goal being to infer these components by application of Bayes' rule. The framework serves to unify various existing models for natural time-series; it relates to both the Wiener and Kalman filters, and with suitable assumptions yields inferential interpretations of the short-time Fourier transform, spectrogram, filter bank, and wavelet representations. Value is gained by placing time-frequency analysis on the same probabilistic basis as is often employed in applications such as denoising, source separation, or recognition. Uncertainty in the time-frequency representation can be propagated correctly to application-specific stages, improving the handing of noise and missing data. Probabilistic learning allows modules to be co-adapted; thus, the time-frequency representation can be adapted to both the demands of the application and the time-varying statistics of the signal at hand. Similarly, the application module can be adapted to fine properties of the signal propagated by the initial time-frequency processing. We demonstrate these benefits by combining probabilistic time-frequency representations with non-negative matrix factorization, finding benefits in audio denoising and inpainting tasks, albeit with higher computational cost than incurred by the standard approach.},
  isbn = {1053-587X VO - 62},
  keywords = {Audio signal processing,inference,machine- learning,time-frequency analysis},
  file = {/home/marnix/Zotero/storage/EZFFWRM9/Turner2014 Time-frequency analysis as probabilistic inference.pdf;/home/marnix/Zotero/storage/P2MCM238/supplemental-final.pdf}
}

@misc{Turner2024,
  title = {An {{Introduction}} to {{Transformers}}},
  author = {Turner, Richard E.},
  year = {2024},
  month = feb,
  number = {arXiv:2304.10557},
  eprint = {2304.10557},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-07},
  abstract = {The transformer is a neural network component that can be used to learn useful representations of sequences or sets of data-points. The transformer has driven recent advances in natural language processing, computer vision, and spatio-temporal modelling. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing. Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture. We will not discuss training as this is rather standard. We assume that the reader is familiar with fundamental topics in machine learning including multi-layer perceptrons, linear transformations, softmax functions and basic probability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/marnix/Zotero/storage/24T5HURS/Turner - 2024 - An Introduction to Transformers.pdf;/home/marnix/Zotero/storage/7LDK5CA4/2304.html}
}

@article{Tzikas2008,
  title = {The Variational Approximation for {{Bayesian}} Inference},
  author = {Tzikas, Dimitris G. and Likas, Aristidis C. and Galatsanos, Nikolaos P.},
  year = {2008},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {25},
  number = {6},
  pages = {131--146},
  issn = {1053-5888, 1558-0792},
  doi = {10.1109/MSP.2008.929620},
  urldate = {2021-08-16},
  langid = {english},
  file = {/home/marnix/Zotero/storage/YAYDN497/Tzikas et al. - 2008 - The variational approximation for Bayesian inferen.pdf}
}

@article{Uneson1998,
  title = {An {{Introduction}} to {{Dynamic Bayesian Networks}} for {{Automatic Speech Recognition}}},
  author = {Uneson, Marcus},
  year = {1998},
  volume = {1},
  number = {10},
  file = {/home/marnix/Zotero/storage/JL35QGGW/Uneson - 1998 - An Introduction to Dynamic Bayesian Networks for Automatic Speech Recognition.pdf}
}

@article{Urteaga2017,
  title = {Sequential {{Monte Carlo}} for Inference of Latent {{ARMA}} Time-Series with Innovations Correlated in Time},
  author = {Urteaga, I{\~n}igo and Bugallo, M{\'o}nica F. and Djuri{\'c}, Petar M.},
  year = {2017},
  month = dec,
  journal = {EURASIP Journal on Advances in Signal Processing},
  volume = {2017},
  number = {1},
  pages = {84},
  issn = {1687-6180},
  doi = {10.1186/s13634-017-0518-4},
  urldate = {2019-04-24},
  abstract = {We consider the problem of sequential inference of latent time-series with innovations correlated in time and observed via nonlinear functions. We accommodate time-varying phenomena with diverse properties by means of a flexible mathematical representation of the data. We characterize statistically such time-series by a Bayesian analysis of their densities. The density that describes the transition of the state from time t to the next time instant t+1 is used for implementation of novel sequential Monte Carlo (SMC) methods. We present a set of SMC methods for inference of latent ARMA time-series with innovations correlated in time for different assumptions in knowledge of parameters. The methods operate in a unified and consistent manner for data with diverse memory properties. We show the validity of the proposed approach by comprehensive simulations of the challenging stochastic volatility model.},
  file = {/home/marnix/Zotero/storage/EKYCPRNR/Urteaga et al. - 2017 - Sequential Monte Carlo for inference of latent ARM.pdf;/home/marnix/Zotero/storage/KDQNPYCK/s13634-017-0518-4.html}
}

@article{Vallabha2002,
  title = {Systematic Errors in the Formant Analysis of Steady-State Vowels},
  author = {Vallabha, Gautam K. and Tuller, Betty},
  year = {2002},
  month = sep,
  journal = {Speech Communication},
  volume = {38},
  number = {1},
  pages = {141--160},
  issn = {0167-6393},
  doi = {10.1016/S0167-6393(01)00049-8},
  urldate = {2019-04-27},
  abstract = {The locations of formants in a speech signal are usually estimated by computing the linear predictive coefficients (LPC) over a sliding window and finding the peaks in the spectrum of the resulting LP filter. The peak locations are estimated either by root-solving or by computing a coarse spectrum and finding its maxima. We discuss four sources of systematic error in this analysis: (1) quantization of the speech signal due to the fundamental frequency, (2) incorrect order for the LP filter, (3) exclusive reliance upon root-solving, and (4) the three-point parabolic interpolation used to compensate for the coarse spectrum. We show that the expected error due to F0 quantization is {$\sim$}10\% of F0, and that the other three sources can independently skew the final formant estimates by 10--80 Hz. We also show that errors due to incorrect filter order are related to systematic differences between speakers and phonetic classes, and that root-solving is especially error-prone for low formants or when formants are close to each other. We discuss methods for avoiding these errors and improving the accuracy of formant estimation, and give a heuristic for estimating the optimal filter order of a steady-state signal.},
  keywords = {Formant estimation,LPC,Speech analysis,Vowels},
  file = {/home/marnix/Zotero/storage/D4PI3KRS/Vallabha and Tuller - 2002 - Systematic errors in the formant analysis of stead.pdf;/home/marnix/Zotero/storage/K5VYU8W7/S0167639301000498.html}
}

@book{Vallee1994,
  title = {Syst{\`e}mes Vocaliques: De La Typologie Aux Pr{\'e}dictions},
  author = {Vall{\'e}e, N.},
  year = {1994},
  publisher = {Th{\`e}se pr{\'e}par{\'e}e au sein de l'Institut de la Communication Parl{\'e}e (Grenoble-URA C.N.R.S. no 368)},
  place = {Grenoble},
  file = {/home/marnix/Zotero/storage/QD6FM7BE/Vallée - 1994 - Systèmes vocaliques de la typologie aux prédictio.pdf}
}

@article{Vandermarliere2015,
  title = {Beyond the Power Law: {{Uncovering}} Stylized Facts in Interbank Networks},
  author = {Vandermarliere, B and Karas, A and Ryckebusch, J and Schoors, K},
  year = {2015},
  month = jun,
  journal = {Physica A Statistical Mechanics and its Applications},
  volume = {428},
  eprint = {1409.3738},
  eprintclass = {q-fin.GN},
  pages = {443--457},
  doi = {10.1016/j.physa.2015.01.058},
  archiveprefix = {arXiv},
  arxivid = {q-fin.GN/1409.3738},
  keywords = {Distributions,Heavy tailed,Interbank networks,Lognormal,Power law,Stretched exponential},
  file = {/home/marnix/Zotero/storage/INHLX7WC/Vandermarliere et al. - 2015 - Beyond the power law Uncovering stylized facts in interbank networks.pdf}
}

@phdthesis{Vandyke2014,
  title = {Glottal Waveforms for Speaker Inference \& a Regression Score Post-Processing Method Applicable to General Classification Problems},
  author = {Vandyke, David James},
  year = {2014},
  school = {Citeseer},
  file = {/home/marnix/Zotero/storage/BNGWUGRY/Vandyke et al. - 2011 - Glottal Waveforms for Speaker Inference A Regressi.pdf}
}

@techreport{VanHorn2001,
  title = {A {{Maximum-entropy Solution}} to the {{Frame-dependency Problem}} in {{Speech Recognition}}},
  author = {Van Horn, Kevin S.},
  year = {2001},
  abstract = {The HMM assumption of conditional independence of observations causes a variety of problems for speech-recognition applications. Previous attempts to construct acoustic models that remove this assumption have suffered from a significant increase in the number of parameters to train. Another weakness of current acoustic models is that they do not account for the origin of derived features (estimated derivatives). We show how to both remove the independence assumption and properly account for derived features, with little or no increase in the number of parameters to train, by applying the principle of maximum entropy. We also show that ignoring the origins of derived features in training HMM acoustic models can lead to severe distortions of the effective language model. Evaluation of our maxent model on a simple problem cuts an already-low error rate in half compared to an equivalent HMM with the same number of parameters.},
  file = {/home/marnix/Zotero/storage/IUWZXZXT/Horn - 2001 - A Maximum-entropy Solution to the Frame-dependency.pdf;/home/marnix/Zotero/storage/829SCC9P/summary.html}
}

@article{VanHorn2001a,
  title = {Parameter Estimation for a Maximum-Entropy Acoustic Model},
  author = {Van Horn, Kevin},
  year = {2001},
  file = {/home/marnix/Zotero/storage/DCEAU5DL/3bda19fc29b77aea9108e1d7e85d3d2a9427.pdf}
}

@article{VanHorn2009,
  title = {Efficient {{Computation Of Statistics For Banded Multivariate Normal Distributions}}},
  author = {Van Horn, Kevin S.},
  year = {2009},
  abstract = {We look at the problem of computing statistics for a large multivariate normal distribution whose precision matrix has limited bandwidth. In particular, we wish to randomly sample from the distribution, compute its mean vector, and compute a central band of its covariance matrix. We show how to accomplish these tasks in time linear in the dimensionality of the distribution, for xed bandwidths. Our algorithms have applications in parameter estimation for certain maximum-entropy models.  Key words. maximum entropy, Monte Carlo methods, band matrix, multivariate normal, mean, covariance  AMS subject classications. 65C10, 65C50, 65C60  1. Problem Statement. We are given the following as input: 1. integers n, k, and  with 0  k   {$<$} n;  2. an n-variate, bandwidth-k normal form F = (A; b; c).  Definition 1.1. F = (A; b; c) is an n-variate normal form if A is an n  n,  symmetric, negative-denite, real matrix; b is a real n-vector; and c is a real scalar. We say that F has bandwidth k ...},
  file = {/home/marnix/Zotero/storage/DLGPHM7R/Horn and B - Efficient Computation Of Statistics For Banded Mul.pdf;/home/marnix/Zotero/storage/7TVPKNBN/summary.html}
}

@article{VanKuyk2017,
  title = {An {{Instrumental Intelligibility Metric Based}} on {{Information Theory}}},
  author = {Van Kuyk, Steven and Kleijn, W. B. and Hendriks, Richard Christian},
  year = {2017},
  journal = {IEEE Signal Processing Letters},
  volume = {9908},
  number = {c},
  pages = {1--5},
  issn = {10709908},
  doi = {10.1109/LSP.2017.2774250},
  keywords = {Intelligibility,mutual information},
  file = {/home/marnix/Zotero/storage/HUWB9LGD/Van Kuyk2017 An Instrumental Intelligibility Metric Based on Information Theory.pdf}
}

@inproceedings{VanKuyk2017a,
  title = {On the Information Rate of Speech Communication},
  booktitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Van Kuyk, Steven and Kleijn, W. Bastiaan and Hendriks, Richard C.},
  year = {2017},
  month = mar,
  pages = {5625--5629},
  publisher = {IEEE},
  doi = {10.1109/ICASSP.2017.7953233},
  isbn = {978-1-5090-4117-6},
  file = {/home/marnix/Zotero/storage/XRIRTQY6/Van Kuyk2017 On the information rate of speech communication.pdf}
}

@article{VanNierop1973,
  title = {Frequency Analysis of {{Dutch}} Vowels from 25 Female Speakers},
  author = {Van Nierop, Dick JPJ and Pols, Louis CW and Plomp, Reinier},
  year = {1973},
  journal = {Acta Acustica united with Acustica},
  volume = {29},
  number = {2},
  pages = {110--118},
  publisher = {S. Hirzel Verlag},
  file = {/home/marnix/Zotero/storage/C4MK3QST/Van Nierop et al. - 1973 - Frequency analysis of Dutch vowels from 25 female .pdf}
}

@misc{VanSoom2017,
  title = {Multiscale Analysis of the {{Russian}} Interbank Loan Network},
  booktitle = {Masther Thesis at Ghent University},
  author = {Van Soom, Marnix},
  year = {2017},
  publisher = {UGent},
  keywords = {interbank,multiplex,multiscale,network,Russian,thesis,UGent},
  file = {/home/marnix/Zotero/storage/4E4NRBIY/Van Soom - 2017 - Multiscale analysis of the Russian interbank loan network.pdf;/home/marnix/Zotero/storage/HB7MWLIR/pres.pdf}
}

@article{VanSoom2019,
  title = {Loan Maturity Aggregation in Interbank Lending Networks Obscures Mesoscale Structure and Economic Functions},
  author = {Van Soom, Marnix and {van den Heuvel}, Milan and Ryckebusch, Jan and Schoors, Koen},
  year = {2019},
  month = aug,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {1--14},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-48924-5},
  urldate = {2019-09-11},
  abstract = {Since the 2007--2009 financial crisis, substantial academic effort has been dedicated to improving our understanding of interbank lending networks (ILNs). Because of data limitations or by choice, the literature largely lacks multiple loan maturities. We employ a complete interbank loan contract dataset to investigate whether maturity details are informative of the network structure. Applying the layered stochastic block model of Peixoto (2015) and other tools from network science on a time series of bilateral loans with multiple maturity layers in the Russian ILN, we find that collapsing all such layers consistently obscures mesoscale structure. The optimal maturity granularity lies between completely collapsing and completely separating the maturity layers and depends on the development phase of the interbank market, with a more developed market requiring more layers for optimal description. Closer inspection of the inferred maturity bins associated with the optimal maturity granularity reveals specific economic functions, from liquidity intermediation to financing. Collapsing a network with multiple underlying maturity layers or extracting one such layer, common in economic research, is therefore not only an incomplete representation of the ILN's mesoscale structure, but also conceals existing economic functions. This holds important insights and opportunities for theoretical and empirical studies on interbank market functioning, contagion, stability, and on the desirable level of regulatory data disclosure.},
  copyright = {2019 The Author(s)},
  langid = {english},
  file = {/home/marnix/Zotero/storage/HSN9E7NU/Soom et al. - 2019 - Loan maturity aggregation in interbank lending net.pdf;/home/marnix/Zotero/storage/VEGLGTQH/s41598-019-48924-5.html}
}

@article{VanSoom2019a,
  title = {A {{New Approach}} to the {{Formant Measuring Problem}}},
  author = {Van Soom, Marnix and {de Boer}, Bart},
  year = {2019},
  journal = {Proceedings},
  volume = {33},
  number = {1},
  pages = {29},
  doi = {10.3390/proceedings2019033029},
  urldate = {2020-01-13},
  abstract = {Formants are characteristic frequency components in human speech that are caused by resonances in the vocal tract during speech production. They are of primary concern in acoustic phonetics and speech recognition. Despite this, making accurate measurements of the formants, which we dub ``the formant measurement problem'' for convenience, is as yet not considered to be fully resolved. One particular shortcoming is the lack of error bars on the formant frequencies' estimates. As a first step towards remedying this, we propose a new approach for the formant measuring problem in the particular case of steady-state vowels---a case which occurs quite abundantly in natural speech. The approach is to look at the formant measuring problem from the viewpoint of Bayesian spectrum analysis. We develop a pitch-synchronous linear model for steady-state vowels and apply it to the open-mid front unrounded vowel [{$\varepsilon$}] observed in a real speech utterance.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {acoustic phonetics,Bayesian inference,formant,general linear model,steady-state,vowel},
  file = {/home/marnix/Zotero/storage/NBSVWBYP/Van Soom and de Boer - 2019 - A New Approach to the Formant Measuring Problem.pdf;/home/marnix/Zotero/storage/DQULM357/29.html}
}

@article{VanSoom2020,
  title = {Detrending the {{Waveforms}} of {{Steady-State Vowels}}},
  author = {Van Soom, Marnix and {de Boer}, Bart},
  year = {2020},
  month = mar,
  journal = {Entropy},
  volume = {22},
  number = {3},
  pages = {331},
  publisher = {Multidisciplinary Digital Publishing Institute},
  doi = {10.3390/e22030331},
  urldate = {2020-03-28},
  abstract = {Steady-state vowels are vowels that are uttered with a momentarily fixed vocal tract configuration and with steady vibration of the vocal folds. In this steady-state, the vowel waveform appears as a quasi-periodic string of elementary units called pitch periods. Humans perceive this quasi-periodic regularity as a definite pitch. Likewise, so-called pitch-synchronous methods exploit this regularity by using the duration of the pitch periods as a natural time scale for their analysis. In this work, we present a simple pitch-synchronous method using a Bayesian approach for estimating formants that slightly generalizes the basic approach of modeling the pitch periods as a superposition of decaying sinusoids, one for each vowel formant, by explicitly taking into account the additional low-frequency content in the waveform which arises not from formants but rather from the glottal pulse. We model this low-frequency content in the time domain as a polynomial trend function that is added to the decaying sinusoids. The problem then reduces to a rather familiar one in macroeconomics: estimate the cycles (our decaying sinusoids) independently from the trend (our polynomial trend function); in other words, detrend the waveform of steady-state waveforms. We show how to do this efficiently.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {acoustic phonetics,detrending,formant,model averaging,nested sampling,probability theory,source-filter theory,steady-state,uncertainty quantification,vowel},
  file = {/home/marnix/Zotero/storage/KZMZWUSP/Van Soom and de Boer - 2020 - Detrending the Waveforms of Steady-State Vowels.pdf;/home/marnix/Zotero/storage/ZWUQJW4H/331.html}
}

@article{VanSoom2021,
  title = {A {{Weakly Informative Prior}} for {{Resonance Frequencies}}},
  author = {Van Soom, Marnix and {de Boer}, Bart},
  year = {2021},
  journal = {Physical Sciences Forum},
  volume = {3},
  number = {1},
  pages = {2},
  publisher = {Multidisciplinary Digital Publishing Institute},
  doi = {10.3390/psf2021003002},
  urldate = {2021-11-08},
  abstract = {We derive a weakly informative prior for a set of ordered resonance frequencies from Jaynes' principle of maximum entropy. The prior facilitates model selection problems in which both the number and the values of the resonance frequencies are unknown. It encodes a weakly inductive bias, provides a reasonable density everywhere, is easily parametrizable, and is easy to sample. We hope that this prior can enable the use of robust evidence-based methods for a new class of problems, even in the presence of multiplets of arbitrary order.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {maximum entropy,model selection,resonance frequency,weakly uninformative prior},
  file = {/home/marnix/Zotero/storage/93RWSJLJ/main.pdf;/home/marnix/Zotero/storage/YXNWCXNV/Van Soom and de Boer - 2021 - A Weakly Informative Prior for Resonance Frequenci.pdf;/home/marnix/Zotero/storage/NQ6N5JTD/2.html}
}

@inproceedings{VanSoom2022,
  title = {Softly {{Constrained Agent-Based Models}}},
  shorttitle = {Softly {{Constrainted Agent-Based Models}}},
  booktitle = {Joint {{Conference}} on {{Language Evolution}}},
  author = {Van Soom, Marnix and De Boer, Bart},
  year = {2022},
  address = {Kanazawa, Japan},
  copyright = {All rights reserved},
  file = {/home/marnix/Zotero/storage/EDPU5Q89/ml4evolang_2022_program.pdf;/home/marnix/Zotero/storage/MHGBGJSQ/Van Soom and De Boer - 2022 - Softly Constrainted Agent-Based Models.pdf}
}

@book{Vaughn2008,
  title = {Introduction to {{Mathematical Physics}}},
  author = {Vaughn, Mt},
  year = {2008},
  isbn = {978-3-527-40627-2},
  file = {/home/marnix/Zotero/storage/F4NCGGZJ/Introduction to Mathematical Physics(Michael T. Vaughn).pdf}
}

@book{Veld2014,
  title = {Complex {{Systems}} in {{Financial Economics}}: {{Applications}} to {{Interbank}} and {{Stock Markets}}},
  booktitle = {2015},
  author = {Veld, Daan Laurens and Laurens, Daan},
  year = {2014},
  publisher = {Universiteit van Amsterdam [Host]},
  file = {/home/marnix/Zotero/storage/FXJGCN98/Laurens - 2008 - Complex Systems in Financial Economics Applications to Interbank and Stock Markets.pdf}
}

@article{Veldhuis1998,
  title = {A Computationally Efficient Alternative for the {{Liljencrants-Fant}} Model and Its Perceptual Evaluation},
  author = {Veldhuis, R.},
  year = {1998},
  month = jan,
  journal = {The Journal of the Acoustical Society of America},
  volume = {103},
  number = {1},
  pages = {566--571},
  issn = {0001-4966},
  doi = {10.1121/1.421103},
  abstract = {An alternative for the Liljencrants-Fant (LF) glottal-pulse model is presented. This alternative is derived from the Rosenberg model. Therefore, it is called the Rosenberg++ model. In the derivation a general framework is used for glottal-pulse models. The Rosenberg++ model is described by the same set of T or R parameters as the LF model but it has the advantage over the LF model that it is computationally more efficient. It is compared with the LF model in a psychoacoustic experiment, from which it is concluded that in a practical situation it is capable of producing synthetic speech which is perceptually equivalent to speech generated with the LF model.},
  langid = {english},
  pmid = {9440341},
  keywords = {{Models, Biological},Auditory Perception,Electronic Data Processing,Humans}
}

@article{Verdolini1995,
  title = {The {{Application}} of {{Laboratory Formulas}} to {{Clinical Voice Management}}},
  author = {Verdolini, Katherine and Titze, Ingo R.},
  year = {1995},
  month = may,
  journal = {American Journal of Speech-Language Pathology},
  volume = {4},
  number = {2},
  pages = {62--69},
  publisher = {American Speech-Language-Hearing Association},
  doi = {10.1044/1058-0360.0402.62},
  urldate = {2025-09-26},
  abstract = {In this paper, we discuss the application of mathematical formulas to guide the development of clinical interventions in voice disorders. Discussion of case examples includes fundamental frequency and intensity deviations, pitch and loudness abnormalities, laryngeal hyperand hypoadduction, and phonatory effort. The paper illustrates the interactive nature of theoretical and applied work in vocology}
}

@article{Vernikov2016,
  title = {A {{Guide}} to {{Russian Bank Data}}: {{Breaking Down}} the {{Sample}} of {{Banks}}},
  author = {Vernikov, Andrei},
  year = {2016},
  journal = {Available at SSRN 2600738},
  file = {/home/marnix/Zotero/storage/88QS2V9Q/Vernikov - 2016 - A Guide to Russian Bank Data Breaking Down the Sample of Banks.pdf}
}

@article{Vinck2012,
  title = {Estimation of the Entropy Based on Its Polynomial Representation},
  author = {Vinck, Martin and Battaglia, Francesco P. and Balakirsky, Vladimir B. and Vinck, A. J. Han and Pennartz, Cyriel M. A.},
  year = {2012},
  month = may,
  journal = {Physical Review E},
  volume = {85},
  number = {5},
  pages = {051139},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.85.051139},
  urldate = {2021-12-21},
  abstract = {Estimating entropy from empirical samples of finite size is of central importance for information theory as well as the analysis of complex statistical systems. Yet, this delicate task is marred by intrinsic statistical bias. Here we decompose the entropy function into a polynomial approximation function and a remainder function. The approximation function is based on a Taylor expansion of the logarithm. Given n observations, we give an unbiased, linear estimate of the first n power series terms based on counting sets of k coincidences. For the remainder function we use nonlinear Bayesian estimation with a nearly flat prior distribution on the entropy that was developed by Nemenman, Shafee, and Bialek. Our simulations show that the combined entropy estimator has reduced bias in comparison to other available estimators.},
  file = {/home/marnix/Zotero/storage/FT6QG48S/Vinck et al. - 2012 - Estimation of the entropy based on its polynomial .pdf;/home/marnix/Zotero/storage/CMDRP7QL/PhysRevE.85.html}
}

@article{Viset2024,
  title = {Exploiting {{Hankel-Toeplitz Structures}} for {{Fast Computation}} of {{Kernel Precision Matrices}}},
  author = {Viset, Frida Marie and Kullberg, Anton and Wesel, Frederiek and Solin, Arno},
  year = {2024},
  month = may,
  journal = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  urldate = {2025-04-24},
  abstract = {The Hilbert-space Gaussian process (HGP) approach offers a hyperparameter-independent basis function approximation for speeding up Gaussian process (GP) inference by projecting the GP onto \$M\$ basis functions. These properties result in a favorable data-independent \${\textbackslash}mathcal\{O\}(M{\textasciicircum}3)\$ computational complexity during hyperparameter optimization but require a dominating one-time precomputation of the precision matrix costing \${\textbackslash}mathcal\{O\}(NM{\textasciicircum}2)\$ operations. In this paper, we lower this dominating computational complexity to \${\textbackslash}mathcal\{O\}(NM)\$ with no additional approximations. We can do this because we realize that the precision matrix can be split into a sum of Hankel-Toeplitz matrices, each having \${\textbackslash}mathcal\{O\}(M)\$ unique entries. Based on this realization we propose computing only these unique entries at \${\textbackslash}mathcal\{O\}(NM)\$ costs. Further, we develop two theorems that prescribe sufficient conditions for the complexity reduction to hold generally for a wide range of other approximate GP models, such as the Variational Fourier features approach. The two theorems do this with no assumptions on the data and no additional approximations of the GP models themselves. Thus, our contribution provides a pure speed-up of several existing, widely used, GP approximations, without further approximations},
  langid = {english},
  file = {/home/marnix/Zotero/storage/RUQD7DK5/Viset et al. - 2024 - Exploiting Hankel-Toeplitz Structures for Fast Computation of Kernel Precision Matrices.pdf}
}

@article{Viterbi1967,
  title = {Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm},
  author = {Viterbi, A.},
  year = {1967},
  month = apr,
  journal = {IEEE Transactions on Information Theory},
  volume = {13},
  number = {2},
  pages = {260--269},
  issn = {1557-9654},
  doi = {10.1109/TIT.1967.1054010},
  abstract = {The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates aboveR\_0, the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates aboveR\_0and whose performance bears certain similarities to that of sequential decoding algorithms.},
  file = {/home/marnix/Zotero/storage/SKBFBKK5/Viterbi - 1967 - Error bounds for convolutional codes and an asympt.pdf;/home/marnix/Zotero/storage/MJSFSG72/stamp.html}
}

@book{vonderLinden2014,
  title = {Bayesian {{Probability Theory}}: {{Applications}} in the {{Physical Sciences}}},
  shorttitle = {Bayesian {{Probability Theory}}},
  author = {{von der Linden}, Wolfgang and Dose, Volker and {von Toussaint}, Udo},
  year = {2014},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9781139565608},
  urldate = {2021-03-08},
  isbn = {978-1-139-56560-8},
  langid = {english},
  file = {/home/marnix/Zotero/storage/2SXTJLMS/von der Linden et al. - 2014 - Bayesian Probability Theory Applications in the P.pdf}
}

@article{Vorperian2019,
  title = {Corner Vowels in Males and Females Ages 4 to 20 Years: {{Fundamental}} and {{F1}}--{{F4}} Formant Frequencies},
  shorttitle = {Corner Vowels in Males and Females Ages 4 to 20 Years},
  author = {Vorperian, Houri K. and Kent, Raymond D. and Lee, Yen and Bolt, Daniel M.},
  year = {2019},
  month = nov,
  journal = {The Journal of the Acoustical Society of America},
  volume = {146},
  number = {5},
  pages = {3255--3274},
  issn = {0001-4966},
  doi = {10.1121/1.5131271},
  urldate = {2021-01-19},
  abstract = {The purpose of this study was to determine the developmental trajectory of the four corner vowels' fundamental frequency (fo) and the first four formant frequencies (F1--F4), and to assess when speaker-sex differences emerge. Five words per vowel, two of which were produced twice, were analyzed for fo and estimates of the first four formants frequencies from 190 (97 female, 93 male) typically developing speakers ages 4--20\,years old. Findings revealed developmental trajectories with decreasing values of fo and formant frequencies. Sex differences in fo emerged at age 7. The decrease of fo was larger in males than females with a marked drop during puberty. Sex differences in formant frequencies appeared at the earliest age under study and varied with vowel and formant. Generally, the higher formants (F3-F4) were sensitive to sex differences. Inter- and intra-speaker variability declined with age but had somewhat different patterns, likely reflective of maturing motor control that interacts with the changing anatomy. This study reports a source of developmental normative data on fo and the first four formants in both sexes. The different developmental patterns in the first four formants and vowel-formant interactions in sex differences likely point to anatomic factors, although speech-learning phenomena cannot be discounted.},
  pmcid = {PMC6850954},
  pmid = {31795713},
  file = {/home/marnix/Zotero/storage/Z57IPN7G/Vorperian et al. - 2019 - Corner vowels in males and females ages 4 to 20 ye.pdf}
}

@article{Wagener2012,
  title = {Why Do {{People}} (Not) {{Cough}} in {{Concerts}}? {{The}} Economics of Concert Etiquette},
  author = {Wagener, Andreas},
  year = {2012},
  publisher = {Citeseer},
  file = {/home/marnix/Zotero/storage/HWGTYG9D/Wagener - Why Do People (Not) Cough in Concerts The Economi.pdf}
}

@article{Wainwright2007,
  title = {Graphical {{Models}}, {{Exponential Families}}, and {{Variational Inference}}},
  author = {Wainwright, Martin J. and Jordan, Michael I.},
  year = {2007},
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  volume = {1},
  number = {1--2},
  pages = {1--305},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000001},
  urldate = {2020-06-30},
  langid = {english},
  file = {/home/marnix/Zotero/storage/6USJ755U/Wainwright and Jordan - 2007 - Graphical Models, Exponential Families, and Variat.pdf}
}

@inproceedings{Walker2005,
  title = {Advanced Methods for Glottal Wave Extraction},
  booktitle = {Nonlinear Analyses and Algorithms for Speech Processing: {{International}} Conference on Non-Linear Speech Processing, {{NOLISP}} 2005, Barcelona, Spain, April 19-22, 2005, Revised Selected Papers},
  author = {Walker, Jacqueline and Murphy, Peter},
  year = {2005},
  pages = {139--149},
  organization = {Springer},
  file = {/home/marnix/Zotero/storage/2F4F2EQ4/Walker and Murphy - 2005 - Advanced methods for glottal wave extraction.pdf}
}

@incollection{Walker2007,
  title = {A {{Review}} of {{Glottal Waveform Analysis}}},
  booktitle = {Progress in {{Nonlinear Speech Processing}}},
  author = {Walker, Jacqueline and Murphy, Peter},
  editor = {Stylianou, Yannis and {Faundez-Zanuy}, Marcos and Esposito, Anna},
  year = {2007},
  volume = {4391},
  pages = {1--21},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-71505-4_1},
  urldate = {2023-04-14},
  isbn = {978-3-540-71503-0 978-3-540-71505-4},
  langid = {english},
  file = {/home/marnix/Zotero/storage/WKU5ZK32/Walker and Murphy - 2007 - A Review of Glottal Waveform Analysis.pdf}
}

@inproceedings{Wang2012,
  title = {Truncation-Free Stochastic Variational Inference for {{Bayesian}} Nonparametric Models},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 1},
  author = {Wang, Chong and Blei, David M.},
  year = {2012},
  month = dec,
  series = {{{NIPS}}'12},
  pages = {413--421},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2020-12-23},
  abstract = {We present a truncation-free stochastic variational inference algorithm for Bayesian nonparametric models. While traditional variational inference algorithms require truncations for the model or the variational distribution, our method adapts model complexity on the fly. We studied our method with Dirichlet process mixture models and hierarchical Dirichlet process topic models on two large data sets. Our method performs better than previous stochastic variational inference algorithms.}
}

@article{Wang2013,
  title = {{{MCMC}} Methods for {{Gaussian}} Process Models Using Fast Approximations for the Likelihood},
  author = {Wang, Chunyi and Neal, Radford M.},
  year = {2013},
  number = {Mcmc},
  eprint = {1305.2235},
  pages = {1--21},
  abstract = {Gaussian Process (GP) models are a powerful and flexible tool for non-parametric regression and classification. Computation for GP models is intensive, since computing the posterior density, \${\textbackslash}pi\$, for covariance function parameters requires computation of the covariance matrix, C, a \$pn{\textasciicircum}2\$ operation, where p is the number of covariates and n is the number of training cases, and then inversion of C, an \$n{\textasciicircum}3\$ operation. We introduce MCMC methods based on the "temporary mapping and caching" framework, using a fast approximation, \${\textbackslash}pi{\textasciicircum}*\$, as the distribution needed to construct the temporary space. We propose two implementations under this scheme: "mapping to a discretizing chain", and "mapping with tempered transitions", both of which are exactly correct MCMC methods for sampling \${\textbackslash}pi\$, even though their transitions are constructed using an approximation. These methods are equivalent when their tuning parameters are set at the simplest values, but differ in general. We compare how well these methods work when using several approximations, finding on synthetic datasets that a \${\textbackslash}pi{\textasciicircum}*\$ based on the "Subset of Data" (SOD) method is almost always more efficient than standard MCMC using only \${\textbackslash}pi\$. On some datasets, a more sophisticated \${\textbackslash}pi{\textasciicircum}*\$ based on the "Nystr{\textbackslash}"om-Cholesky" method works better than SOD.},
  archiveprefix = {arXiv},
  arxivid = {1305.2235},
  file = {/home/marnix/Zotero/storage/7VPJNDUS/Wang2013 MCMC methods for Gaussian process models using fast approximations for the likelihood.pdf}
}

@article{Wang2013a,
  title = {Variational Inference in Nonconjugate Models},
  author = {Wang, Chong and Blei, David M},
  year = {2013},
  journal = {Journal of Machine Learning Research},
  volume = {14},
  number = {Apr},
  pages = {1005--1031},
  file = {/home/marnix/Zotero/storage/8YAEYFNM/Wang - Variational Inference in Nonconjugate Models.pdf}
}

@inproceedings{Wang2016,
  title = {Sequential Inference for Deep {{Gaussian}} Process},
  booktitle = {Artificial Intelligence and Statistics},
  author = {Wang, Yali and Brubaker, Marcus and {Chaib-Draa}, Brahim and Urtasun, Raquel},
  year = {2016},
  pages = {694--703},
  file = {/home/marnix/Zotero/storage/FGYM4874/Wang et al. - 2016 - Sequential inference for deep Gaussian process.pdf}
}

@inproceedings{Wang2016a,
  title = {Use of Particle Filtering and {{MCMC}} for Inference in {{Probabilistic Acoustic Tube}} Model},
  booktitle = {2016 {{IEEE Statistical Signal Processing Workshop}} ({{SSP}})},
  author = {Wang, Ruobai and Zhang, Yang and Ou, Zhijian and {Hasegawa-Johnson}, Mark},
  year = {2016},
  month = jun,
  pages = {1--5},
  doi = {10.1109/SSP.2016.7551748},
  abstract = {The Probabilistic Acoustic Tube (PAT) model is a probabilistic generative model of speech. By associating every generative parameter with a probability distribution, it becomes possible to convert every standard speech analysis task into a probabilistic inference task, thereby grounding every such task with quantifiable measures of bias and consistency. The previously published PAT model did not adequately model AM-FM and therefore phase of the voice source. In this paper, we model the AM-FM of the voice source using an autoregressive process. The resulting model is a non-linear state-space model and thus has no closed-form inference algorithm, but effective inference can be achieved by using Auxiliary Particle Filtering (APF) and Taylor expansion assisted Markov Chain Monte Carlo (MCMC). Results demonstrate that, unlike previous speech models, this model is able to account for the phase of the voice source, achieving signal reconstruction with 8.79dB SNR.},
  keywords = {MCMC,Monte Carlo methods,particle filter,Probabilistic logic,Proposals,Signal processing,Speech,Speech modeling,State-space methods,Taylor series},
  file = {/home/marnix/Zotero/storage/FGPVNXV5/Wang et al. - 2016 - Use of particle filtering and MCMC for inference i.pdf;/home/marnix/Zotero/storage/FYMAIYXR/stamp.html}
}

@misc{Wang2019,
  title = {Exact {{Gaussian Processes}} on a {{Million Data Points}}},
  author = {Wang, Ke Alexander and Pleiss, Geoff and Gardner, Jacob R. and Tyree, Stephen and Weinberger, Kilian Q. and Wilson, Andrew Gordon},
  year = {2019},
  month = dec,
  number = {arXiv:1903.08114},
  eprint = {1903.08114},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1903.08114},
  urldate = {2023-01-05},
  abstract = {Gaussian processes (GPs) are flexible non-parametric models, with a capacity that grows with the available data. However, computational constraints with standard inference procedures have limited exact GPs to problems with fewer than about ten thousand training points, necessitating approximations for larger datasets. In this paper, we develop a scalable approach for exact GPs that leverages multi-GPU parallelization and methods like linear conjugate gradients, accessing the kernel matrix only through matrix multiplication. By partitioning and distributing kernel matrix multiplies, we demonstrate that an exact GP can be trained on over a million points, a task previously thought to be impossible with current computing hardware, in less than 2 hours. Moreover, our approach is generally applicable, without constraints to grid data or specific kernel classes. Enabled by this scalability, we perform the first-ever comparison of exact GPs against scalable GP approximations on datasets with \$10{\textasciicircum}4 {\textbackslash}!-{\textbackslash}! 10{\textasciicircum}6\$ data points, showing dramatic performance improvements.},
  archiveprefix = {arXiv},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing},Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/PAPLDZHE/Wang et al. - 2019 - Exact Gaussian Processes on a Million Data Points.pdf;/home/marnix/Zotero/storage/YQLUPX8H/1903.html}
}

@article{Watanabe2011,
  title = {Bayesian {{Approaches}} in {{Speech Recognition}} {$\ast$}},
  author = {Watanabe, Shinji},
  year = {2011},
  number = {Mdl},
  file = {/home/marnix/Zotero/storage/SHQ93ITH/Watanabe2011 Bayesian Approaches in Speech Recognition.pdf}
}

@article{Watts1998,
  title = {Collective Dynamics of Small-World Networks},
  author = {Watts, D J and Strogatz, S H},
  year = {1998},
  month = jun,
  journal = {Nature},
  volume = {393},
  number = {6684},
  pages = {440--442},
  publisher = {Nature Publishing Group},
  location = {Department of Theoretical and Applied Mechanics, Cornell University, Ithaca, New York 14853, USA. djw24@columbia.edu},
  issn = {0028-0836},
  doi = {10.1038/30918},
  abstract = {Networks of coupled dynamical systems have been used to model biological oscillators1, 2, 3, 4, Josephson junction arrays5,6, excitable media7, neural networks8, 9, 10, spatial games11, genetic control networks12 and many other self-organizing systems. Ordinarily, the connection topology is assumed to be either completely regular or completely random. But many biological, technological and social networks lie somewhere between these two extremes. Here we explore simple models of networks that can be tuned through this middle ground: regular networks 'rewired' to introduce increasing amounts of disorder. We find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. We call them 'small-world' networks, by analogy with the small-world phenomenon13,14 (popularly known as six degrees of separation15). The neural network of the worm Caenorhabditis elegans, the power grid of the western United States, and the collaboration graph of film actors are shown to be small-world networks. Models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. In particular, infectious diseases spread more easily in small-world networks than in regular lattices.},
  pmid = {9623998},
  keywords = {collective,networks,small-world},
  file = {/home/marnix/Zotero/storage/DTQ4PZQ7/Watts, Strogatz - 1998 - Collective dynamics of small-world networks.pdf}
}

@misc{Weber2025,
  title = {Laplax -- {{Laplace Approximations}} with {{JAX}}},
  author = {Weber, Tobias and Mucs{\'a}nyi, B{\'a}lint and Rommel, Lenard and Christie, Thomas and Kas{\"u}schke, Lars and Pf{\"o}rtner, Marvin and Hennig, Philipp},
  year = {2025},
  month = jul,
  number = {arXiv:2507.17013},
  eprint = {2507.17013},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.17013},
  urldate = {2025-09-11},
  abstract = {The Laplace approximation provides a scalable and efficient means of quantifying weight-space uncertainty in deep neural networks, enabling the application of Bayesian tools such as predictive uncertainty and model selection via Occam's razor. In this work, we introduce laplax, a new open-source Python package for performing Laplace approximations with jax. Designed with a modular and purely functional architecture and minimal external dependencies, laplax offers a flexible and researcher-friendly framework for rapid prototyping and experimentation. Its goal is to facilitate research on Bayesian neural networks, uncertainty quantification for deep learning, and the development of improved Laplace approximation techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/marnix/Zotero/storage/5CPTS93H/Weber et al. - 2025 - laplax -- Laplace Approximations with JAX.pdf;/home/marnix/Zotero/storage/5R6V5A5A/2507.html}
}

@misc{Wenger2023,
  title = {Posterior and {{Computational Uncertainty}} in {{Gaussian Processes}}},
  author = {Wenger, Jonathan and Pleiss, Geoff and Pf{\"o}rtner, Marvin and Hennig, Philipp and Cunningham, John P.},
  year = {2023},
  month = oct,
  number = {arXiv:2205.15449},
  eprint = {2205.15449},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.15449},
  urldate = {2025-09-04},
  abstract = {Gaussian processes scale prohibitively with the size of the dataset. In response, many approximation methods have been developed, which inevitably introduce approximation error. This additional source of uncertainty, due to limited computation, is entirely ignored when using the approximate posterior. Therefore in practice, GP models are often as much about the approximation method as they are about the data. Here, we develop a new class of methods that provides consistent estimation of the combined uncertainty arising from both the finite number of data observed and the finite amount of computation expended. The most common GP approximations map to an instance in this class, such as methods based on the Cholesky factorization, conjugate gradients, and inducing points. For any method in this class, we prove (i) convergence of its posterior mean in the associated RKHS, (ii) decomposability of its combined posterior covariance into mathematical and computational covariances, and (iii) that the combined variance is a tight worst-case bound for the squared error between the method's posterior mean and the latent function. Finally, we empirically demonstrate the consequences of ignoring computational uncertainty and show how implicitly modeling it improves generalization performance on benchmark datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Numerical Analysis,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/PZ43TE9A/Wenger et al. - 2023 - Posterior and Computational Uncertainty in Gaussian Processes.pdf}
}

@article{Whalen2022,
  title = {Formants Are Easy to Measure; Resonances, Not so Much: {{Lessons}} from {{Klatt}} (1986)},
  shorttitle = {Formants Are Easy to Measure; Resonances, Not so Much},
  author = {Whalen, D. H. and Chen, Wei-Rong and Shadle, Christine H. and Fulop, Sean A.},
  year = {2022},
  month = aug,
  journal = {The Journal of the Acoustical Society of America},
  volume = {152},
  number = {2},
  pages = {933--941},
  publisher = {Acoustical Society of America},
  issn = {0001-4966},
  doi = {10.1121/10.0013410},
  urldate = {2022-08-18},
  file = {/home/marnix/Zotero/storage/IS883DFE/Whalen et al. - 2022 - Formants are easy to measure; resonances, not so m.pdf}
}

@misc{Wiedemer2025,
  title = {Video Models Are Zero-Shot Learners and Reasoners},
  author = {Wiedemer, Thadd{\"a}us and Li, Yuxuan and Vicol, Paul and Gu, Shixiang Shane and Matarese, Nick and Swersky, Kevin and Kim, Been and Jaini, Priyank and Geirhos, Robert},
  year = {2025},
  month = sep,
  number = {arXiv:2509.20328},
  eprint = {2509.20328},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2509.20328},
  urldate = {2025-10-08},
  abstract = {The remarkable zero-shot capabilities of Large Language Models (LLMs) have propelled natural language processing from task-specific models to unified, generalist foundation models. This transformation emerged from simple primitives: large, generative models trained on web-scale data. Curiously, the same primitives apply to today's generative video models. Could video models be on a trajectory towards general-purpose vision understanding, much like LLMs developed general-purpose language understanding? We demonstrate that Veo 3 can solve a broad variety of tasks it wasn't explicitly trained for: segmenting objects, detecting edges, editing images, understanding physical properties, recognizing object affordances, simulating tool use, and more. These abilities to perceive, model, and manipulate the visual world enable early forms of visual reasoning like maze and symmetry solving. Veo's emergent zero-shot capabilities indicate that video models are on a path to becoming unified, generalist vision foundation models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/marnix/Zotero/storage/53TJNEUT/Wiedemer et al. - 2025 - Video models are zero-shot learners and reasoners.pdf;/home/marnix/Zotero/storage/IJN9JBPB/2509.html}
}

@techreport{Wiemers2003,
  title = {Why Do We Have an Interbank Money Market?},
  author = {Wiemers, J{\"u}rgen and Neyer, Ulrike},
  year = {2003},
  institution = {IWH Discussion Papers},
  file = {/home/marnix/Zotero/storage/5XK8LKT2/Wiemers, Neyer - 2003 - Why do we have an interbank money market.pdf}
}

@misc{Wilk2020,
  title = {A {{Framework}} for {{Interdomain}} and {{Multioutput Gaussian Processes}}},
  author = {van der Wilk, Mark and Dutordoir, Vincent and John, S. T. and Artemev, Artem and Adam, Vincent and Hensman, James},
  year = {2020},
  month = mar,
  number = {arXiv:2003.01115},
  eprint = {2003.01115},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2003.01115},
  urldate = {2025-05-28},
  abstract = {One obstacle to the use of Gaussian processes (GPs) in large-scale problems, and as a component in deep learning system, is the need for bespoke derivations and implementations for small variations in the model or inference. In order to improve the utility of GPs we need a modular system that allows rapid implementation and testing, as seen in the neural network community. We present a mathematical and software framework for scalable approximate inference in GPs, which combines interdomain approximations and multiple outputs. Our framework, implemented in GPflow, provides a unified interface for many existing multioutput models, as well as more recent convolutional structures. This simplifies the creation of deep models with GPs, and we hope that this work will encourage more interest in this approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/8BQF8NNJ/Wilk et al. - 2020 - A Framework for Interdomain and Multioutput Gaussian Processes.pdf;/home/marnix/Zotero/storage/DVSYB42K/2003.html}
}

@article{Wilkinson,
  title = {Bayes--{{Newton Methods}} for {{Approximate Bayesian Inference}} with {{PSD Guarantees}}},
  author = {Wilkinson, William J and Sarkka, Simo and Solin, Arno},
  abstract = {We formulate natural gradient variational inference (VI), expectation propagation (EP), and posterior linearisation (PL) as generalisations of Newton's method for optimising the parameters of a Bayesian posterior distribution. This viewpoint explicitly casts inference algorithms under the framework of numerical optimisation. We show that common approximations to Newton's method from the optimisation literature, namely Gauss--Newton and quasi-Newton methods (e.g., the BFGS algorithm), are still valid under this `Bayes--Newton' framework. This leads to a suite of novel algorithms which are guaranteed to result in positive semi-definite (PSD) covariance matrices, unlike standard VI and EP. Our unifying viewpoint provides new insights into the connections between various inference schemes. All the presented methods apply to any model with a Gaussian prior and non-conjugate likelihood, which we demonstrate with (sparse) Gaussian processes and state space models.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/8I27VYAN/Wilkinson et al. - Bayes–Newton Methods for Approximate Bayesian Inference with PSD Guarantees.pdf}
}

@phdthesis{Wilkinson2019,
  type = {Thesis},
  title = {Gaussian {{Process Modelling}} for {{Audio Signals}}},
  author = {Wilkinson, W.},
  year = {2019},
  month = oct,
  urldate = {2020-03-30},
  abstract = {Audio signals are characterised and perceived based on how their spectral make-up changes with time. Uncovering the behaviour of latent spectral components is at the heart of many real-world applications involving sound, but is a highly ill-posed task given the infi nite number of ways any signal can be decomposed. This motivates the use of prior knowledge and a probabilistic modelling paradigm that can characterise uncertainty. This thesis studies the application of Gaussian processes to audio, which offer a principled non-parametric way to specify probability distributions over functions whilst also encoding prior knowledge. Along the way we consider what prior knowledge we have about sound, the way it behaves, and the way it is perceived, and write down these assumptions in the form of probabilistic models. We show how Bayesian time-frequency analysis can be reformulated as a spectral mixture Gaussian process, and utilise modern day inference methods to carry out joint time-frequency analysis and nonnegative matrix factorisation. Our reformulation results in increased modelling flexibility, allowing more sophisticated prior knowledge to be encoded, which improves performance on a missing data synthesis task. We demonstrate the generality of this paradigm by showing how the joint model can additionally be applied to both denoising and source separation tasks without modi cation. We propose a hybrid statistical-physical model for audio spectrograms based on observations about the way amplitude envelopes decay over time, as well as a nonlinear model based on deep Gaussian processes. We examine the benefi ts of these methods, all of which are generative in the sense that novel signals can be sampled from the underlying models, allowing us to consider the extent to which they encode the important perceptual characteristics of sound.},
  langid = {english},
  school = {Queen Mary University of London},
  annotation = {Accepted: 2019-11-13T10:56:23Z},
  file = {/home/marnix/Zotero/storage/ZDNZYRMP/Wilkinson - 2019 - Gaussian Process Modelling for Audio Signals.pdf;/home/marnix/Zotero/storage/MG2NF6V3/61329.html}
}

@article{Willems1997,
  title = {On Interconnections, Control, and Feedback},
  author = {Willems, J.C.},
  year = {1997},
  month = mar,
  journal = {IEEE Transactions on Automatic Control},
  volume = {42},
  number = {3},
  pages = {326--339},
  issn = {00189286},
  doi = {10.1109/9.557576},
  urldate = {2020-03-14},
  abstract = {The purpose of this paper is to study interconnections and control of dynamical systems in a behavioral context. We start with an extensive physical example which serves to illustrate that the familiar input--output feedback loop structure is not as universal as we have been taught to believe. This leads to a formulation of control problems in terms of interconnections. Subsequently, we study interconnections of linear time-invariant systems from this vantage point. Let us mention two of the results obtained. The first one states that any polynomial can be achieved as the characteristic polynomial of the interconnection with a given plant, provided the plant is not autonomous. The second result states that any subsystem of a controllable system can be implemented by means of a singular feedback control law. These results yield pole placement and stabilization of controllable plants as a special case. These ideas are finally applied to the stabilization of a nonlinear system around an operating point.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/3I96KRHD/Willems - 1997 - On interconnections, control, and feedback.pdf}
}

@article{Williams1998,
  title = {Computation with {{Infinite Neural Networks}}},
  author = {Williams, Christopher K. I.},
  year = {1998},
  month = jul,
  journal = {Neural Computation},
  volume = {10},
  number = {5},
  pages = {1203--1216},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976698300017412},
  urldate = {2025-05-29},
  abstract = {For neural networks with a wide class of weight priors, it can be shown that in the limit of an infinite number of hidden units, the prior over functions tends to a gaussian process. In this article, analytic forms are derived for the covariance function of the gaussian processes corresponding to networks with sigmoidal and gaussian hidden units. This allows predictions to be made efficiently using networks with an infinite number of hidden units and shows, somewhat paradoxically, that it may be easier to carry out Bayesian prediction with infinite networks rather than finite ones.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/X76CG2YJ/Williams - 1998 - Computation with Infinite Neural Networks.pdf}
}

@inproceedings{Wilson2013,
  title = {Gaussian Process Kernels for Pattern Discovery and Extrapolation},
  booktitle = {International Conference on Machine Learning},
  author = {Wilson, Andrew and Adams, Ryan},
  year = {2013},
  pages = {1067--1075},
  file = {/home/marnix/Zotero/storage/KMZMVF3N/Wilson and Adams - Gaussian Process Kernels for Pattern Discovery and.pdf}
}

@phdthesis{Wilson2014,
  title = {Covariance Kernels for Fast Automatic Pattern Discovery and Extrapolation with {{Gaussian}} Processes},
  author = {Wilson, Andrew Gordon},
  year = {2014},
  school = {University of Cambridge},
  file = {/home/marnix/Zotero/storage/LRUHXFQT/Wilson - 2014 - Covariance kernels for fast automatic pattern disc.pdf}
}

@article{Wilson2014a,
  title = {Bayesian {{Inference}} for {{NMR Spectroscopy}} with {{Applications}} to {{Chemical Quantification}}},
  author = {Wilson, Andrew Gordon and Wu, Yuting and Holland, Daniel J. and Nowozin, Sebastian and Mantle, Mick D. and Gladden, Lynn F. and Blake, Andrew},
  year = {2014},
  month = feb,
  journal = {arXiv:1402.3580 [stat]},
  eprint = {1402.3580},
  primaryclass = {stat},
  urldate = {2021-03-19},
  abstract = {Nuclear magnetic resonance (NMR) spectroscopy exploits the magnetic properties of atomic nuclei to discover the structure, reaction state and chemical environment of molecules. We propose a probabilistic generative model and inference procedures for NMR spectroscopy. Specifically, we use a weighted sum of trigonometric functions undergoing exponential decay to model free induction decay (FID) signals. We discuss the challenges in estimating the components of this general model -- amplitudes, phase shifts, frequencies, decay rates, and noise variances -- and offer practical solutions. We compare with conventional Fourier transform spectroscopy for estimating the relative concentrations of chemicals in a mixture, using synthetic and experimentally acquired FID signals. We find the proposed model is particularly robust to low signal to noise ratios (SNR), and overlapping peaks in the Fourier transform of the FID, enabling accurate predictions (e.g., 1\% sensitivity at low SNR) which are not possible with conventional spectroscopy (5\% sensitivity).},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/marnix/Zotero/storage/52RG7LH9/Wilson et al. - 2014 - Bayesian Inference for NMR Spectroscopy with Appli.pdf;/home/marnix/Zotero/storage/VQH55DEE/1402.html}
}

@article{Wilson2015,
  title = {Kernel {{Interpolation}} for {{Scalable Structured Gaussian Processes}} ({{KISS-GP}})},
  author = {Wilson, Andrew Gordon and Nickisch, Hannes},
  year = {2015},
  month = mar,
  journal = {arXiv:1503.01057 [cs, stat]},
  eprint = {1503.01057},
  primaryclass = {cs, stat},
  urldate = {2021-11-04},
  abstract = {We introduce a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian processes (GPs). SKI methods produce kernel approximations for fast computations through kernel interpolation. The SKI framework clarifies how the quality of an inducing point approach depends on the number of inducing (aka interpolation) points, interpolation strategy, and GP covariance kernel. SKI also provides a mechanism to create new scalable kernel methods, through choosing different kernel interpolation strategies. Using SKI, with local cubic kernel interpolation, we introduce KISS-GP, which is 1) more scalable than inducing point alternatives, 2) naturally enables Kronecker and Toeplitz algebra for substantial additional gains in scalability, without requiring any grid data, and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n) time and storage for GP inference. We evaluate KISS-GP for kernel matrix approximation, kernel learning, and natural sound modelling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/Y9PZVGVZ/Wilson and Nickisch - 2015 - Kernel Interpolation for Scalable Structured Gauss.pdf;/home/marnix/Zotero/storage/UV8R3UJR/1503.html}
}

@misc{Wilson2020,
  title = {Efficiently {{Sampling Functions}} from {{Gaussian Process Posteriors}}},
  author = {Wilson, James T. and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc Peter},
  year = {2020},
  month = aug,
  number = {arXiv:2002.09309},
  eprint = {2002.09309},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2002.09309},
  urldate = {2025-04-16},
  abstract = {Gaussian processes are the gold standard for many real-world modeling problems, especially in cases where a model's success hinges upon its ability to faithfully represent predictive uncertainty. These problems typically exist as parts of larger frameworks, wherein quantities of interest are ultimately defined by integrating over posterior distributions. These quantities are frequently intractable, motivating the use of Monte Carlo methods. Despite substantial progress in scaling up Gaussian processes to large training sets, methods for accurately generating draws from their posterior distributions still scale cubically in the number of test locations. We identify a decomposition of Gaussian processes that naturally lends itself to scalable sampling by separating out the prior from the data. Building off of this factorization, we propose an easy-to-use and general-purpose approach for fast posterior sampling, which seamlessly pairs with sparse approximations to afford scalability both during training and at test time. In a series of experiments designed to test competing sampling schemes' statistical properties and practical ramifications, we demonstrate how decoupled sample paths accurately represent Gaussian process posteriors at a fraction of the usual cost.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/P4LE5L2V/Wilson et al. - 2020 - Efficiently Sampling Functions from Gaussian Process Posteriors.pdf;/home/marnix/Zotero/storage/QISLJSF3/2002.html}
}

@article{Winn2005,
  title = {Variational Message Passing},
  author = {Winn, John and Bishop, Christopher M},
  year = {2005},
  journal = {Journal of Machine Learning Research},
  volume = {6},
  number = {Apr},
  pages = {661--694},
  file = {/home/marnix/Zotero/storage/I6QAYVYH/Winn and Bishop - 2005 - Variational message passing.pdf}
}

@article{Wise1976,
  title = {Maximum Likelihood Pitch Estimation},
  author = {Wise, J. and Caprio, J. and Parks, T.},
  year = {1976},
  month = oct,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {24},
  number = {5},
  pages = {418--423},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1976.1162852},
  abstract = {A method for estimating the pitch period of voiced speech sounds is developed based on a maximum likelihood (ML) formulation. It is capable of resolution finer than one sampling period and is shown to perform better in the presence of noise than the cepstrum method.},
  keywords = {Encoding,Gaussian noise,Maximum likelihood estimation,Nonlinear filters,Phase estimation,Resonance,Signal to noise ratio,Speech enhancement,Vocoders,Working environment noise},
  file = {/home/marnix/Zotero/storage/K59C36TJ/Wise et al. - 1976 - Maximum likelihood pitch estimation.pdf;/home/marnix/Zotero/storage/26QBYT4E/1162852.html}
}

@article{Witt1997,
  title = {Self-Organization and Economics--What Is New?},
  author = {Witt, Ulrich},
  year = {1997},
  month = oct,
  journal = {Structural Change and Economic Dynamics},
  volume = {8},
  number = {4},
  pages = {489--507},
  doi = {10.1016/s0954-349x(97)00022-2},
  abstract = {The theory of self-organization, discussed recently in the sciences, provides an abstract, general description of evolutionary processes. This paper argues that it is also of some relevance to economics. Insights into the functioning of dissipative systems--the basic unit of self-organization--can shed new light on the economic theory of the means of production and long-term growth. Moreover, as the economic counterpart to self-organization, the classical \&lsquo;invisible hand\&rsquo; conjecture concerning the self-regulation capacity of markets is incomplete. Self-organization theory suggests to extend it to account for the self-amplifying features of innovative change in the markets.},
  keywords = {economics,self-organization},
  file = {/home/marnix/Zotero/storage/44CW3BA3/Witt - 1997 - Self-organization and economics--what is new.pdf}
}

@article{Witten1987,
  title = {Arithmetic Coding for Data Compression},
  author = {Witten, Ian H and Neal, Radford M and Cleary, John G},
  year = {1987},
  journal = {Communications of the ACM},
  volume = {30},
  number = {6},
  pages = {520--540},
  publisher = {ACM New York, NY, USA},
  file = {/home/marnix/Zotero/storage/57NWFF6C/WIllEN et al. - 1987 - ARITHMETIC CODING FOR DATA COIUPRESSION.pdf}
}

@article{Witthaut2015,
  title = {Critical Links and Nonlocal Rerouting in Complex Supply Networks},
  author = {Witthaut, Dirk and Rohden, Martin and Zhang, Xiaozhu and Hallerberg, Sarah and Timme, Marc},
  year = {2015},
  number = {2},
  eprint = {1510.08976},
  pages = {1--21},
  doi = {10.1103/PhysRevLett.116.138701},
  abstract = {Link failures repeatedly induce large-scale outages in power grids and other supply networks. Yet, it is still not well understood, which links are particularly prone to inducing such outages. Here we analyze how the nature and location of each link impact the network's capability to maintain stable supply. We propose two criteria to identify critical links on the basis of the topology and the load distribution of the network prior to link failure. They are determined via a link's redundant capacity and a renormalized linear response theory we derive. These criteria outperform critical link prediction based on local measures such as loads. The results not only further our understanding of the physics of supply networks in general. As both criteria are available before any outage from the state of normal operation, they may also help real-time monitoring of grid operation, employing counter-measures and support network planning and design.},
  archiveprefix = {arXiv},
  arxivid = {1510.08976},
  file = {/home/marnix/Zotero/storage/Y9Y2XJ5Y/Witthaut et al. - 2015 - Critical links and nonlocal rerouting in complex supply networks.pdf}
}

@article{Wong1979,
  title = {Least Squares Glottal Inverse Filtering from the Acoustic Speech Waveform},
  author = {Wong, D. and Markel, J. and Gray, A.},
  year = {1979},
  month = aug,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {27},
  number = {4},
  pages = {350--355},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1979.1163260},
  abstract = {Covariance analysis as a least squares approach for accurately performing glottal inverse filtering from the acoustic speech waveform is discussed. Best results are obtained by situating the analysis window within a stable closed glottis interval. Based on a linear model of speech production, it is shown that both the moment of glottal closure and opening can be determined from the normalized total squared error with proper choices of analysis window length and filter order. Results from actual speech are presented to illustrate the technique.},
  keywords = {Acoustic pulses,Acoustic waves,Active shape model,Filtering,Least squares methods,Linear predictive coding,Performance analysis,Pulse shaping methods,Speech analysis},
  file = {/home/marnix/Zotero/storage/AKB48G96/Wong et al. - 1979 - Least squares glottal inverse filtering from the a.pdf;/home/marnix/Zotero/storage/3UXC95GQ/1163260.html}
}

@article{Wright2022,
  title = {Deep Physical Neural Networks Trained with Backpropagation},
  author = {Wright, Logan G. and Onodera, Tatsuhiro and Stein, Martin M. and Wang, Tianyu and Schachter, Darren T. and Hu, Zoey and McMahon, Peter L.},
  year = {2022},
  month = jan,
  journal = {Nature},
  volume = {601},
  number = {7894},
  pages = {549--555},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-04223-6},
  urldate = {2023-09-11},
  abstract = {Deep-learning models have become pervasive tools in science and engineering. However, their energy requirements now increasingly limit their scalability1. Deep-learning accelerators2--9 aim to perform deep learning energy-efficiently, usually targeting the inference phase and often by exploiting physical substrates beyond conventional electronics. Approaches so far10--22 have been unable to apply the backpropagation algorithm to train unconventional novel hardware in situ. The advantages of backpropagation have made it the~de facto~training method for large-scale neural networks, so this deficiency constitutes a major impediment. Here we introduce a hybrid in situ--in silico algorithm,~called physics-aware training, that applies backpropagation to train controllable physical systems. Just as deep learning realizes computations with deep neural networks made from layers of mathematical functions, our approach allows us to train deep~physical neural networks~made from layers of controllable physical systems, even when the physical layers lack any mathematical isomorphism to conventional artificial neural network layers. To demonstrate the universality of our approach, we train diverse physical neural networks based on optics, mechanics and electronics to experimentally perform audio and image classification tasks. Physics-aware training combines the scalability of backpropagation with the automatic mitigation of imperfections and noise achievable with~in situ~algorithms.~Physical neural networks have the potential to perform machine learning faster and more energy-efficiently than conventional electronic processors and, more broadly, can endow physical systems with automatically designed physical functionalities, for example, for robotics23--26, materials27--29 and smart sensors30--32.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Computational science,Nonlinear optics},
  file = {/home/marnix/Zotero/storage/BJNJGBDF/Wright et al. - 2022 - Deep physical neural networks trained with backpro.pdf}
}

@article{Wu2008,
  title = {Handbook of {{Data Visualization}}},
  author = {Wu, Han-Ming and Tzeng, ShengLi and Chen, Chun-houh},
  year = {2008},
  journal = {Handbook of Data Visualization},
  number = {Chapter 8},
  eprint = {1011.1669v3},
  pages = {681--708},
  issn = {18737838},
  doi = {10.1007/978-3-540-33037-0},
  abstract = {The graphical exploration of quantitative/qualitative data is an initial but essential step inmodern statistical data analysis.Matrix{\textbackslash}n visualization (Chen, 2002; Chen et al., 2004) is a graphical technique that can simultaneously explore the associations between{\textbackslash}n thousands of subjects, variables, and their interactions, without needing to first reduce the dimensions of the data. Matrix{\textbackslash}n visualization involves permuting the rows and columns of the raw data matrix using suitable seriation (reordering) algorithms,{\textbackslash}n together with the corresponding proximity matrices.The permuted raw data matrix and two proximity matrices are then displayed{\textbackslash}n as matrix maps via suitable color spectra, and the subject clusters, variable groups, and interactions embedded in the dataset{\textbackslash}n can be extracted visually.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1011.1669v3},
  isbn = {9783540330363},
  pmid = {20501337},
  file = {/home/marnix/Zotero/storage/PA6FBIMM/handbook of data visualization.pdf}
}

@article{Xia2000,
  title = {Discrete {{Chirp-Fourier Transform}} and {{Its Application}} to {{Chirp Rate Estimation}}},
  author = {Xia, Xiang-Gen},
  year = {2000},
  volume = {48},
  number = {11},
  pages = {3122--3133},
  file = {/home/marnix/Zotero/storage/S5HPU4Z2/Xia2000 Discrete Chirp-Fourier Transform and Its Application to Chirp Rate Estimation.pdf}
}

@article{Xiang2020,
  title = {Model-Based {{Bayesian}} Analysis in Acoustics-{{A}} Tutorial.},
  author = {Xiang, N.},
  year = {2020},
  journal = {The Journal of the Acoustical Society of America},
  doi = {10.1121/10.0001731},
  abstract = {Bayesian analysis has been increasingly applied in many acoustical applications. In these applications, prediction models are often involved to better understand the process under investigation by purposely learning from the experimental observations. When involving the model-based data analysis within a Bayesian framework, issues related to incorporating the experimental data and assigning probabilities into the inferential learning procedure need fundamental consideration. This paper introduces Bayesian probability theory on a tutorial level, including fundamental rules for manipulating the probabilities, and the principle of maximum entropy for assignment of necessary probabilities prior to the data analysis. This paper also employs a number of examples recently published in this journal to explain detailed steps on how to apply the model-based Bayesian inference to solving acoustical problems.},
  file = {/home/marnix/Zotero/storage/DTNLKWIJ/Xiang - 2020 - Model-based Bayesian analysis in acoustics-A tutor.pdf}
}

@article{Xu2017,
  title = {A {{Fast Algorithm}} for the {{Convolution}} of {{Functions}} with {{Compact Support Using Fourier Extensions}}},
  author = {Xu, Kuan and Austin, Anthony P. and Wei, Ke},
  year = {2017},
  month = dec,
  journal = {SIAM Journal on Scientific Computing},
  volume = {39},
  number = {6},
  publisher = {SIAM},
  issn = {1064-8275},
  doi = {10.1137/17M1114764},
  urldate = {2021-11-24},
  abstract = {In this paper, we present a new algorithm for computing the convolution of two compactly supported functions. The algorithm approximates the functions to be convolved using Fourier extensions and then uses the fast Fourier transform to efficiently compute Fourier extension approximations to the pieces of the result. Finally, the complexity of the algorithm is O(N(log N)2), where N is the number of degrees of freedom used in each of the Fourier extensions.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/NNBZ2JNZ/Xu et al. - 2017 - A Fast Algorithm for the Convolution of Functions .pdf;/home/marnix/Zotero/storage/GYIJ5V9J/1427516.html}
}

@article{Xu2018,
  title = {Spectral Approximation of Convolution Operator},
  author = {Xu, Kuan and Loureiro, Ana},
  year = {2018},
  month = jan,
  journal = {SIAM Journal on Scientific Computing},
  volume = {40},
  number = {4},
  eprint = {1804.08762},
  pages = {A2336-A2355},
  issn = {1064-8275, 1095-7197},
  doi = {10.1137/17M1149249},
  urldate = {2021-11-24},
  abstract = {We develop a unified framework for constructing matrix approximations to the convolution operator of Volterra type defined by functions that are approximated using classical orthogonal polynomials on \$[-1, 1]\$. The numerically stable algorithms we propose exploit recurrence relations and symmetric properties satisfied by the entries of these convolution matrices. Laguerre-based convolution matrices that approximate Volterra convolution operator defined by functions on \$[0, {\textbackslash}infty]\$ are also discussed for the sake of completeness.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Numerical Analysis},
  file = {/home/marnix/Zotero/storage/74YXB3MZ/Xu and Loureiro - 2018 - Spectral approximation of convolution operator.pdf;/home/marnix/Zotero/storage/QQTHL8VC/1804.html}
}

@article{Xu2019,
  title = {Lorentzian-{{Model-Based Bayesian Analysis}} for {{Automated Estimation}} of {{Attenuated Resonance Spectrum}}},
  author = {Xu, K. and Marrelec, G. and Bernard, S. and Grimal, Q.},
  year = {2019},
  month = jan,
  journal = {IEEE Transactions on Signal Processing},
  volume = {67},
  number = {1},
  pages = {4--16},
  issn = {1941-0476},
  doi = {10.1109/TSP.2018.2878543},
  abstract = {Extracting information from a signal exhibiting damped resonances is a challenging task in many practical cases due to the presence of noise and high attenuation. The interpretation of the signal relies on a model whose order (i.e., the number of resonances) is in general unknown. In this study, the signal is modeled as a sum of Lorentzian lineshapes, and a Bayesian framework is designed to simultaneously remove the baseline distortion, select the number of resonances, and recover the parameters of each lineshape including frequency, damping factor, resonance amplitude, and noise magnitude. The Bayesian problem is solved resorting to a reversible jump Markov chain Monte Carlo (RJ-MCMC) sampling scheme. The algorithm is tested on synthetic signals as well as experimental data from a resonant ultrasound spectroscopy experiment aiming to measure elastic properties. The results show that, compared to the well-known linear prediction singular value decomposition method, the RJ-MCMC method achieves a better performance with the advantages of joint model selection, high accuracy estimation, and uncertainty evaluation. We found that when the signal-to-noise-ratio is larger than 20 dB, the average relative error for frequency extraction is smaller than 0.5\%. Such an algorithm enables to estimate the number of resonances and extract tens of resonance parameters from a highly attenuated spectrum, which can significantly facilitate the automated processing of signals exhibiting damped resonances.},
  keywords = {automated attenuated resonance spectrum estimation,automated signal processing,Bayes methods,Bayesian framework,Bayesian method,Bayesian problem,damped resonances,damping factor,Data models,deconvolution,elastic properties,elasticity,electromagnetic wave attenuation,Estimation,feature extraction,frequency extraction,high accuracy estimation,information extraction,joint model selection,lineshape including frequency,Lorentzian lineshapes,Lorentzian-model-based Bayesian analysis,Markov processes,Mathematical model,model selection,Monte Carlo methods,noise magnitude,parameter estimation,Q-factor,resonance,resonance amplitude,resonance parameters,Resonant frequency,resonant ultrasound spectroscopy experiment,reversible jump Markov chain Monte Carlo (RJ-MCMC),reversible jump Markov chain Monte Carlo sampling scheme,RJ-MCMC method,sampling methods,signal processing,signal-to-noise-ratio,Spectroscopy,Spectrum analysis,synthetic signals},
  file = {/home/marnix/Zotero/storage/T2K627KT/Xu et al. - 2019 - Lorentzian-Model-Based Bayesian Analysis for Autom.pdf;/home/marnix/Zotero/storage/SMIN8SG9/8514051.html}
}

@misc{Xu2024,
  title = {{{LLaVA-UHD}}: An {{LMM Perceiving Any Aspect Ratio}} and {{High-Resolution Images}}},
  shorttitle = {{{LLaVA-UHD}}},
  author = {Xu, Ruyi and Yao, Yuan and Guo, Zonghao and Cui, Junbo and Ni, Zanlin and Ge, Chunjiang and Chua, Tat-Seng and Liu, Zhiyuan and Sun, Maosong and Huang, Gao},
  year = {2024},
  month = mar,
  number = {arXiv:2403.11703},
  eprint = {2403.11703},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.11703},
  urldate = {2024-04-16},
  abstract = {Visual encoding constitutes the basis of large multimodal models (LMMs) in understanding the visual world. Conventional LMMs process images in fixed sizes and limited resolutions, while recent explorations in this direction are limited in adaptivity, efficiency, and even correctness. In this work, we first take GPT-4V and LLaVA-1.5 as representative examples and expose systematic flaws rooted in their visual encoding strategy. To address the challenges, we present LLaVA-UHD, a large multimodal model that can efficiently perceive images in any aspect ratio and high resolution. LLaVA-UHD includes three key components: (1) An image modularization strategy that divides native-resolution images into smaller variable-sized slices for efficient and extensible encoding, (2) a compression module that further condenses image tokens from visual encoders, and (3) a spatial schema to organize slice tokens for LLMs. Comprehensive experiments show that LLaVA-UHD outperforms established LMMs trained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our model built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088) resolution images using only 94\% inference computation, and achieves 6.4 accuracy improvement on TextVQA. Moreover, the model can be efficiently trained in academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of LLaVA-1.5). We make the data and code publicly available at https://github.com/thunlp/LLaVA-UHD.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/marnix/Zotero/storage/4V8KTQCJ/Xu et al. - 2024 - LLaVA-UHD an LMM Perceiving Any Aspect Ratio and .pdf;/home/marnix/Zotero/storage/QYCDWKQU/2403.html}
}

@misc{Xuan2021,
  title = {Bayesian {{Transfer Learning}}: {{An Overview}} of {{Probabilistic Graphical Models}} for {{Transfer Learning}}},
  shorttitle = {Bayesian {{Transfer Learning}}},
  author = {Xuan, Junyu and Lu, Jie and Zhang, Guangquan},
  year = {2021},
  month = sep,
  number = {arXiv:2109.13233},
  eprint = {2109.13233},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-03-09},
  abstract = {Transfer learning where the behavior of extracting transferable knowledge from the source domain(s) and reusing this knowledge to target domain has become a research area of great interest in the field of artificial intelligence. Probabilistic graphical models (PGMs) have been recognized as a powerful tool for modeling complex systems with many advantages, e.g., the ability to handle uncertainty and possessing good interpretability. Considering the success of these two aforementioned research areas, it seems natural to apply PGMs to transfer learning. However, although there are already some excellent PGMs specific to transfer learning in the literature, the potential of PGMs for this problem is still grossly underestimated. This paper aims to boost the development of PGMs for transfer learning by 1) examining the pilot studies on PGMs specific to transfer learning, i.e., analyzing and summarizing the existing mechanisms particularly designed for knowledge transfer; 2) discussing examples of real-world transfer problems where existing PGMs have been successfully applied; and 3) exploring several potential research directions on transfer learning using PGM.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/marnix/Zotero/storage/7AZJFD6P/Xuan et al. - 2021 - Bayesian Transfer Learning An Overview of Probabi.pdf;/home/marnix/Zotero/storage/HAQQS73P/2109.html}
}

@misc{Yaghoobi2023,
  title = {Parallel Square-Root Statistical Linear Regression for Inference in Nonlinear State Space Models},
  author = {Yaghoobi, Fatemeh and Corenflos, Adrien and Hassan, Sakira and S{\"a}rkk{\"a}, Simo},
  year = {2023},
  month = apr,
  number = {arXiv:2207.00426},
  eprint = {2207.00426},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.00426},
  urldate = {2024-02-12},
  abstract = {In this article, we introduce parallel-in-time methods for state and parameter estimation in general nonlinear non-Gaussian state-space models using the statistical linear regression and the iterated statistical posterior linearization paradigms. We also reformulate the proposed methods in a square-root form, resulting in improved numerical stability while preserving the parallelization capabilities. We then leverage the fixed-point structure of our methods to perform likelihood-based parameter estimation in logarithmic time with respect to the number of observations. Finally, we demonstrate the practical performance of the methodology with numerical experiments run on a graphics processing unit (GPU).},
  archiveprefix = {arXiv},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing},Statistics - Computation},
  file = {/home/marnix/Zotero/storage/A8L3APAI/Yaghoobi et al. - 2023 - Parallel square-root statistical linear regression.pdf;/home/marnix/Zotero/storage/4TZFTUSX/2207.html}
}

@article{Yan2007,
  title = {Formant Tracking Linear Prediction Model Using {{HMMs}} and {{Kalman}} Filters for Noisy Speech Processing},
  author = {Yan, Qin and Vaseghi, Saeed and Zavarehei, Esfandiar and Milner, Ben and Darch, Jonathan and White, Paul and Andrianakis, Ioannis},
  year = {2007},
  month = jul,
  journal = {Computer Speech \& Language},
  volume = {21},
  number = {3},
  pages = {543--561},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2006.11.001},
  urldate = {2021-01-12},
  abstract = {This paper presents a formant tracking linear prediction (LP) model for speech processing in noise. The main focus of this work is on the utilization of the correlation of the energy contours of speech, along the formant tracks, for improved formant and LP model estimation in noise. The approach proposed in this paper provides a systematic framework for modelling and utilization of the inter-frame correlation of speech parameters across successive speech frames; the within frame correlations are modelled by the LP parameters. The formant tracking LP model estimation is composed of three stages: (1) a pre-cleaning spectral amplitude estimation stage where an initial estimate of the LP model of speech for each frame is obtained, (2) a formant classification and estimation stage using probability models of formants and Viterbi-decoders and (3) an inter-frame formant de-noising and smoothing stage where Kalman filters are used to model the formant trajectories and reduce the effect of residue noise on formants. The adverse effects of car and train noise on estimates of formant tracks and LP models are investigated. The evaluation results for the estimation of the formant tracking LP model demonstrate that the proposed combination of the initial noise reduction stage with formant tracking and Kalman smoothing stages, results in a significant reduction in errors and distortions.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/D2TG5DYU/Yan et al. - 2007 - Formant tracking linear prediction model using HMM.pdf;/home/marnix/Zotero/storage/NTUE53G6/S0885230806000763.html}
}

@article{Ye2014,
  title = {Approximate von {{Neumann}} Entropy for Directed Graphs},
  author = {Ye, Cheng and Wilson, Richard C and Comin, C{\'e}sar H and Costa, Luciano da F and Hancock, Edwin R},
  year = {2014},
  journal = {Physical Review E},
  volume = {89},
  number = {5},
  pages = {52804},
  publisher = {APS},
  file = {/home/marnix/Zotero/storage/3BVT9996/Ye et al. - 2014 - Approximate von Neumann entropy for directed graphs.pdf}
}

@misc{Ye2023,
  title = {A {{Comprehensive Capability Analysis}} of {{GPT-3}} and {{GPT-3}}.5 {{Series Models}}},
  author = {Ye, Junjie and Chen, Xuanting and Xu, Nuo and Zu, Can and Shao, Zekai and Liu, Shichun and Cui, Yuhan and Zhou, Zeyang and Gong, Chao and Shen, Yang and Zhou, Jie and Chen, Siming and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  year = {2023},
  month = mar,
  number = {arXiv:2303.10420},
  eprint = {2303.10420},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.10420},
  urldate = {2023-08-25},
  abstract = {GPT series models, such as GPT-3, CodeX, InstructGPT, ChatGPT, and so on, have gained considerable attention due to their exceptional natural language processing capabilities. However, despite the abundance of research on the difference in capabilities between GPT series models and fine-tuned models, there has been limited attention given to the evolution of GPT series models' capabilities over time. To conduct a comprehensive analysis of the capabilities of GPT series models, we select six representative models, comprising two GPT-3 series models (i.e., davinci and text-davinci-001) and four GPT-3.5 series models (i.e., code-davinci-002, text-davinci-002, text-davinci-003, and gpt-3.5-turbo). We evaluate their performance on nine natural language understanding (NLU) tasks using 21 datasets. In particular, we compare the performance and robustness of different models for each task under zero-shot and few-shot scenarios. Our extensive experiments reveal that the overall ability of GPT series models on NLU tasks does not increase gradually as the models evolve, especially with the introduction of the RLHF training strategy. While this strategy enhances the models' ability to generate human-like responses, it also compromises their ability to solve some tasks. Furthermore, our findings indicate that there is still room for improvement in areas such as model robustness.},
  archiveprefix = {arXiv},
  keywords = {68-06,Computer Science - Computation and Language,I.2}
}

@misc{Yin2023,
  title = {A {{Survey}} on {{Multimodal Large Language Models}}},
  author = {Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
  year = {2023},
  month = jun,
  number = {arXiv:2306.13549},
  eprint = {2306.13549},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-31},
  abstract = {Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/marnix/Zotero/storage/HCBERKYR/Yin et al. - 2023 - A Survey on Multimodal Large Language Models.pdf;/home/marnix/Zotero/storage/VQPE9K89/2306.html}
}

@article{Yoshii2012,
  title = {{{INFINITE COMPOSITE AUTOREGRESSIVE MODELS FOR MUSIC SIGNAL ANALYSIS}}},
  author = {Yoshii, Kazuyoshi and Goto, Masataka},
  year = {2012},
  abstract = {This paper presents novel probabilistic models that can be used to estimate multiple fundamental frequencies (F0s) from polyphonic audio signals. These models are nonparametric Bayesian extensions of nonnegative matrix factorization (NMF) based on the source-filter paradigm, and in them an amplitude or power spectrogram is decomposed as the product of two kinds of spectral atoms (sources and filters) and time-varying gains of source-filter pairs. In this study we model musical instruments as autoregressive systems that combine two types of sources---periodic signals (comb-shaped densities) and white noise (flat density)---with all-pole filters representing resonance characteristics. One of the main problems with such composite autoregressive models (CARMs) is that the numbers of sources and filters should be given in advance. To solve this problem, we propose nonparametric Bayesian models based on gamma processes and efficient variational and multiplicative learning algorithms. These infinite CARMs (iCARMs) can discover appropriate numbers of sources and filters in a data-driven manner. We report the experimental results of multipitch analysis on the MAPS piano database.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/G38LSYWE/Yoshii and Goto - INFINITE COMPOSITE AUTOREGRESSIVE MODELS FOR MUSIC SIGNAL ANALYSIS.pdf}
}

@inproceedings{Yoshii2013,
  title = {Infinite Kernel Linear Prediction for Joint Estimation of Spectral Envelope and Fundamental Frequency},
  booktitle = {2013 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Yoshii, Kazuyoshi and Goto, Masataka},
  year = {2013},
  month = may,
  pages = {463--467},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2013.6637690},
  abstract = {This paper presents a new probabilistic formulation of linear prediction (LP) for jointly estimating the spectral envelope and fundamental frequency (F0) of a speech signal. A main problem of classical LP is that the peaks of the estimated envelope are highly biased toward the harmonic partials of a speech spectrum. To solve this problem, we propose a nonparametric Bayesian model called infinite kernel linear prediction (IKLP) based on a Gaussian process with multiple kernel learning. Our model can represent the periodicity of a speech signal by using a weighted sum of infinitely many periodic kernels that correspond to different F0s. We put a gamma process prior on the positive weights of those kernels and perform sparse learning to determine a predominant kernel indicating the F0 at the same time of spectral envelope estimation. The experimental results showed that our model can estimate spectral envelopes and F0s of speech and singing signals while identifying pitched segments.},
  keywords = {Bayes methods,Bayesian nonparametrics,Estimation,F0,gamma process,Gaussian and gamma processes,Gaussian process,Gaussian processes,Harmonic analysis,harmonic speech spectrum partials,Hidden Markov models,IKLP,infinite kernel linear prediction,joint spectral envelope and fundamental frequency estimation,Kernel,kernel methods,learning (artificial intelligence),Linear prediction,LP,multiple kernel learning,nonparametric Bayesian model,nonparametric statistics,pitched segment identification,predominant kernel,probabilistic formulation,Probabilistic logic,singing signals,source-filter model,sparse learning,spectral analysis,Speech,speech processing,speech signal processing},
  file = {/home/marnix/Zotero/storage/4V78J94H/Yoshii and Goto - 2013 - Infinite kernel linear prediction for joint estima.pdf;/home/marnix/Zotero/storage/NMBTYW5N/6637690.html}
}

@article{Yoshii2013a,
  title = {Infinite {{Positive Semidefinite Tensor Factorization}} for {{Source Separation}} of {{Mixture Signals}}},
  author = {Yoshii, Kazuyoshi and Tomioka, Ryota and Mochihashi, Daichi and Goto, Masataka},
  year = {2013},
  abstract = {This paper presents a new class of tensor factorization called positive semidefinite tensor factorization (PSDTF) that decomposes a set of positive semidefinite (PSD) matrices into the convex combinations of fewer PSD basis matrices. PSDTF can be viewed as a natural extension of nonnegative matrix factorization. One of the main problems of PSDTF is that an appropriate number of bases should be given in advance. To solve this problem, we propose a nonparametric Bayesian model based on a gamma process that can instantiate only a limited number of necessary bases from the infinitely many bases assumed to exist. We derive a variational Bayesian algorithm for closed-form posterior inference and a multiplicative update rule for maximumlikelihood estimation. We evaluated PSDTF on both synthetic data and real music recordings to show its superiority.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/WLYDJJRU/Yoshii et al. - Infinite Positive Semidefinite Tensor Factorization for Source Separation of Mixture Signals.pdf}
}

@article{Yule1927,
  title = {{{VII}}. {{On}} a Method of Investigating Periodicities Disturbed Series, with Special Reference to {{Wolfer}}'s Sunspot Numbers},
  author = {Yule, George Udny},
  year = {1927},
  month = jan,
  journal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
  volume = {226},
  number = {636-646},
  pages = {267--298},
  doi = {10.1098/rsta.1927.0007},
  urldate = {2019-08-02},
  abstract = {If we take a curve representing a simple harmonic function of the time, and superpose on the ordinates small random errors, the only effect is to make the graph somewhat irregular, leaving the suggestion of periodicity still quite clear to the eye. Fig. 1 (a) shows such a curve, the random errors having been determined by the throws of dice. If the errors are increased in magnitude, as in fig. 1 (b), the graph becomes more irregular, the suggestion of periodicity more obscure, and we have only sufficiently to increase the ``errors'' to mask completely any appearance of periodicity. But, however large the errors, periodogram analysis is applicable to such a curve, and, given a sufficient number of periods, should yield a close approximation to the period and amplitude of the underlying harmonic function. When periodogram analysis is applied to data respecting any physical phenomenon in the expectation of eliciting one or more true periodicities, there is usually, as it seems to me, a tendency to start from the initial hypothesis that the periodicity or periodicities are masked solely by such more or less random superposed fluctuations--- fluctuations which do not in any way disturb the steady course of the underlying periodic function or functions. It is true that the periodogram itself will indicate the truth or otherwise of the hypothesis made, but there seems no reason for assuming it to be the hypothesis most likely a priori.},
  file = {/home/marnix/Zotero/storage/ZI27QV4X/Yule George Udny - 1927 - VII. On a method of investigating periodicities di.pdf;/home/marnix/Zotero/storage/Z9NCWCTE/rsta.1927.html}
}

@article{Yusuf2020,
  title = {A {{Hierarchical Subspace Model}} for {{Language-Attuned Acoustic Unit Discovery}}},
  author = {Yusuf, Bolaji and Ondel, Lucas and Burget, Lukas and Cernocky, Jan and Saraclar, Murat},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.03115 [cs, eess]},
  eprint = {2011.03115},
  primaryclass = {cs, eess},
  urldate = {2020-12-23},
  abstract = {In this work, we propose a hierarchical subspace model for acoustic unit discovery. In this approach, we frame the task as one of learning embeddings on a low-dimensional phonetic subspace, and simultaneously specify the subspace itself as an embedding on a hyper-subspace. We train the hyper-subspace on a set of transcribed languages and transfer it to the target language. In the target language, we infer both the language and unit embeddings in an unsupervised manner, and in so doing, we simultaneously learn a subspace of units specific to that language and the units that dwell on it. We conduct our experiments on TIMIT and two low-resource languages: Mboshi and Yoruba. Results show that our model outperforms major acoustic unit discovery techniques, both in terms of clustering quality and segmentation accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/marnix/Zotero/storage/YCEVTAUR/Yusuf et al. - 2020 - A Hierarchical Subspace Model for Language-Attuned.pdf;/home/marnix/Zotero/storage/DLL43849/2011.html}
}

@article{Zen2007,
  title = {Reformulating the {{HMM}} as a Trajectory Model by Imposing Explicit Relationships between Static and Dynamic Feature Vector Sequences},
  author = {Zen, Heiga and Tokuda, Keiichi and Kitamura, Tadashi},
  year = {2007},
  month = jan,
  journal = {Computer Speech \& Language},
  volume = {21},
  number = {1},
  pages = {153--173},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2006.01.002},
  urldate = {2020-06-16},
  abstract = {In the present paper, a trajectory model, derived from a hidden Markov model (HMM) by imposing explicit relationships between static and dynamic feature vector sequences, is developed and evaluated. The derived model, named a trajectory HMM, can alleviate two limitations of the standard HMM, which are (i) piece-wise constant statistics within a state and (ii) conditional independence assumption of state output probabilities, without increasing the number of model parameters. In the present paper, a Viterbi-type training algorithm based on the maximum likelihood criterion is also derived. The performance of the trajectory HMM was evaluated both in speech recognition and synthesis. In a speaker-dependent continuous speech recognition experiment, the trajectory HMM achieved an error reduction over the corresponding standard HMM. Subjective listening test results showed that the introduction of the trajectory HMM improved the naturalness of synthetic speech.},
  langid = {english},
  file = {/home/marnix/Zotero/storage/PW48UY7N/Zen et al. - 2007 - Reformulating the HMM as a trajectory model by imp.pdf;/home/marnix/Zotero/storage/UVLJKP3K/S0885230806000052.html}
}

@article{Zhang2015,
  title = {Identification of Core-Periphery Structure in Networks},
  author = {Zhang, Xiao and Martin, Travis and Newman, M. E.J.},
  year = {2015},
  journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
  volume = {91},
  number = {3},
  eprint = {1409.4813},
  pages = {1--10},
  issn = {15502376},
  doi = {10.1103/PhysRevE.91.032803},
  abstract = {Many networks can be usefully decomposed into a dense core plus an outlying, loosely-connected periphery. Here we propose an algorithm for performing such a decomposition on empirical network data using methods of statistical inference. Our method fits a generative model of core-periphery structure to observed data using a combination of an expectation--maximization algorithm for calculating the parameters of the model and a belief propagation algorithm for calculating the decomposition itself. We find the method to be efficient, scaling easily to networks with a million or more nodes and we test it on a range of networks, including real-world examples as well as computer-generated benchmarks, for which it successfully identifies known core-periphery structure with low error rate. We also demonstrate that the method is immune from the detectability transition observed in the related community detection problem, which prevents the detection of community structure when that structure is too weak. There is no such transition for core-periphery structure, which is detectable, albeit with some statistical error, no matter how weak it is.},
  archiveprefix = {arXiv},
  arxivid = {1409.4813},
  isbn = {02555476 (ISSN)},
  pmid = {25871153},
  file = {/home/marnix/Zotero/storage/FCHPJJ32/Zhang2015 Identification of core-periphery structure in networks.pdf}
}

@article{Zhang2020,
  title = {Statistical Inference of Assortative Community Structures},
  author = {Zhang, Lizhi and Peixoto, Tiago P.},
  year = {2020},
  month = nov,
  journal = {Physical Review Research},
  volume = {2},
  number = {4},
  pages = {043271},
  issn = {2643-1564},
  doi = {10.1103/PhysRevResearch.2.043271},
  urldate = {2024-09-18},
  langid = {english},
  file = {/home/marnix/Zotero/storage/TGL8NXJV/Zhang and Peixoto - 2020 - Statistical inference of assortative community str.pdf}
}

@misc{Zhang2021,
  title = {Differentiable {{Annealed Importance Sampling}} and the {{Perils}} of {{Gradient Noise}}},
  author = {Zhang, Guodong and Hsu, Kyle and Li, Jianing and Finn, Chelsea and Grosse, Roger},
  year = {2021},
  month = oct,
  number = {arXiv:2107.10211},
  eprint = {2107.10211},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.10211},
  urldate = {2022-06-29},
  abstract = {Annealed importance sampling (AIS) and related algorithms are highly effective tools for marginal likelihood estimation, but are not fully differentiable due to the use of Metropolis-Hastings correction steps. Differentiability is a desirable property as it would admit the possibility of optimizing marginal likelihood as an objective using gradient-based methods. To this end, we propose Differentiable AIS (DAIS), a variant of AIS which ensures differentiability by abandoning the Metropolis-Hastings corrections. As a further advantage, DAIS allows for mini-batch gradients. We provide a detailed convergence analysis for Bayesian linear regression which goes beyond previous analyses by explicitly accounting for the sampler not having reached equilibrium. Using this analysis, we prove that DAIS is consistent in the full-batch setting and provide a sublinear convergence rate. Furthermore, motivated by the problem of learning from large-scale datasets, we study a stochastic variant of DAIS that uses mini-batch gradients. Surprisingly, stochastic DAIS can be arbitrarily bad due to a fundamental incompatibility between the goals of last-iterate convergence to the posterior and elimination of the accumulated stochastic error. This is in stark contrast with other settings such as gradient-based optimization and Langevin dynamics, where the effect of gradient noise can be washed out by taking smaller steps. This indicates that annealing-based marginal likelihood estimation with stochastic gradients may require new ideas.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marnix/Zotero/storage/Q3RA85L9/Zhang et al. - 2021 - Differentiable Annealed Importance Sampling and th.pdf;/home/marnix/Zotero/storage/Q44RSBEH/2107.html}
}

@misc{Zhang2023,
  title = {Google {{USM}}: {{Scaling Automatic Speech Recognition Beyond}} 100 {{Languages}}},
  shorttitle = {Google {{USM}}},
  author = {Zhang, Yu and Han, Wei and Qin, James and Wang, Yongqiang and Bapna, Ankur and Chen, Zhehuai and Chen, Nanxin and Li, Bo and Axelrod, Vera and Wang, Gary and Meng, Zhong and Hu, Ke and Rosenberg, Andrew and Prabhavalkar, Rohit and Park, Daniel S. and Haghani, Parisa and Riesa, Jason and Perng, Ginger and Soltau, Hagen and Strohman, Trevor and Ramabhadran, Bhuvana and Sainath, Tara and Moreno, Pedro and Chiu, Chung-Cheng and Schalkwyk, Johan and Beaufays, Fran{\c c}oise and Wu, Yonghui},
  year = {2023},
  month = sep,
  number = {arXiv:2303.01037},
  eprint = {2303.01037},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.01037},
  urldate = {2024-01-25},
  abstract = {We introduce the Universal Speech Model (USM), a single large model that performs automatic speech recognition (ASR) across 100+ languages. This is achieved by pre-training the encoder of the model on a large unlabeled multilingual dataset of 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller labeled dataset. We use multilingual pre-training with random-projection quantization and speech-text modality matching to achieve state-of-the-art performance on downstream multilingual ASR and speech-to-text translation tasks. We also demonstrate that despite using a labeled training set 1/7-th the size of that used for the Whisper model, our model exhibits comparable or better performance on both in-domain and out-of-domain speech recognition tasks across many languages.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/marnix/Zotero/storage/LYCR2QN8/Zhang et al. - 2023 - Google USM Scaling Automatic Speech Recognition B.pdf;/home/marnix/Zotero/storage/KNZAIJIB/2303.html}
}

@misc{Zhao2023,
  title = {{{BuboGPT}}: {{Enabling Visual Grounding}} in {{Multi-Modal LLMs}}},
  shorttitle = {{{BuboGPT}}},
  author = {Zhao, Yang and Lin, Zhijie and Zhou, Daquan and Huang, Zilong and Feng, Jiashi and Kang, Bingyi},
  year = {2023},
  month = jul,
  number = {arXiv:2307.08581},
  eprint = {2307.08581},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.08581},
  urldate = {2023-08-20},
  abstract = {LLMs have demonstrated remarkable abilities at interacting with humans through language, especially with the usage of instruction-following data. Recent advancements in LLMs, such as MiniGPT-4, LLaVA, and X-LLM, further enlarge their abilities by incorporating multi-modal inputs, including image, video, and speech. Despite their effectiveness at generating precise and detailed language understanding of the given modality signal, these LLMs give up the ability to ground specific parts of inputs, thus only constructing a coarse-grained mapping. However, explicit and informative correspondence between text and other modalities will not only improve the user experience but also help to expand the application scenario of multi-modal LLMs. Therefore, we propose BuboGPT, a multi-modal LLM with visual grounding that can perform cross-modal interaction between vision, audio and language, providing fine-grained understanding of visual objects and other given modalities. As a result, BuboGPT is able to point out the specific location of an object in the image, when it is generating response or description for that object. Our contributions are two-fold: 1) An off-the-shelf visual grounding module based on SAM that extracts entities in a sentence and find corresponding masks in the image. 2) A two-stage training scheme and instruction dataset to endow joint text-image-audio understanding. Our experiments show that BuboGPT achieves impressive multi-modality understanding and visual grounding abilities during the interaction with human. It performs consistently well when provided by arbitrary modality combinations (either aligned or unaligned). Our code, model and dataset are available at https://bubo-gpt.github.io .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/marnix/Zotero/storage/PUSMCWW6/Zhao et al. - 2023 - BuboGPT Enabling Visual Grounding in Multi-Modal .pdf;/home/marnix/Zotero/storage/5DHJZZXC/2307.html}
}

@inproceedings{Zheng2004,
  title = {Formant Tracking by Mixture State Particle Filter},
  booktitle = {2004 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {{Yanli Zheng} and {Hasegawa-Johnson}, M.},
  year = {2004},
  month = may,
  volume = {1},
  pages = {I-565},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2004.1326048},
  abstract = {This paper presents a mixture state particle filter method for formant tracking during both vowels and consonants. We show that the mixture state particle filter model is able to incorporate prior information about phoneme class into the system, which helps the system to find global optimal solutions. Formant frequencies are defined as eigenfrequencies of the vocal tract in this paper, and by exploring this fact using spectral estimation techniques, the observation PDF of the particle filter can be simplified. We show that by using this likelihood function in the importance weights, the system is able to track the formants using a small number of particles.},
  keywords = {Bandwidth,consonants,eigenfrequencies,eigenvalues and eigenfunctions,formant frequencies,formant tracking,frequency estimation,Frequency estimation,global optimal solutions,Humans,importance weights,Iterative algorithms,mixture state particle filter,observation PDF,optimisation,Particle filters,Particle tracking,phoneme class,Poles and zeros,prior information,probability,Production,spectral analysis,spectral estimation,Speech,speech processing,Transfer functions,vocal tract,vowels},
  file = {/home/marnix/Zotero/storage/8NC2Y5LW/Yanli Zheng and Hasegawa-Johnson - 2004 - Formant tracking by mixture state particle filter.pdf;/home/marnix/Zotero/storage/XI2C4LKM/1326048.html}
}

@article{Zhou2016,
  title = {Controlling the Motion of Multiple Objects on a {{Chladni}} Plate},
  author = {Zhou, Quan and Sariola, Veikko and Latifi, Kourosh and Liimatainen, Ville},
  year = {2016},
  month = sep,
  journal = {Nature Communications},
  volume = {7},
  number = {1},
  pages = {12764},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/ncomms12764},
  urldate = {2023-09-09},
  abstract = {The origin of the idea of moving objects by acoustic vibration can be traced back to 1787, when Ernst Chladni reported the first detailed studies on the aggregation of sand onto nodal lines of a vibrating plate. Since then and to this date, the prevailing view has been that the particle motion out of nodal lines is random, implying uncontrollability. But how random really is the out-of-nodal-lines motion on a Chladni plate? Here we show that the motion is sufficiently regular to be statistically modelled, predicted and controlled. By playing carefully selected musical notes, we can control the position of multiple objects simultaneously and independently using a single acoustic actuator. Our method allows independent trajectory following, pattern transformation and sorting of multiple miniature objects in a wide range of materials, including electronic components, water droplets loaded on solid carriers, plant seeds, candy balls and metal parts.},
  copyright = {2016 The Author(s)},
  langid = {english},
  keywords = {Acoustics,Electrical and electronic engineering,Mechanical engineering},
  file = {/home/marnix/Zotero/storage/LPE49E7C/Zhou et al. - 2016 - Controlling the motion of multiple objects on a Ch.pdf}
}

@misc{Zhu2023,
  title = {{{MiniGPT-4}}: {{Enhancing Vision-Language Understanding}} with {{Advanced Large Language Models}}},
  shorttitle = {{{MiniGPT-4}}},
  author = {Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  year = {2023},
  month = apr,
  number = {arXiv:2304.10592},
  eprint = {2304.10592},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.10592},
  urldate = {2023-07-31},
  abstract = {The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. We believe the primary reason for GPT-4's advanced multi-modal generation capabilities lies in the utilization of a more advanced large language model (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen LLM, Vicuna, using just one projection layer. Our findings reveal that MiniGPT-4 possesses many capabilities similar to those exhibited by GPT-4 like detailed image description generation and website creation from hand-written drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, providing solutions to problems shown in images, teaching users how to cook based on food photos, etc. In our experiment, we found that only performing the pretraining on raw image-text pairs could produce unnatural language outputs that lack coherency including repetition and fragmented sentences. To address this problem, we curate a high-quality, well-aligned dataset in the second stage to finetune our model using a conversational template. This step proved crucial for augmenting the model's generation reliability and overall usability. Notably, our model is highly computationally efficient, as we only train a projection layer utilizing approximately 5 million aligned image-text pairs. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/marnix/Zotero/storage/SJPV9G7S/Zhu et al. - 2023 - MiniGPT-4 Enhancing Vision-Language Understanding.pdf;/home/marnix/Zotero/storage/FQUHU92D/2304.html}
}

@misc{Zhuge2023,
  title = {Mindstorms in {{Natural Language-Based Societies}} of {{Mind}}},
  author = {Zhuge, Mingchen and Liu, Haozhe and Faccio, Francesco and Ashley, Dylan R. and Csord{\'a}s, R{\'o}bert and Gopalakrishnan, Anand and Hamdi, Abdullah and Hammoud, Hasan Abed Al Kader and Herrmann, Vincent and Irie, Kazuki and Kirsch, Louis and Li, Bing and Li, Guohao and Liu, Shuming and Mai, Jinjie and Pi{\k e}kos, Piotr and Ramesh, Aditya and Schlag, Imanol and Shi, Weimin and Stani{\'c}, Aleksandar and Wang, Wenyi and Wang, Yuhui and Xu, Mengmeng and Fan, Deng-Ping and Ghanem, Bernard and Schmidhuber, J{\"u}rgen},
  year = {2023},
  month = may,
  number = {arXiv:2305.17066},
  eprint = {2305.17066},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-31},
  abstract = {Both Minsky's "society of mind" and Schmidhuber's "learning to think" inspire diverse societies of large multimodal neural networks (NNs) that solve problems by interviewing each other in a "mindstorm." Recent implementations of NN-based societies of minds consist of large language models (LLMs) and other NN-based experts communicating through a natural language interface. In doing so, they overcome the limitations of single LLMs, improving multimodal zero-shot reasoning. In these natural language-based societies of mind (NLSOMs), new agents -- all communicating through the same universal symbolic language -- are easily added in a modular fashion. To demonstrate the power of NLSOMs, we assemble and experiment with several of them (having up to 129 members), leveraging mindstorms in them to solve some practical AI tasks: visual question answering, image captioning, text-to-image synthesis, 3D generation, egocentric retrieval, embodied AI, and general language-based task solving. We view this as a starting point towards much larger NLSOMs with billions of agents-some of which may be humans. And with this emergence of great societies of heterogeneous minds, many new research questions have suddenly become paramount to the future of artificial intelligence. What should be the social structure of an NLSOM? What would be the (dis)advantages of having a monarchical rather than a democratic structure? How can principles of NN economies be used to maximize the total reward of a reinforcement learning NLSOM? In this work, we identify, discuss, and try to answer some of these questions.},
  archiveprefix = {arXiv},
  keywords = {68T07,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multiagent Systems,I.2.11,I.2.6},
  file = {/home/marnix/Zotero/storage/ZIGFAATT/Zhuge et al. - 2023 - Mindstorms in Natural Language-Based Societies of .pdf;/home/marnix/Zotero/storage/DIRARADX/2305.html}
}

@article{Zlatic2009,
  title = {On the Rich-Club Effect in Dense and Weighted Networks},
  author = {Zlatic, V and Bianconi, G and {D{\textbackslash}'{\textbackslash}iaz-Guilera}, A and Garlaschelli, D and Rao, F and Caldarelli, G},
  year = {2009},
  month = jan,
  journal = {The European Physical Journal B - Condensed Matter and Complex Systems},
  volume = {67},
  eprint = {0807.0793},
  pages = {271--275},
  issn = {1434-6028},
  doi = {10.1140/epjb/e2009-00007-9},
  abstract = {Abstract For many complex networks present in nature only a single instance, usually of large size, is available. Any measurement made on this single instance cannot be repeated on different realizations. In order to detect significant patterns in a real-world network it is therefore crucial to compare the measured results with a null model counterpart. Here we focus on dense and weighted networks, proposing a suitable null model and studying the behaviour of the degree correlations as measured by the rich-club coefficient. Our method solves an existing problem with the randomization of dense unweighted graphs, and at the same time represents a generalization of the rich-club coefficient to weighted networks which is complementary to other recently proposed ones.},
  archiveprefix = {arXiv},
  arxivid = {0807.0793},
  keywords = {effect,rich-club},
  file = {/home/marnix/Zotero/storage/HTKXHVA5/Zlatic et al. - 2009 - On the rich-club effect in dense and weighted networks.pdf}
}

@article{Zorzi2013,
  title = {Modeling Language and Cognition with Deep Unsupervised Learning: {{A}} Tutorial Overview},
  author = {Zorzi, Marco and Testolin, Alberto and Stoianov, Ivilin P.},
  year = {2013},
  journal = {Frontiers in Psychology},
  volume = {4},
  number = {AUG},
  pages = {1--14},
  issn = {16641078},
  doi = {10.3389/fpsyg.2013.00515},
  abstract = {Deep unsupervised learning in stochastic recurrent neural networks with many layers of hidden units is a recent breakthrough in neural computation research. These networks build a hierarchy of progressively more complex distributed representations of the sensory data by fitting a hierarchical generative model. In this article we discuss the theoretical foundations of this approach and we review key issues related to training, testing and analysis of deep networks for modeling language and cognitive processing. The classic letter and word perception problem of McClelland and Rumelhart (1981) is used as a tutorial example to illustrate how structured and abstract representations may emerge from deep generative learning. We argue that the focus on deep architectures and generative (rather than discriminative) learning represents a crucial step forward for the connectionist modeling enterprise, because it offers a more plausible model of cortical learning as well as a way to bridge the gap between emergentist connectionist models and structured Bayesian models of cognition.},
  isbn = {1664-1078 (Electronic)},
  pmid = {23970869},
  keywords = {Connectionist modeling,Deep learning,Hierarchical generative models,Neural networks,Unsupervised learning,Visual word recognition},
  file = {/home/marnix/Zotero/storage/Y2HKFIV3/Zorzi, Testolin, Stoianov - 2013 - Modeling language and cognition with deep unsupervised learning A tutorial overview.pdf}
}

@misc{zotero-item-2649,
  title = {The Politics of {{AI}}: {{ChatGPT}} and Political Bias},
  shorttitle = {The Politics of {{AI}}},
  journal = {Brookings},
  urldate = {2024-04-14},
  abstract = {When asked to indicate support or lack of support for a variety of political statements, ChatGPT's responses tend to replicate a liberal point of view, albeit with logical inconsistencies, emblematizing the issue of bias embedded in AI systems through their datasets and human trainers.},
  howpublished = {https://www.brookings.edu/articles/the-politics-of-ai-chatgpt-and-political-bias/},
  langid = {american},
  file = {/home/marnix/Zotero/storage/8748KIYZ/the-politics-of-ai-chatgpt-and-political-bias.html}
}