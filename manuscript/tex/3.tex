% !TeX root = ../thesis.tex
\chapter{A nonparametric prior for $u(t)$\label{chapter:3}}

\begin{chaptersections}{%
In this Chapter, a nonparametric prior for the glottal flow derivative (DGF) $u'(t)$ is formulated as a reduced-rank Gaussian process (GP) which is constrained to incorporate important prior information.
}

\section{Introduction}

% See proj/sflinear/notes.md. It is a goldmine

Note that the Chapter title talks about the GF, while we model the DGF.
But we can go from one to the other trivially.

We now model the DGF with the Matern family.

"Gaussian process (GP) regression models have become a standard tool in Bayesian signal estimation due to their expressiveness, robustness to overfitting and tractability" Quote from \citep{Tobar2015}

Sample prior and calculate statistics (the five parameters of \citep{Doval2006}) of these by fitting a glottal flow model to them.

That we can solve the null integral constraint analytically and efficiently is a huge bon because the standard LF model is unwieldly and relatively slow \cite{Perrotin2021}. Also special care needs to be taken when solving the implicit equations of the transformed LF model \cite{Fant1994,Fant1995}, because numerically solving them does not preserve the $R_d$ constraint automatically \cite{Gobl2017}. Also, good open source code was missing. We supplied a Python implementation, but it does not work for all possible parameter configurations, so it did not converge for all the sampled parameters we threw at it. Since it has been shown that the LF model is perceptually equivalent to simpler implementations \citep{Perrotin2021}, which are more stable to implement, and LF is close to host of other models \cite{Doval2006}, it is a valid question to ask why why to use LF other than tradition.

Nonparametric means models have infty amount of parameters
and complexity grows with data
Nonparametric source signal because (1) it is traditionally viewed as `the signal' and GP have good track records in modeling signals and (2) lots of things are known about this signal and we can use GPs to implement linear constraints on them analytically if we make a few approximations and GPs kernels also describe signal stuff (1st derivative).
We also know less about glottal source signal


\section{Prior information: constraints on the GP}

% See proj/GlottalFlow/notebooks/basisfunctions.jl
% and py/dgf/test/test_polarity.ipynb

\begin{description}
\item[Stationarity.]
A stationary kernels ensures a well-defined frequency spectrum.
Since the DGF can be described compactly in the frequency domain (the most used parametric models such as LF, as reviewed by \cite{Doval2006}, have a simple representation that can be ordered by a third-order filter), it makes sense to assume that the corresponding GP must be stationary (otherwise frequency domain descriptions would not be as effective).
``Indeed, voice quality is often described in spectral terms (e.g., voice spectral tilt, brightness, tenseness): spectral parameters are closer to perception than time-domain parameters.'' \citep[p.~1274]{Perrotin2021}
\item[Smoothness.]
How many times it can be differentiated. See \cite{Doval2006}.
\item[GOI.]
\item[Null integral.] 

\item[Well-defined parity.]
Always points up or down \citep{Chen2016}.
We do not constrain this but it is possible: See Appendix~\ref{dgf-parity}.
\end{description}

\subsection{Advantages of the Hilbert basis}

The necessity of expanding the GP approximately onto a basis to allow arbitrary stationary GPs to be constrained and allow convolutions with $e^{pt}$ (next Chapter) and to possibly speed up inference became clear early on.
But the appropriate basis for the expansion was less clear.
We now argue why the Hilbert basis proposed in \citep{Solin2020} is perfect for our goal compared to the alternatives in Appendix~\ref{other-bases}.
\begin{enumerate}
\item
Hilbert expansions maximally exploit stationarity, calculating the KL coefficients in only $O(M)$ time as opposed to $O(M^3)$ time of other methods (Appendix~\ref{polynomial-bases}).
Likewise, it is extremely easy to implement.
\item
The basis allows for analytic and numerically stable convolution with $e^{pt}$ kernels, and the FT of the basis (and thus $u'(t)$) is conveniently expressible, since the basis literally distributes power as along the components (i.e., frequencies: each sine in the basis corresponds to a frequency... at least approximately because the signal is not perfectly periodic).
\item
The necessary constraints (integrals) can be expressed in closed form (null integral).
In addition, the smooth opening (GOI condition) is `for free' because of Dirichlet boundary condition.
\item
Error of the approximation is well understood \citep[Section~4]{Solin2020}.
\item
It is possible to literally design the DGF in the frequency domain, \emph{bypassing the need for an explicit GP representation}.
This accomodates DGF models which are formulated entirely in the frequency domain, such as the causal/anti-causal linear model in CALM \citep{Doval2003} and the all-causal linear model used in the Cantor Digitalis singing synthesiser \citep{Feugere2017}.
\end{enumerate}

The main disadvantage is choosing the approximation domain $[0,L]$ and the \citep{Riutort-Mayol2020}, but this does not influence the results much for the regime in which are our hyperparameters.

Also allows for negative flow, though this might actually be a boon.
"Note that contrarily to all other models, this [Fujisaki model] one does not exclude a negative flow due for instance to a lowering of the vocal folds following the glottal closure.'' \citep[p.~35]{Degottex2010}

Example of incorporating hard constraints directly:
Note that this connects with ``After this, the fine-tuning of the resistors of the inverse network was conducted by searching for such values that yielded zero flow after the instant of glottal closure.'' \citep[p.~626]{Alku2011} by the first study on glottal inverse filtering by \citep{Miller1959}.
Softer constraints are learned by "transfer learning" and incorporated into priors.

\section{Single pitch period: $u_p(t)$}

\section{Multiple pitch periods: $u(t)$}

Now introduce latent GPs for continuity AND the amplitude correlation (this is new, we did not have this for the LF prior).

\section{Discussion}

Conclude this Chapter.

The nonparametric prior can also be interpreted as a wavelet basis
The natural generalization of the model
consisted of deriving a family of wavelets that make up the fundamental building blocks of
voiced speech. 7 The simplest (first-order) member of this family of wavelets can be
understood as a glottal pulse exciting a resonant cavity in the vocal tract and has a
mathematical form identical to the exponentially-modified Gaussian distribution. More
generally, the wavelet family is characterized by a set of “conjugate” time and frequency
parameters which enable it to describe spectro-temporal features directly without the need for
any data-derived features such as MFCC vectors, and has several convenient analytical
properties, such as being an approximately orthogonal basis for speech time series and having
closed form expressions for all wavelet orders. These properties show that the derived wavelet
family is similar to the Gabor wavelets originally suggested as spectro-temporal feature
detectors in the research proposal (Ezzat et al. 2007), with the difference that these wavelets
are specifically fine-tuned for voiced speech. In addition, the wavelet family also exhibits
several direct links to theoretical issues in acoustic phonetics. It can be shown that two well-
known models of glottal flow production (Alku et al. 2002 and Titze \& Sundberg 1992, resp.)
can be understood as instances of the smallest possible consistent set of these wavelets of
orders 2 and 3, resp. Furthermore, this wavelet family provides a clear answer to why “the
shape of the glottal flow derivative can often be recognized in acoustic speech or singing
voice waveform itself” (Doval et al. 2006, p. 3). Finally, we note that the wavelet family is in
principle applicable to any quasi-linear system driven by a pulse train and a reasonably small
number of (prominent) resonances, opening the door to applications outside human speech
(e.g. mammal vocal communication research: Taylor \& Reby 2010).

\end{chaptersections}

\begin{chapterappendices}{5}

\section{Preserving the GF parity\label{dgf-parity}}

\shloppy{Show how consecutive samples can preserve parity.}

If we would fix parity this would lead to a $2^N_p$ reduction in possibilies, where $N_p$ is number of pitch periods (I wrote this down somewhere I think).

\section{Other stationary kernels\label{other-kernels}}

We have made a case for the Matérn family in this Chapter, but any other stationary kernel with a convenient spectral density may be used as a drop-in replacement.\footnote{%
Actually, the spectral density can be used directly; no stationary kernel representation need exist.%
}

Examples are the centered Sinc kernel \citep{Tobar2019}, which has uniform spectral density over $[0,f_s]$, and the family of kernels with covariance functions that are piecewise polynomial functions with compact support \citep[p.~87]{Rasmussen2006}.
These kernels have been implemented in our code but their use for modeling the DGF has not been investigated yet.

\section{Other bases\label{other-bases}}

In this chapter, we have argued that modeling the DGF $u'(t)$ as a reduced-rank GP results in a flexible nonparametric prior which incorporates important prior information.
Our basis of choice is the Hilbert basis introduced in \cite{Solin2020} because it is expressive and optimally efficient for expanding a stationary kernel.
To get a feel for just how efficient it is, we now compare it to two more conventional \emph{polynomial} bases.

\paragraph{Chebyshev and Legendre bases.}
These are both bases on a compact domain but differ with respect to their weighing functions and are therefore characterized by different approximation qualities \citep{Trefethen2018}.
In both cases, a given covariance function $k(t,t')$ is truncated on a compact domain and discretized at a set of $M$ nodes, i.e., a set of precomputed two-dimensional points.
This information is then used to calculate the KL coefficients in a KL expansion of $k(t,t')$. % which allows GP regression in O(NM^2) time [and in our case, it allows the constraints and $e^{pt}$ convolutions to be expressed analytically].

For the Legendre basis, details of this procedure together with precise error bounds is given by \cite{Greengard2021}.
Their algorithm to calculate the KL coefficients is roughly $O(M^3)$.

On the other hand, using the Chebyshev basis directly to calculate a KL expansion in $O(M^3)$ time is a new approach to the best of our knowledge.\footnote{%
Although random functions (not necessarily expressible as samples from a GP) have been parametrized directly by Chebyshev polynomials before \citep{Filip2019}.%
}
\cite{Scheiber2015} gives an $O(M^2\log M)$ method to discretize $k(t,t')$ with a symmetric even `checkerboard matrix' \citep{Jones2018}, whose structure can be further exploited by using two small $O(M^3)$ Cholesky decompositions to arrive at the KL coefficients.

These polynomial bases are more general than the Hilbert basis because they do not require stationary kernels, only compact domains.
In addition, choosing the expansion domain is easy: the more compact the domain (while still encompassing the data points), the better the approximation.
The precise relation between accuracy and the approximation domain in the case of the Hilbert basis is considerably more involved \citep{Riutort-Mayol2020}.

The Chebyshev and Legendre are classical orthogonal polynomials and share the property that the various convolutions and other integrals which emerge when the DGF is put to work can be expressed analytically and in some cases calculated efficiently \citep{Fokas2012, Dixit2014, Xu2017, Xu2018, Hale2014}.\footnote{%
This is also true of the Laguerre basis.
In fact, this tractability was the original reason for investigating these before settling on the Hilbert basis.
}

Despite this, the Hilbert basis is still the method of choice in this chapter, because it can obtain the KL expansion coefficients in only $O(M)$ time -- essentially for free!
By assuming that $k(t,t')$ is stationary, we are able to get away with discretizing a one-dimensional spectral density instead of having to discretize a two-dimensional covariance function [since a stationary covariance function is really only a function of one argument: $k(t,t') = k(t - t')$].
Finally, the Hilbert basis functions are just harmonics, and these are easier to convolve with rational transfer functions compared to polynomials.
This property will allow us to express voiced speech straightforwardly as yet another reduced-rank GP in Chapter~\ref{chapter:7}.

\paragraph{Laguerre basis.}
On a more speculative note, one of the most intruiging possibilities is a Laguerre expansion because of an interesting symmetry between the glottal pulse $u(t)$ and the impulse responses of the vocal tract $h(t)$:
Glottal pulses tend to have an `exponential' buildup (because of the smooth opening of the glottal folds), while impulse responses typically have an exponential decay.
Given that Laguerre functions are well suited as a basis for decaying signals on the half-open real line, the possibility of expanding \emph{both} $u(t)$ and $h(t)$ in the Laguerre basis in order to exploit various inner products presents itself as an interesting avenue for future work on improving efficiency and generalizing the approach of \cite{Guarnizo2018} and \cite{Tobar2015}.
Note that this approach allows us to go beyond rational transfer functions, because we work with $h(t)$ directly and do not require the partial fraction decomposition of $\tilde{h}(x)$.

\end{chapterappendices}
