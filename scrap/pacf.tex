\documentclass[11pt]{article}
\usepackage{amsmath,amssymb}
\usepackage{geometry}
\usepackage{hyperref}
\geometry{margin=1in}
\setlength{\parskip}{6pt}
\setlength{\parindent}{0pt}

\title{Stable Gaussian Priors for AR($P$) Coefficients\\[2pt]
       via Moment Matching of a Uniform PACF Law}
\date{\today}

\begin{document}\maketitle

\begin{abstract}
Sampling the partial--autocorrelation function (PACF) uniformly on
$(-1,1)^P$ generates only covariance--stationary autoregressive (AR)
coefficient vectors.  We derive the \emph{unique} Gaussian distribution
$p(a\mid\mu,\Sigma)=\mathcal N(a\mid\mu,\Sigma)$ that minimises the
Kullback--Leibler divergence $D_{\mathrm{KL}}\bigl(q\,\|\,p\bigr)$,
where $q$ is the PACF push--forward law.  The optimum has
$\mu^\star=0$ by symmetry and
$\Sigma^\star=\mathbb E_q[a\,a^{\top}]$, the second moment of $q$.
Because the optimisation is convex, $\Sigma^\star$ is the global
solution.  We also discuss Monte--Carlo estimation of $\Sigma^\star$
and how the resulting covariance clarifies empirical shrinkage rules
such as $\lambda\propto 1/P$ in isotropic priors.
\end{abstract}


%--------------------------------------------------------------------
\section{Background: stability and the Monahan map}

For an AR($P$) process
\[
  x_t=\sum_{p=1}^{P} a_p\,x_{t-p}+\varepsilon_t,
  \qquad \varepsilon_t\sim\mathcal N(0,\sigma^2),
\]
covariance stationarity holds iff the roots of
$1-\sum_{p=1}^{P}a_p z^p$ lie inside the unit disk.  The recursion of
\cite{Monahan1984} provides a smooth bijection
\[
  T:(-1,1)^P\;\longrightarrow\;\mathcal A_P\subset\mathbb R^P,
  \qquad a = T(\phi),
\]
where $\phi=(\phi_1,\dots,\phi_P)$ are PACF parameters and
$\mathcal A_P$ is the stability region.  Drawing
$\phi_i \stackrel{\text{iid}}{\sim}\mathrm{Unif}[-1,1]$ therefore yields
only admissible $a$.


%--------------------------------------------------------------------
\section{KL objective and global optimum}

Let $q$ be the density of $a=T(\phi)$ under the uniform PACF draw.
We minimise
\[
  D_{\mathrm{KL}}\!\bigl(q\,\|\,p_{\mu,\Sigma}\bigr)
  \;=\;
  \mathbb E_q\bigl[\log q(a)-\log p_{\mu,\Sigma}(a)\bigr],
  \qquad
  p_{\mu,\Sigma}(a)=\mathcal N(a\mid\mu,\Sigma).
\]
Terms involving $\log q$ do not depend on $(\mu,\Sigma)$.
Writing the second moment
$S_q:=\mathbb E_q[a\,a^{\top}]$ and expanding
$\log p_{\mu,\Sigma}$ gives the objective
\[
  F(\mu,\Sigma)=
    \tfrac12\!\Bigl(
      \log\det\Sigma
      +\operatorname{tr}\Sigma^{-1}S_q
      +(\mu-\mathbb E_q[a])^{\!\top}\Sigma^{-1}(\mu-\mathbb E_q[a])
    \Bigr)
    +\text{const.}
\]

\paragraph{Optimal mean.}
Uniformity of $\phi$ is sign--symmetric and the Monahan map is odd in
each coordinate, hence $\mathbb E_q[a]=0$; the quadratic term is
minimised by
\(
  \boxed{\mu^\star=0}.
\)

\paragraph{Optimal covariance.}
Setting $\mu=0$, differentiate $F$ with respect to $\Sigma$:
\[
  \nabla_\Sigma F
  =\tfrac12\Sigma^{-1}S_q\Sigma^{-1}-\tfrac12\Sigma^{-1}=0
  \;\;\Longrightarrow\;\;
  \boxed{\Sigma^\star=S_q}.
\]

\paragraph{Global optimality.}
Parameterise by the precision $\Lambda=\Sigma^{-1}$.  Then
\[
  \mathbb E_q[\log p_{0,\Lambda^{-1}}(a)]
  =-\tfrac12\,\mathrm{tr}(\Lambda S_q)+\tfrac12\log\det\Lambda+{\rm const.}
\]
The trace term is linear in $\Lambda$ and $\log\det\Lambda$ is concave,
so the objective is concave in $\Lambda$.  Maximising a concave
function over $\Lambda\succ0$ is a convex problem, hence the stationary
point $\Sigma^\star$ is the \emph{global} minimiser of the KL
divergence.\vspace{6pt}


%--------------------------------------------------------------------
\section{Monte--Carlo estimation of $\Sigma^\star$}

Generate $N$ draws
$\phi^{(n)}\stackrel{\text{iid}}{\sim}\mathrm{Unif}[-1,1]^P$ and let
$a^{(n)}=T(\phi^{(n)})$.  The unbiased estimator
\[
  \widehat\Sigma
  =\frac1N\sum_{n=1}^{N}a^{(n)}a^{(n)\top}
\]
converges at the usual $\mathcal O(N^{-1/2})$ rate.  Variance may be
reduced further by quasi--Monte Carlo (Sobol/Halton), Latin--hypercube
sampling, or simple control variates.\vspace{4pt}


%--------------------------------------------------------------------
\section{Implications for popular shrinkage rules}

A common heuristic prior is isotropic
$p(a)=\mathcal N\!\bigl(a\mid0,\lambda I_P\bigr)$ with
$\lambda\propto 1/P$ or, in some empirical Bayes treatments,
$\lambda\propto 1/P^2$.  Moment matching provides the first principled
derivation of \emph{why} such scalings arise:

\begin{itemize}
  \item For large $P$, the diagonal of $\Sigma^\star$ shrinks roughly
        like $1/P$ because each coordinate $a_p$ is a bounded random
        transform of a uniform PACF parameter.
  \item Off--diagonal (lag--to--lag) correlations in $\Sigma^\star$
        concentrate additional mass inside the stability region,
        explaining why the naive isotropic prior must shrink even
        harder (occasionally down to $1/P^2$) to avoid unstable draws.
\end{itemize}
Hence $\Sigma^\star$ not only recovers the familiar $1/P$ law but also
quantifies the \emph{full} covariance structure that the heuristic
ignores.

%--------------------------------------------------------------------
\section{Discussion}

Moment matching yields a closed--form Gaussian approximation that
captures all second--order geometry of the stability region.  Because
the optimisation is convex, no numerical search is required once
$S_q$ is available.  Future work may consider sparse--precision
representations (e.g.\ graphical Lasso on $\Sigma^{\star-1}$) or robust
covariance estimators for heavy--tailed settings.

%--------------------------------------------------------------------
\begin{thebibliography}{9}\small
\bibitem{Monahan1984}
G.\,E. Monahan (1984).\;
A note on enforcing stationarity in autoregressive models.\;
\emph{Biometrika}~71(2), 403--404.
\end{thebibliography}

\end{document}
